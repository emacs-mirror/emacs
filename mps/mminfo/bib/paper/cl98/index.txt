                 USING GENERATIONAL GARBAGE COLLECTION
              TO IMPLEMENT CACHE-CONSCIOUS DATA PLACEMENT
                   Trishul M. Chilimbi;James R. Larus
                             ACM, 01/10/98
                           ISMM'98 pp.37--48
http://www.acm.org/pubs/citations/proceedings/plan/286860/p37-chilimbi/

                               paper.cl98
                               incomplete
                            pekka 1999-12-03


ABSTRACT:

Processor and memory technology trends show a continual increase in the cost of 
accessing main memory.  Machine designers have tried to mitigate the effect of 
this trend through a variety of techniques that attempt to reduce or tolerate 
memory latency.  These techniques, unfortunately, have only been partially 
successful for pointer-manipulating programs.  Recent research has demonstrated 
that these programs can benefit greatly from the complementary approach of 
reorganizing pointer data structures to improve cache locality.  This paper 
describes how a generational garbage collector can be used to achieve a 
cache-conscious data layout, in which objects with high temporal affinity are 
placed next to each other, so they are likely to reside in the same cache 
block.  The paper demonstrates the feasibility of collecting low overhead, 
real-time profiling information about data access patterns for object-oriented 
languages, and describes a new copying algorithm that utilizes this information 
to produce a cache-conscious object layout.  Preliminary results indicate that 
this technique reduces cache miss rates by 21-42\%, and improves program 
performance by 14-37\%.

REVIEWS:

He worked with the Cecil Vortex compiler and instrumented it to annotate 
references to the "base" of an object to a buffer.  At the start of a GC he 
analyzed the buffer by sliding a window across the buffer and creating an 
"affinity" graph where links are weighted by how many times an object appears 
in the window with another object.  Then the node with the highest edge weight 
in the affinity graph is used as a root and the graph traversed greedy 
depth-first, thus packing objects in the graph.  Finally the actual roots are 
used to complete the collection.

Despite this overhead on object accesses and collection, and despite the 
affinity graph traversal potentially retaining garbage, very impressive 
speed-up numbers are obtained and L2 cache misses (on the testbed UltraSPARC:  
1Mb L2 cache w/ 64byte blocks) are reduced 21-42%.

ptw 1998-11-06



