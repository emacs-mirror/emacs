        GARBAGE COLLECTION USING A DYNAMIC THREATENING BOUNDARY
                     David A. Barrett;Benjamin Zorn
              University of Colorado at Boulder, 01/07/93
                     Technical Report CU-CS-659-93
    ftp://ftp.cs.colorado.edu/pub/techreports/zorn/CU-CS-659-93.ps.Z

                            paper.barrett93
                                 draft
                             dsm 1995-07-28


ABSTRACT:

Generational techniques have been very successful in reducing the
impact of garbage collection algorithms upon the performance of
programs.  However, it is impossible for designers of collection
algorithms to anticipate the memory allocation behavior of all
applications in advance.  Existing generational collectors rely upon
the applications programmer to tune the behavior of the collector to
achieve maximum performance for each application.  Unfortunately,
because the many tuning parameters require detailed knowledge of both
the collection algorithm and the program allocation behavior in order
to be used effectively, such tuning is difficult and error prone. We
propose a new garbage collection algorithm that uses just two easily
understood tuning parameters that directly reflect the maximum memory
and pause time constraints familiar to application programmers and
users.

Like generational collectors, ours divides memory into two spaces, one
for short-lived, and another for long-lived objects.  Unlike previous
work, our collector dynamically adjusts the boundary between these two
spaces in order to directly meet the resource constraints specified by
the user.  We describe two methods for adjusting this boundary,
compare them with several existing algorithms, and show how
effectively ours meets the specified constraints.  Our pause time
collector saved memory by holding median pause times closer to the
constraint than the other pause time constrained algorithm and, when
not over-constrained, our memory constrained collector exhibited the
lowest CPU overhead of the algorithms we measured yet was capable of
maintaining a maximum memory constraint.

REVIEWS:

dsm:

SUMMARY

The creation order of objects on the heap is known.  The nth
collection looks at objects younger than TB(n) (the threatening
boundary).

The paper compares six algorithms, implemented by using a formula to
set TB(n):
  Full
    Non-generational - all objects collected on each collection
  Fixed1
    Objects that survive one collection are tenured (i.e. not collected
    again!)
  Fixed4
    Ditto but with four generations
  FeedMed
    Same algorithm as Ungar and Jackson's Feedback Mediation collector
    [An adaptive tenuring policy for generation scavengers]
    If the previous collection took too long, this algorithm advances the
    threatening boundary thus tenuring some objects.  (Tenured objects
    are never collected.)
  DtbFM
    Same as FeedMed, but also retreats the TB if previous collection
    was quicker than the target pause time.
  DtbMem
    This adjusts TB to try and achieve a maximum memory constraint.

The results are unsurprising.  DtbMem achieves the memory constraint
only if it is possible, but doesn't guarantee any reasonable pause
times.  DtbFM performs better than FeedMed, in that it reclaims more
space.


COMMENTS

It seems to me that the algorithms as presented are not useful.  DtbFM
allows the ratio of memory used to live memory to grow without bound.

The concept of a continuous boundary between generations is an
interesting one.  The threatening boundary is a division between
objects created before a certain time and the younger objects.  One
penalty of this finer control is that the remember set is more
expensive to keep track of.  The implementation in the paper
remembered all back pointers.

However, the order of objects is chronological, which is not the same
as the order in which the objects are most likely to die (unless, of
course we are lucky).  After making appropriate assumptions, the
probability of an object dying, depends on (1) how old it is and (2)
when it was last proved alive.  I cannot see how a threatening
boundary makes it any easier to keep track of (2), without
implementing a conventional generational scheme on top.  Varying from
that scheme, by making small adjustments to the position of the
threatening boundary, does not help as objects are not stored in most
likely to die first order.  Schemes which ignore (2), as the ones in
the paper do, seem to me to lose the benefits of more standard
generational schemes.


pekka:

Broadly speaking I agree with David: the algorithms are not useful as such.  
The cost is high and the benefits small, even by their own measurements 
(although they measured the  algorithms so far from their optimum configuration 
that the results are almost useless).

I must take issue with his point (2) -- the probability depends not on when it 
was last condemned, but on when all the other stuff that might be keeping it 
alive was (in generational schemes, these are closely related).  Keeping track 
of that might help to avoid extra work, but it still doesn't change the fact 
that younger objects are likelier to die, and hence if you have the cycles to 
spare, you might as well try from the younger end.  In other words, objects 
_are_ stored roughly in most likely to die first order.

Their control equations seems to produce very erratic behaviour.  Some of it 
might be that they aim for the optimum using bad estimates.  Lest in Table 1 is 
not just bad, the justification is just wrong: Tracen-1 could contain garbage 
help onto by tenured garbage.
Figure 2 also contains a mistake: To reduce tenured garbage, it's not enough to 
collect more, you must actually condemn something you didn't condemn the last 
time, i.e. TBn < tn-1.

Can you say "not peer-reviewed"?  pekka 1998-11-03


ATTACHMENT
   "CU-CS-~1.PS"

