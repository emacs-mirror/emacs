             DESIGN OF THE OPPORTUNISTIC GARBAGE COLLECTOR
                      Paul R Wilson;Thomas G Moher
                             ACM, 01/10/89
                     ACM 089791-333-7/89/0010/0023

                              paper.wm1989
                                 draft
                             drj 1998-07-02


ABSTRACT:

The Opportunistic Garbage Collector (OGC) is a generational garbage collector 
for stock hardware and operating systems.  While incorporating important 
features of previous systems, the OGC includes several innovations.  A new 
bucket brigade heap organization supports advancement thresholds between one 
and two scavenges, using only two or three spaces per generation, and without 
requiring per-object counts.  Opportunistic scavenging decouples scavenging 
from the filling of available memory, in order to hide potentially disruptive 
scavenge pauses and improve efficiency.  Card marking efficiently records which 
small areas of the heap may contain pointers into younger generations, and is 
supported by a refinement of the crossing map technique, to enable scanning of 
arbitrary cards.

REVIEWS:

Four topics are discussed in the paper.

(1) Advancement Policy and Heap Organization: Using a two-bucket bucket-brigade 
as a means of reducing premature tenuring without having to resort to explicit 
age counts.

(2) Number of Generations: Multiple generations vs large generations vs buckets.

(3) Scavenge Scheduling: The use of opportunistic or scheduled scavenges as a 
means of hiding GC pauses and/or improving the effectiveness of GC.

(4) Intergenerational Pointers: Use of card marking as an alternative to 
remembered sets or VM page marking for dealing with intergenerational 
references.

Below is a discussion of each of these topics.  Overall I find the paper 
readable and interesting, but I dislike two aspects of it:

- There are no empirical data on the techniques proposed.  They do use 
empirical data to argue their position; those data are taken from Shaw's and 
Steenkiste's dissertations and from the 1988 Ungar/Jackson OOPSLA paper.  They 
only use the trends from those data sets, not the actual data.  As far as I can 
tell, nor have they run any simulations.

- The (largely unstated) assumption that motivates the first two topics is that 
RAM is scarce, which is rather less true now than when the paper was written.  
The authors comment on this very briefly by noting that more plentiful RAM will 
make their bucket-brigade promption policy less interesting; I will comment on 
this below.

(1) Advancement policy

The authors propose a simple bucket brigade to avoid premature tenuring without 
the costs of per-object age counts at a slight cost in the amount of copying.  
Their method guarantees that all data promoted from a generation is at least 
one full allocation cycle old.  It works like this.  A generation consists of 
two buckets, New and Old.  When the generation is scavenged, live data from the 
next younger generation (or from the creation space) are promoted into the New 
bucket, the data in the Old bucket are promoted to the next older generation, 
and at the end of the collection cycle, the New bucket becomes the Old bucket.  
Then the next allocation cycle starts.

If the creation space is k bytes, objects in generation n will always be at 
least k bytes older than objects in generation n-1.

The crux of the argument is the following paragraph (p25, second column, top):

     While increasing the threshold beyond two scavenges is not an 
     obvious win, the increment from one (as in Moon's system) to 
     two is highly worthwhile.  This is primarily because of the 
     increased precision of the advancement mechanism -- very young 
     objects are never advanced, and most of them die before the next 
     scavenge.  In general, the second scavenge reduces advancement
     by a factor of two or more, while increasing copying costs by
     less than half.

They then use a set of figures, based on Shaw's data, to illustrate the point.  
The argument only makes sense to me if we assume a very small number of 
generations (they appear to assume two, although it's not explicitly stated).  
In that case, it's not hard to believe that the advancement precision could pay 
off.  Beyond a small number of generations, however, it seems like Moon's 
system would do just as well because every other generation acts as a bucket in 
the Wilson/Moher system.

[Bucket brigades were invented by Robert Shaw in his Ph.D. thesis. @@@ 
Reference.]

The authors also discuss the possibility of varying the promotion rate by 
dynamically adjusting the boundary between the buckets.  For example, one could 
promote both the Old bucket and the older part of the New bucket, thereby 
lowering copying costs but increasing promotion (and possibly premature 
promotion).  This allows one to trade age precision for copying costs.

(2) Number of generations

The discussion of the number of generations to choose is somewhat superficial 
and no experimental data are presented.  Their major concern is reducing the 
consumption of physical memory for the younger generations.  They observe that 
one large generation is probably better than two small generations (their data 
support this, and other authors have observed this as well; both Moon and Ungar 
make similar observations in their papers), if the size of available RAM is 
fixed.

The inevitable conclusion of their discussion is that a small number of 
generations is better than a large number of generations, and this justifies 
their work on bucket brigade advancement.  

I suggest that given today's RAM sizes, bucket brigade advancement has few or 
none advantages over Moon's method, particularly since bucket brigades increase 
copying and seem to little or nothing for locality. [It might be interesting to 
go back to Shaw's work (which they don't review) and see how he motivated it.]

(3) Scheduling scavenges

This section contains suggestions on picking GC times:

- At the end of computation-intensive phases (i.e., as part of a long "pause" 
from the user's perspective).

- While waiting for the user to do something.

- Following stack retractions (for example on stack cache underflow). They note 
that Lieberman and Hewitt also proposed this.

The authors suggest that by running the GC opportunistically, overall GC 
performance can improve.  The intuition is that a user pause or end of 
computation represents a phase pause (my phrase) in the program where there 
will be a lot of dead data.  If the garbage is collected opportunistically at 
the phase pause, then more space will be available for allocation during the 
next phase of the computation.  Since more allocation space may result in a 
lower mark/cons ratio, overall GC performance may improve.

Some non-compelling evidence based on Shaw's data is presented, but no actual 
measurements of their opportunistic system are presented.

(4) Intergenerational Pointers

The authors advocate an unconditional card-marking system using 32-word cards 
and a crossing map.  They note that the card info can be used to separate dirty 
and clean objects during scavenging, thereby heuristically improving the 
subsequent VM locality of written-to objects.  (They provide no data to support 
this.)  [Has anyone pursued this?  I've never seen it mentioned elsewhere.]

They obliquely suggest that the crossing map can be avoided if all objects are 
either pointer-containing or non-pointer containing, and objects of one class 
are segregated from objects of the other class. Only dirty pointer-containing 
cards need be scanned, without regard for where objects start.

They note that remembered sets may be a better choice (their implementation is 
a Scheme implementation), but this contrasts with Wilson's later work where he 
advocates byte-marking cards. [No reference available yet]





