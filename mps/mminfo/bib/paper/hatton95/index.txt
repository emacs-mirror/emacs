               LOGARITHMIC DISORDERS IN SOFTWARE SYSTEMS
                               Les Hatton
                   Programming Research Ltd, 01/01/95

                             paper.hatton95
                                 draft
                            nickb 1996-07-11


ABSTRACT:

This paper explores the empirical results of a number of recent (and 
not-so-recent) papers showing that large software components are 
proportionately more reliable than small software components within the same 
system. This is completely counter-intuitive to basic notions of software 
engineering such as that of modularisation or structural decomposition, which 
is purported to reduce complexity, improve reliability, and so on. The nature 
of the empirical evidence is quite complex. Larger components are 
proportionately much more reliable than smaller components, but for very large 
components, which appear in two of the studies, there is a significant trend 
towards proportionately decreasing reliability again.

The paper first demonstrates that a logarithmic distribution of error with 
component complexity very closely fits the observed data over a range of 
component sizes and languages up to around 130 lines or so, before going on to 
show that the observed data then departs significantly from this behaviour 
thereafter.  The paper will review mitigating influences for this non-intuitive 
behaviour before concluding that none is really satisfactory. It then unites 
this complex behaviour in a simple model of the human memory system making use 
of considerable physiological and psychological evidence for the existence of a 
short-term or cache memory in which comprehension and semantic manipulation is 
much more accurate than in long-term or backup memory. The resulting component 
error rate model accurately predicts the observed data for all languages in 
this study. Close parallels are drawn with classical thermodynamics, wherein 
the growth of such disorder is inevitable. The paper then argues that bugs in a 
software system are essentially a manifestation of macroscopic behaviour and 
may be similarly inevitable.

The discussion follows on by building a further simple system error rate model 
for such behaviour and uses this model to make predictions about overall system 
error rates based on the component error rate model. The principle predicted 
features are : 1. that systems built using very small components or relatively 
large ones may be more reliable than systems built using small to medium sized 
components, as is currently the norm in modern systems design. This gives some 
admittedly tenuous theoretical support for object-oriented design, which is 
characterised by very small components; 2. reducing the average component size 
of an existing system may make the system worse, depending on the existing 
average component size; 3. Re-use is unlikely to improve reliability unless it 
dramatically reduces the overall size of a system.

The case studies reported here are an excellent example where an intuitively 
attractive and very widely-practised design principle on the one hand, and 
recent reliability measurements on real systems on the other, are currently in 
serious conflict.

REVIEWS:

Erm, what the abstract said. The maths is a bit flakey in parts, the argument 
isn't as strong as it could be. But strongly recommended in any case, because 
the point is such an important one. The optimum component size for reducing 
defect density is around 150 lines, regardless of whether those are lines of 
assembler, C, FORTRAN, or Ada. Can it be true?



