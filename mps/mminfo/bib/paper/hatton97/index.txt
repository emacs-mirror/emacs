               SOFTWARE FAILURES -- FOLLIES AND FALLACIES
                               Les Hatton
                         IEEE Review, 01/03/97
                          ISSN 0013-5127, p49

                             paper.hatton97
                               incomplete
                           gavinm 1997-03-28


ABSTRACT:

Faults in software are becoming big news.  They cost a fortune and many are 
avoidable.  Les Hatton discusses some common misconceptions.

REVIEWS:

An interesting article, if lightweight.  Some of his conclusions would bear 
investigation:
  - UNIX is vastly more reliable than Macs or PCs;
  - PCs are schooling the public into the acceptance of poor quality software;
  - Defects/1000 lines of source code has remained constant over the last 15 
years -- around 6;
  - Software engineering owes more to the fashion industry than it does to the 
engineering industry;
  - Programming language is at best weakly related to reliability;
  - Object-oriented techniques do not affect fault density, but increase 
maintenance cost 300%;
  - Formal methods do not reduce pre-delivery (QA) defect rates, but do reduce 
post-delivery rates;
  - There is only a weak link between statically detectable fault rates and 
process certification (ISO9001);
  - Defect density is minimal when component size is around 150-250 lines;
  - Reuse can often reduce reliability;
  - 40% of all software failures could have ben detected by static analysis;

The article is abstracted from Les Hatton's soon-to-be-published book "Software 
failure: the huge cost, the avoidable and the unavoidable".  His bibrefs list 
another article by himself "Why is the defect density curve U-shaped with 
component size?", IEEE Software, March 1997.  His email address is 
lesh@oakcomp.demon.co.uk -- GavinM 1997-03-28



