               THE MEMORY FRAGMENTATION PROBLEM: SOLVED?
                    Mark S. Johnstone;Paul R. Wilson
                             ACM, 01/10/98
                            ISMM'98 pp.26-36
http://www.acm.org/pubs/citations/proceedings/plan/286860/p26-johnstone/

                               paper.jw98
                                 draft
                            pekka 1999-12-03


ABSTRACT:

We show that for 8 real and varied C and C++ programs, several conventional 
dynamic storage allocators provide near-zero fragmentation, once overheads due 
to implementation details (headers, alignment, etc.) are properly accounted 
for.  This substantially strengthens our previous results showing that the 
memory fragmentation problem has generally been misunderstood, and that good 
allocator policies can provide good memory usage for most programs.  The new 
results indicate that for most programs, excellent allocator policies are 
readily available, and efficiency of implementation is the major challenge.  
While we believe that our experimental results are state-of-the-art and our 
methodology is superior to most previous work, more work should be done to 
identify and study unusual problematic program behaviors not represented in our 
sample.

REVIEWS:

The thrust of his paper is to factor out implementation overhead and study only 
strategy as it affects (external) fragmentation.

New to me were his 4 ways of measuring fragmentation F = Used by allocator/Used 
by program, with caveats:

[A & B omitted]

C. At point of maximum used by allocator.  This may inflate fragmentation 
artificially.

D. Numerator at point of maximum used by allocator, denominator at point of 
maximum used by program.  This may deflate fragmentation artificially.

He reported his results using C and D.  He studied 54 allocators, but only 8 
test programs (which makes his results somewhat suspect to me).

His conclusions are just about opposite of what I came to for mv2.  He 
concludes the strategy that underlies the best policies is to reallocate 
objects that have died recently (leaving older objects to further coalesce).  
Although in a footnote he notes that one of the better performing policies does 
_not_ have that strategy.

Questions wondered if hiding implementation overhead was valid -- perhaps some 
policies _require_ more overhead than others?  And what is the time overhead of 
achieving minimal fragmentation?  Ungar quoted Deutsch as saying "The 
revolution is not over until allocation is done as freely as a function call!"

Johnstone requested that people send him traces.

ptw 1998-11-06

See my review of paper.jw97.

pekka 1999-12-03



