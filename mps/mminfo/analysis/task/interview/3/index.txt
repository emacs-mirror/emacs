               INITIAL TASK ANALYSIS -- INTERVIEW 3: ANDY
                       analysis.task.interview.3
                             temporary doc
                           richard 1996-07-09

 - Report 3 

Interview Number 3. AndyC. 

Normal Development Tasks: 

Formerly everything for ScriptWorks, GUI, device driver, etc., now mostly core 
RIP. 

Experience: 

Over 9 years. 

Platforms: 

Mac, PC, UNIX.  16 MB up to 600 MB with 20 GB plus stripe drive.  

Software tools and environments: 

Normal Mac ones, Vis C on PC, DBX on UNIX.  

Past use of MM tools: 

None.  Went from Malloc and Free to writing own MM.  On Mac, used virtual 
pointers with own paging system.  Original code was not platform-independent, 
but an effort was made to overcome this with developments for Level 2 
ScriptWorks.  

Memory Problems in past (size / performance): 

Using our own virtual memory system we found that the display lists could 
become huge.  Our paging system could make things crawl.  In particular, NT 
could get very confused.  If NT was in for example 64MB, and we ran a 40 MB 
RIP, there should have been plenty of room.  If we has a job of say 4 images of 
8 MB, we would try to read in each image in to a block.  NT would then try 
cache the 8MB file, and run out of space. It would try to page out what we were 
trying to read it into!  We cured this by using asynchronous read-ahead.  

We also found that NT was strange in terms of paging anyway: running the same 
job repeatedly gave successively shorter execution times until the paging had 
settled down to suit the job in hand.  This was obviously a real problem for 
benchmarking.  We cured it by locking down as much memory as possible.  This 
gives other problems, as where an OEM wants to run co-operative programs in the 
background.  If we have grabbed most of the memory, NT then starts paging as it 
runs the other programs in low memory.  If we could say that we only really 
care about part of what we have taken, then things would be better, but it is 
very hard to do this on NT.  

We have had other problems with fragmentation.  Postscript is an ‘all or 
nothing’ system, where if you try to load, say, an 100 MB image into 98MB of 
memory, the whole image is put down to disc.  Some applications then split this 
into tiles which can be rotated individually, so that you could have some 2000 
images each of exactly 200K, along with some ancillary data.  This gives us 
some fragmentation problems where even small objects were taking up large 
amounts of memory, so that images had to be put down to disc.  We needed 500MB 
to run a 100MB job.  This is still our biggest problem and has not been solved 
yet.  It is made worse by the way we use 64K segments with different memory 
pools.  Objects overlaid on these can cause interactions which consume a great 
deal of space.  It really hits us in specific cases as where an OEM puts 2 
million rectangles on a page.  It is an extreme case, but the OEM really cares 
about it.  Where the rectangles are put vertically down the page, or the page 
is rotated, we use
 much more memory.  Competitor’s RIPs do it differently, and may look as if 
they use much less memory.  They are thus easier to sell, and we have to reduce 
the size of our job. In this instance we have a work-around that gets the size 
down from say 80MB to 20MB, but if we could e.g. group rectangles with similar 
attributes, then we could get much greater reductions.  

Another problem for us is GC - it is a secret that we don’t have complete GC, 
and one of the reasons for moving to MPS is to get this for the core RIP.  It 
has only hit us once before where Level 2 introduced compression filters.  
These are used to retrieve parts of a compressed job in sequence, and for 
colours, four may be used in parallel.  Once used, they were not recognised as 
dead objects, and neither were the buffers used by each filter, so they would 
just hang around.  We put in code to free the data structures, but then had to 
go a stage further to make sure that each new filtering job inherited the old 
filters.  

Solutions: 

See above.

Other Tools that would be useful; Other ways of looking at the data: 

Diagnosing of Fixing? Both.  One thing we did earlier on was to insert code to 
produce diagnostic info.  We have recently lost this with a move to a new MM 
module, but we hope to get it back in due course.  We used this info. to 
support a pretty coloured dial which had a number of lines indicating how 
memory was being occupied.  It was static in the sense that there were only so 
many lines, and they had specific meanings.  There was a line showing red for 
display list memory, and green for general memory.  Another line was for 
images, which sit in display list memory, showing two colours if they were in 
memory or disc.  There was a line for characters, and another for half-tone 
cache.  This graph would get updated as memory was allocated or de-allocated, 
so that you could see what was happening  

All memory allocations in the RIP are now tagged, so that each has a type.  We 
are hoping for a GUI where we could say, create  a new bar, and say ‘display 
allocations of this type’ and get the information to update it.  I actually 
used this instrument to diagnose some of the problems I described above.  If 
you run it for a time before you start to hit a problem you can see what 
happens normally, and so it is fairly easy to say what is going wrong when you 
hit a problem.  You could see when a large image got put out to disc, or when a 
font cache was compressed and re-allocated.  You could see just where it was 
allocated. 

It might be useful to see the distribution of objects in memory, i.e. the 
number and the average size of allocation for that class of object, or even the 
distribution of every single allocation plotted in a neat graph.  We do try to 
set an initial size for jobs, which can be extended as required.  Some jobs 
differ in size though, and if we could know just how much would be required at 
the beginning, then things would run faster.  

For paging problems we use the OS’s tools.  

Software Development

Any particular approach to software or development that works for you? 

We have an approval process in general use.  Our work is classified as bug, 
task, or project.  The smaller jobs get done by individuals, but the larger 
ones (projects) usually have a site meeting to consider the issues, and a 
specification produced.  After coding, the job is checked directly into the 
live trunk of the source.  This means that new code is tested immediately.  An 
approver then looks at the code.  This checks for structure, functioning, and 
that side effects have been considered.  Other checks are for quality, e.g. 
that asserts are in place to check that assumptions are valid.  

Is there a point in development when you specifically consider MM issues?

This is a part of what the approver should look at.  They should check that 
allocations are freed.  This part of our process is probably not as strong as 
it should be.  They should check that the code does not run off the end of 
allocated arrays.  Another thing that should be done, but is probably not, is 
to check for fragmentation issues.  An example would be a transfer function 
that is used for a single colour early in a job.  If it is not used for the 
rest of the job the allocation is still locked in.  We hope that this might 
improve with the MPS system, as will the ability to re-locate things to improve 
fragmentation.  With the MPS we should eventually be able to sub-class memory 
pools to keep objects of similar type located close together.  

How do you handle these issues? 

Fragmentation: we generally only have fragmentation problems with images.  A 
30MB job into a 40MB RIP may be very fast the first time, but  much slower the 
second time.  Our problems are thus in specific circumstances, and so can be 
spotted without much trouble.  We can lose small pieces of memory without 
worrying too much.  We would not generally want to go looking at the memory 
itself.  We used to use fence-posting and checking in the code, but more 
recently the greater use of asserts and internal consistency checking has made 
the kinds of errors associated with this less common.  If you do something 
wrong an assert should fire, and warn you.  In some cases switching to a 
debugging allocator may hide the bug.  We ‘ANSI’fied the core RIP, and have 
compile warnings turned right up .  We write conservative C, with lots of 
asserts.  We should have very solid code like this, and so development is 
easier.

How do you test the results of your decisions? 

Do you have tools to help with this?

Do you avoid particular techniques because of your requirements? 

Thinking about possible MM products. 

For a gain in performance or size, do you want a product ‘out of the box’ or 
tuneable?  

I could give different answers to this.  One thing that matters is the ratio of 
work done to memory allocated.  Ours is quite high.  It is not like a Lisp 
system where you might get lots of allocation for little work.  This means that 
for some things we don’t care whether the allocation takes 20 or 100 cycles, as 
we do so much work with the result.  One control we DO care about is that we 
have a fixed chunk of memory when we boot up.  If we run out of this, we have a 
low-memory handler which will take some actions.  If necessary, it will do a 
partial paint.  This can have a high overhead.  For some customers this can 
work quite well, but for others we would have to compress 1.5 GB of memory.  
This is a huge overhead with reads and writes.  These customers would rather be 
able to extend the ‘arena’ or memory chunk, to avoid doing a partial paint, 
even if the machine thrashed a bit.  The problem is that you don’t know when 
you will reach the end of a page - you might only need a little more memory.  
So, we 
would like to be able to control the impact of allocations on the machine’s 
resources.  An OEM might say that I want to give the RIP 40MB, but only work 
with 20MB so that I can run a co-operative application.  If necessary, I will 
allow the RIP to go up to 40MB to avoid a partial paint.  In this way you could 
determine not just how ScriptWorks behaves, but how other software such as NT 
or other applications work with it.  These kinds of options would probably go 
into an ‘advanced user options’ box.  They would not be hidden, but we would 
not want all users playing with them. 

Would you value a range of MM policies to suit the jobs at hand? 

I think the MM should be able to provide services to de-fragment things, and 
help in trade-offs between speed and size.  It should know about sub-types of 
allocation, to help with de-fragmentation, even if it has to ask for help, as 
in fixing up a pointer.  It should provide debugging facilities such as 
fence-posting, and should tell you what has happened or is happening.  It may 
also control the behaviour of memory in relation to the underlying operating 
system.  I don’t see what else it could do.  

Another area of potential benefit is in the fast in-line allocation for special 
needs as when rendering a page.  We may need lots of 8K objects.   If we don’t 
have them, this can have a dramatic impact on the RIP’s performance.  From the 
RIP’s processing we may have a linked list of path items which are cached, or 
put in main memory if the cache is exhausted.  For a very complex job these can 
get huge, and are fixed down.  If we were able to re-use this cache memory for 
rendering, it could make a huge difference to performance.  We hope the MPS 
will help us to get rid of all these special caches.  This would also help us 
to run in a smaller memory system.  This is starting to matter as we are now 
working for people with much smaller machines, and we want to dynamically use 
memory in an efficient way.  

Would it help to have precise feedback on how a given policy affected your 
code?  

Would you consider any re-engineering of your software to work with the MM 
products?

Do you have any reservations about using MM products? 

We would  not necessarily use the GUI that comes with the MM - we might want to 
hook in our own, or to make changes.  In relation to the MPS system, no real 
reservations.  We would be happy to let the MM team look after this for us.  It 
would be good to have someone else worrying about this on a formal footing.  We 
need to stay ahead as the ‘fastest RIP’, especially in our problem cases.  A 
systematic approach to MM and a purpose-built product would be great.  The 
initial MM for ScriptWorks was written quickly and then tweaked.  

Would this apply for the rest of your team? 

Would you want your users (the OEMs) to have access to the MM tools?  Or even 
the end users?

One possibility is to have monitoring of how the users generally work with the 
system.  One OEM does this already . They can say that e.g. adding only another 
128MB of memory would give a much better performance and would be worth the 
extra cost.  We could add monitoring to see if, for example, the font cache was 
not used much.  This would allow an OEM to see how the RIP could be 
re-configured to make better use of the resources.  This is not in the memory 
allocator but in the RIP.  Feedback about these kinds of things is not really 
in the memory management, but in the RIP, so there is a limit to how useful it 
would be to us.  It is the RIP that has to decide what to do.  

At the moment we have a couple of controls in one of our dialogs which are 
about the interaction between the machine and ScriptWorks rather than internal 
control.  These can say e.g. ‘get every bit of memory you can, but leave a 
reserve’, or ‘use this amount of memory’.  Alternatively you can say ‘I know 
best: use 60MB even on a 40MB machine’.  If you are co-operating with other 
applications you need to know how things work best.  We know what NT needs, but 
if other applications are there you need to make allowance for them.  These are 
the only controls that are there at the moment, and these give a big influence 
on the performance of the RIP.  

You should also remember that some users can be stupid, so we don’t want to 
give them very much access to the system.  In the past they have caused much 
trouble just be changing the case of filenames.  The RIP could  not see the 
fonts.  System operators particularly may not be very smart, so we don’t want 
to give them too many controls to play with.  If they start changing things 
they could damage the performance of the RIP, and so our sales.  The RIP keeps 
control over most decisions, and we don’t want to let users play with this.  
Our name is built on performance, and we cannot risk losing it.  We use 
configuration files now, rather than dialogs, for this reason.  We don’t 
publish all tuneable parameter as it is.  

Anything else to add that we have not covered?

ATTACHMENT
   "Report 3"

