           INITIAL TASK ANALYSIS -- INTERVIEW REPORT 1: ANGUS
                       analysis.task.interview.1
                             temporary doc
                           richard 1996-07-09

 - Report 1 

Interview Number 1. Angus. 24/06/96

Normal Development Tasks: 

ScriptWorks Core: algorithms and data-structures for RIP.  Designing and 
writing these from scratch; fixing bugs, looking for performance improvements.  

Experience: 

Over 2.5 years on this 

Platforms: 

PC, Mac, UNIX.  NT, Win95, and Mac OS are most important.  Up to quad processor 
Alphas, with half a GB of memory, and 20 GB discs.  

Software tools and environments: 

Mostly on NT, using visual C environments - compilers, debuggers, profilers.  
Not for project management though.  For this we use normal "make" tools, which 
we automate.  

Past use of MM tools: 

Not of "Purify" nature.  We have added a lot of "debug" type info. to our own 
memory management, in order to sort out some difficult problems we had.  Using 
Purify does not always make sense, due to the way that we organise our own 
memory.  We ask for large amounts of memory at once, and don"t give it back 
until the end of processing.  

Memory Problems in past: 

Yes, we have had many problems - especially fragmentation due to either the 
software allocating static buffers lazily, or from losing references to memory 
and ending up with leaks. 

Solutions: 

There are several solutions depending on how heavyweight we wanted to be.  The 
simplest ones were just counting the number of allocations and de-allocations 
we have done, and checking this to look for leaks which were than reported.  
Another approach was to do fence-posting at the end of all allocations.  Yet 
another was to do tagging of allocations using a separate memory pool  This 
means getting memory from the system rather than using our own internal system, 
and providing tags for every allocations so that we can trace where it came 
from and what it was used for.  

These were tedious to trace, and had a major performance impact.  They were 
only compiled in for a special debug version of the RIP used to investigate 
special problems.  We have a hierarchy of debug approaches we can use depending 
on whether the bug looks like an overrun on an array etc.  The information 
comes back in a textual form which we have eventually to trace through by 
hand.  

Other Tools that would be useful; Other ways of looking at the data: 

There was a piece of work that Andy (Cave) did for the Mac OS, which was very 
useful.  This showed graphically which areas memory was allocated to.  This 
showed a dial with bars indicating how much was allocated to font cache, to 
half-tone cache, display list etc.  Visual info. like this is much easier to 
use, but we never had to time to produce these tools  cross-platform.  Visual 
displays of structures etc. would make things a lot easier.   Being able to 
trace where things go graphically, rather than step through one level at a time 
in a debugger tracing them down through pointers, would be lovely if someone 
would provide it.  However, it would have to be cross-platform.  This tool was 
updated as the program executed, and gave a running snapshot of the proportions 
of memory allocated.  

Any particular tool which would give you the greatest gain if you could use it? 

Not any one thing which is a silver bullet.  It is all the info. put together 
that is useful.  A proportional dial as described is useful, but is not a full 
solution.  It might help us detect for instance thrashing between caches which 
we might not detect from a tool giving e.g. a time-slice view, or a tool giving 
a structure display.  For other problems other tools would be more 
appropriate.  

Software Development

Any particular approach to software or development that works for you?

Nothing formal in this division.  For larger projects we do try to prepare 
specifications, but not in any formal languages.  These are peer-reviewed.  
There is a fair amount of drift, as the spec. never gets kept up to date with 
the current work, and other requirements are found to apply after the project 
starts.  Personally, I think about it for a long time and write down the 
result.  Even with a spec. there are always some little problems or performance 
problems that need fixing.  

From the spec. we start working.  We generally avoid using third-party 
libraries or freeware etc. due to uncertainty about the legal situation.  We 
have enough problems as is.  We thus tend to re-write things, and recently 
these have been done in a more modular way, to support re-use within the 
company.  

Is there a point in development when you specifically consider MM issues?

These issues are mainly related to the core RIP which is the heaviest user.  
The strategies for this were set long ago.  Other associated software generally 
uses little memory.  For new developments for EP 2000 we are having to 
re-consider and extend things.  Various solutions have been tried in the past, 
and different solutions have been found appropriate for different parts of the 
task.  We now plan to use a combination of the techniques we have used before, 
but we are very performance constrained, so we try to make sure that everything 
fits into the working sector.  We do not want to rely on virtual memory.  Thus 
for the core RIP, we have to think about memory management constantly.  If we 
introduce a new algorithm for instance, we have to consider what happens if it 
runs out of memory?  Can we get it enough temporarily to finish the operation 
and then throw it away? How can we make space? Can we put it out to disc?  
Essentially, how can we carry on?  The main concern is to see that the job will 
process an
d not stop.  Beyond that, we look for performance.  

Do you have tools to help with this?

Not really.  The main source of information is "lore" from those who know the 
RIP.  Those who are new to it generally may miss issues, and, for instance, 
allocate too much memory to get a job done.  This will generally be caught in 
the code "approval" process which is carried out by an experienced RIP 
developer who thinks about these issues. 

Do you avoid particular techniques because of your requirements? 

We generally avoid techniques which require allocating large amounts of 
memory.  Large amounts of temporary allocation are ok, but keeping large 
amounts of tables, buffers, structures around for a long time is not.  Also, we 
try to do allocations in such a way that we do not fragment memory too much.  
We try to use the parts that we have already got.  If we know that we will do 
many allocations, we will take a large block and carve it up ourselves rather 
than do many small allocations.  

What steps do you take to study a program where you suspect MM faults? 

It depends on the type of the problem suspected.  For fragmentation part of it 
is looking at what the low-memory handler is doing.  We look at the ratios of 
the sizes of allocations; e.g. for images, we try to keep all the image data in 
memory if possible.  When we are starting a job we calculate how many images 
there are, and whether they should all be able to fit into memory.  If they 
should but don"t, then we have something preventing it, so we start looking at 
the other allocations going on.  These are things like half-tone caches, font 
caches etc.  First there is the question of whether everything for a page that 
should fit in can fit in.  If not, we have to look at how things are being 
compressed or moved around, and how the different allocations are interleaving 
to prevent it fitting.  

You mentioned various levels of debug information that can be compiled in; can 
you say more?
 
Simplest level is allocation and de-allocation count.  This gives the total 
amount of memory that has been allocated, or is currently allocated to each of 
the pools, along with the total numbers of allocations and de-allocations; (we 
deal with various pools).  Next comes the fence-posting code which will add 
fence-posting to our allocations and check them on allocation or de-allocation 
operations.  This checks for array overruns etc.  The final level is the 
tagging were every allocation is tagged with the line and file-number of where 
is happened, and the actual tag represents a separate memory pool controlled by 
the system rather than by our own code.  It should thus be in an entirely 
different area of memory and should not be affected by any problems affecting 
our own allocations.  

This gives us a "current state of memory" dump if we want it at a given point.  
Where all the allocations came from, which we can use to see which function was 
making the allocation.  It would be better to see that info. structured in a 
more sensible way, such as "here are the fonts allocations" - this would 
effectively give us the full structure of the font cache.  We do not have that 
level of sophistication yet.  

At this level of compilation we also get "asserts" which run over the tag pool 
and check when you are freeing things that you have not freed it already, and 
that it is a valid piece of memory that you are freeing, and that you do not 
have a bad pointer.  

If we hit any of these when the program is running, we can either ignore it, 
drop into the debugger, or abort at that point in order to examine the state. 

Thinking about possible MM products. 

For a gain in performance or size, do you want a product "out of the box" or 
tuneable?  

Something tuneable.  The customers use the RIP in many different ways.  Some 
have very large machines and performance is all, but they tend to have a 
"typical" type of job.  They might have several images at device resolution 
which have to be done very fast.  Thus the memory issues related to image 
handling are most important.  Other customers might work with Quark or 
Illustrator jobs, or even text handling.  For them the issues to do with 
something else might  be important.  Tunability is thus important, in relation 
to such things as cache sizes which are normally initialised to "average" 
values.  We and OEMs may want to have some control over these to configure the 
system for specific types of job.  This would also be useful for "problem" 
jobs, if it allowed you to use an unusual configuration to just squeeze the job 
through, and then re-set to normal.  

Would you value a range of MM policies to suit the jobs at hand? 

Yes, but it is hard to answer without knowing the scale, or how these would be 
managed, or how they might interact within a MM product.  We have certain 
allocations, for example, which we know are going to be long-lasting, in effect 
static allocations.  It would seem inappropriate to use the same allocator for 
them as for frequent and very temporary allocations.  We could thus use some 
method for hinting about whether these were going to be long-lived or not, or 
whether they were going to be de-allocated quickly.  In the RIP we tend to do 
multiple allocations at once, to incrementally allocate stuff.  We then throw 
away large blocks at once.  This is true for display lists, and for path 
allocation.  It is used "as one" and thrown away "as one".  We could tell the 
manager what an allocation"s expected pattern of usage should be;. i.e. it is 
unlikely to be de-allocated quickly, but when it does go, all these others will 
go also.  
Would it help to have precise feedback on how a given policy affected your 
code?  
This is something being developed for the EP 2000 project.  There is talk of 
the Adaptive Systems Group providing "Intelligent Front Ends", and if we can 
feed them information saying e.g. "we are trying this strategy", and "here is 
what the MM says happens", they might be able to say "this does not work very 
well, try something else".  If we can get "information" rather than "data" it 
would be most useful, and could be combined with all our other feedback from 
other points in the process. 

Do you have any reservations about using MM products? 

None at all, except that they should work, and be cross-platform.  At the 
moment we have lost some of our previous debugging ability because of the way 
that the layering is happening.  We will get this back, but.  I am keenly aware 
that if we get problems right now, we would have fewer resources to track to 
problems than we would like.  Certainly, tools which give us statistical 
instrumentation from the MM would be very useful, because often we are guessing 
about things like cache size and expected usage patterns, and it would be great 
to get the information without having to do all the instrumentation ourselves.  
This applies to sizes and frequency of allocation, patterns of usage, - 
everything.  Anything which helps to get a handle on these things would be 
excellent, as long as it did not hit performance in the eventual product.  We 
could work with a "debug" version that was slower, but API identical. 

Would this apply for the rest of your team? 

Yes - we would probably not be using these on a daily basis, but to tackle 
problems like a customer who has cache thrashing.  If we could do a little 
instrumentation and get the data out would it  be most useful.  Applying the 
instrumentation should thus be easy, and take little effort. 

Would you want your users (the OEMs) to have access to the MM tools?  Or even 
the end users?

Not in a direct way, with access to full debugging and contents of memory 
basis.  Certainly not.  In terms of tunability etc., I think that we would want 
to keep control over that.  I would not want to see a direct interface from the 
user to the MM.  However, we might want to give them some chance of input, but 
only under our own control, or through a subsidiary interface. 

Anything else to add that we have not covered? 

No.

ATTACHMENT
   "Report 1"

