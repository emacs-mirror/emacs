                   AUTOMATIC WEAK LINKED POOL CLASSES
                        impl.c.poolawl.sunflower
                               draft doc
                           gavinm 1997-08-06

     1 /* impl.c.poolawl: AUTOMATIC WEAK LINKED POOL CLASS
     2  *
     3  * $HopeName: MMsrc!poolawl.c(MM_dylan_sunflower.9) $
     4  * Copyright (C) 1997 The Harlequin Group Limited.  All rights reserved.
     5  *
     6  * READERSHIP
     7  *
     8  * .readership: Any MPS developer.
     9  *
    10  * DESIGN
    11  *
    12  * .design: design.mps.poolawl
    13  */
    14 
    15 
    16 #include "mpm.h"
    17 #include "mpscawl.h"
    18 
    19 SRCID(poolawl, "$HopeName: MMsrc!poolawl.c(MM_dylan_sunflower.9) $");
    20 
    21 
    22 #define AWLSig ((Sig)0x519b7a37) /* SIGPooLAWL */
    23 
    24 /* design.mps.poolawl.poolstruct */
    25 typedef struct AWLStruct {
    26   PoolStruct poolStruct;
    27   Format format;
    28   Shift alignShift;
    29   ActionStruct actionStruct;
    30   double lastCollected;
    31   Sig sig;
    32 } AWLStruct, *AWL;
    33 
    34 #define AWLGroupSig ((Sig)0x519a379b) /* SIGAWLGrouP */
    35 
    36 /* design.mps.poolawl.group */
    37 typedef struct AWLGroupStruct {
    38   Sig sig;
    39   Seg seg;
    40   BT mark;
    41   BT scanned;
    42   BT alloc;
    43   RefSet rememberedSummary; /* == EMPTY unless seg is condemned */
    44 } AWLGroupStruct, *AWLGroup;
    45 
    46 
    47 static Bool AWLCheck(AWL awl);
    48 static Bool AWLGroupCheck(AWLGroup group);
    49 
    50 
    51 /* PoolPoolAWL -- convert generic Pool to AWL */
    52 
    53 #define PoolPoolAWL(pool) \
    54   PARENT(AWLStruct, poolStruct, (pool))
    55 
    56 #define ActionAWL(action) PARENT(AWLStruct, actionStruct, action)
    57 
    58 
    59 static void AWLGroupDestroy(AWLGroup group)
    60 {
    61   AWL awl;
    62   Pool pool;
    63   Seg seg;
    64   Size tableSize;
    65   Space space;
    66 
    67   seg = group->seg;
    68   pool = SegPool(seg);
    69   AVERT(Pool, pool);
    70   awl = PoolPoolAWL(pool);
    71   AVERT(AWL, awl);
    72   space = PoolSpace(pool);
    73   AVERT(Space, space);
    74   tableSize = BTSize(SegSize(space, seg) >> awl->alignShift);
    75   PoolSegFree(pool, seg);
    76   SpaceFree(space, (Addr)group->mark, tableSize);
    77   SpaceFree(space, (Addr)group->scanned, tableSize);
    78   SpaceFree(space, (Addr)group->alloc, tableSize);
    79   group->sig = SigInvalid;
    80   SpaceFree(space, (Addr)group, sizeof *group);
    81 }
    82   
    83  
    84 static Res AWLGroupCreate(AWLGroup *groupReturn,
    85                           Buffer buffer, Pool pool, Size size)
    86 {
    87   AWL awl;
    88   Seg seg;
    89   AWLGroup group;
    90   void *v;
    91   Count bits; /* number of grains */
    92   Res res;
    93   Size tableSize;
    94   Space space;
    95 
    96   AVER(groupReturn != NULL);
    97   AVERT(Pool, pool);
    98   AVER(size > 0);
    99 
   100   awl = PoolPoolAWL(pool);
   101   AVERT(AWL, awl);
   102 
   103   space = PoolSpace(pool);
   104   AVERT(Space, space);
   105 
   106   size = SizeAlignUp(size, ArenaAlign(space));
   107   /* beware of large sizes overflowing upon rounding */
   108   if(size == 0) {
   109     return ResMEMORY;
   110   }
   111   res = PoolSegAlloc(&seg, SegPrefDefault(), pool, size);
   112   if(res != ResOK)
   113     goto failSegAlloc;
   114   res = SpaceAlloc(&v, space, sizeof *group);
   115   if(res != ResOK)
   116     goto failSpaceAlloc0;
   117   group = v;
   118   bits = size >> awl->alignShift;
   119   tableSize = BTSize(bits);
   120   res = SpaceAlloc(&v, space, tableSize);
   121   if(res != ResOK)
   122     goto failSpaceAlloc1;
   123   group->mark = v;
   124   res = SpaceAlloc(&v, space, tableSize);
   125   if(res != ResOK)
   126     goto failSpaceAlloc2;
   127   group->scanned = v;
   128   res = SpaceAlloc(&v, space, tableSize);
   129   if(res != ResOK)
   130     goto failSpaceAlloc3;
   131   group->alloc = v;
   132   BTResRange(group->mark, 0, bits);
   133   BTResRange(group->scanned, 0, bits);
   134   BTResRange(group->alloc, 0, bits);
   135   SegSetSummary(seg, RefSetUNIV);
   136   SegSetRankSet(seg, BufferRankSet(buffer));
   137   SegSetP(seg, group);
   138   group->rememberedSummary = RefSetEMPTY;
   139   group->seg = seg;
   140   group->sig = AWLGroupSig;
   141   AVERT(AWLGroup, group);
   142   *groupReturn = group;
   143   return ResOK;
   144 
   145 failSpaceAlloc3:
   146   SpaceFree(space, (Addr)group->scanned, tableSize);
   147 failSpaceAlloc2:
   148   SpaceFree(space, (Addr)group->mark, tableSize);
   149 failSpaceAlloc1:
   150   SpaceFree(space, (Addr)group, sizeof *group);
   151 failSpaceAlloc0:
   152   PoolSegFree(pool, seg);
   153 failSegAlloc:
   154   return res;
   155 }
   156 
   157 
   158 static Bool AWLGroupAlloc(Addr *baseReturn, Addr *limitReturn,
   159                           AWLGroup group, AWL awl, Size size)
   160 {
   161   Count n, bits;
   162   Index i, j;
   163   Space space;
   164 
   165   AVER(baseReturn != NULL);
   166   AVER(limitReturn != NULL);
   167   AVERT(AWLGroup, group);
   168   AVERT(AWL, awl);
   169   AVER(size > 0);
   170 
   171   space = PoolSpace(&awl->poolStruct);
   172   AVERT(Space, space);
   173 
   174 
   175   if(size > SegSize(space, group->seg)) {
   176     return FALSE;
   177   }
   178   bits = SegSize(space, group->seg) >> awl->alignShift;
   179   n = size >> awl->alignShift;
   180   if(BTFindResRange(&i, &j, group->alloc, 0, bits, n)) {
   181     *baseReturn = AddrAdd(SegBase(space, group->seg), i << 
awl->alignShift);
   182     *limitReturn = AddrAdd(SegBase(space, group->seg), j << 
awl->alignShift);
   183     return TRUE;
   184   }
   185   return FALSE;
   186 }
   187 
   188 
   189 static Res AWLInit(Pool pool, va_list arg)
   190 {
   191   AWL awl;
   192   Format format;
   193 
   194   awl = PoolPoolAWL(pool);
   195 
   196   format = va_arg(arg, Format);
   197 
   198   AVERT(Format, format);
   199   awl->format = format;
   200   pool->alignment = format->alignment;
   201   awl->alignShift = SizeLog2(pool->alignment);
   202   ActionInit(&awl->actionStruct, pool);
   203   awl->lastCollected = PoolSpace(pool)->allocTime;
   204   awl->sig = AWLSig;
   205 
   206   AVERT(AWL, awl);
   207 
   208   return ResOK;
   209 }
   210 
   211 
   212 static void AWLFinish(Pool pool)
   213 {
   214   AWL awl;
   215   Ring ring, node;
   216 
   217   /* pool argument already checked by generic PoolFinish */
   218 
   219   awl = PoolPoolAWL(pool);
   220   AVERT(AWL, awl);
   221 
   222   ring = &pool->segRing;
   223   node = RingNext(ring);
   224   while(node != ring) {
   225     Ring next = RingNext(node);
   226     Seg seg = SegOfPoolRing(node);
   227     AWLGroup group;
   228 
   229     AVERT(Seg, seg);
   230     group = (AWLGroup)SegP(seg);
   231     AVERT(AWLGroup, group);
   232     AWLGroupDestroy(group);
   233     node = next;
   234   }
   235   ActionFinish(&awl->actionStruct);
   236 }
   237 
   238 
   239 static Res AWLBufferFill(Seg *segReturn, Addr *baseReturn, Addr 
*limitReturn,
   240                          Pool pool, Buffer buffer, Size size)
   241 {
   242   Addr base, limit;
   243   AWLGroup group;
   244   AWL awl;
   245   Res res;
   246   Ring node;
   247   Space space;
   248 
   249   AVER(baseReturn != NULL);
   250   AVERT(Pool, pool);
   251   AVERT(Buffer, buffer);
   252   AVER(size > 0);
   253 
   254   space = PoolSpace(pool);
   255 
   256   awl = PoolPoolAWL(pool);
   257   AVERT(AWL, awl);
   258 
   259   RING_FOR(node, &pool->segRing) {
   260     Seg seg;
   261 
   262     seg = SegOfPoolRing(node);
   263     AVERT(Seg, seg);
   264     group = (AWLGroup)SegP(seg);
   265     AVERT(AWLGroup, group);
   266 
   267     if(SegBuffer(seg) == NULL &&
   268        SegRankSet(seg) == BufferRankSet(buffer)) {
   269       if(AWLGroupAlloc(&base, &limit, group, awl, size)) {
   270  goto found;
   271       }
   272     }
   273   }
   274 
   275   /* No free space in existing groups, so create new group */
   276 
   277   res = AWLGroupCreate(&group, buffer, pool, size);
   278   if(res != ResOK) {
   279     return res;
   280   }
   281   base = SegBase(space, group->seg);
   282   limit = SegLimit(space, group->seg);
   283 
   284 found:
   285   SegSetSummary(group->seg, RefSetUNIV);
   286   {
   287     Index i, j;
   288     i = AddrOffset(SegBase(space, group->seg), base) >> awl->alignShift;
   289     j = AddrOffset(SegBase(space, group->seg), limit) >> awl->alignShift;
   290     BTSetRange(group->alloc, i, j);
   291 
   292     /* Objects allocated must be black.  See */
   293     /* change.dylan.sunflower.7.170467. */
   294     BTSetRange(group->mark, i, j);
   295     BTSetRange(group->scanned, i, j);
   296   }
   297   *segReturn = group->seg;
   298   *baseReturn = base;
   299   *limitReturn = limit;
   300   return ResOK;
   301 }
   302 
   303 
   304 static void AWLBufferEmpty(Pool pool, Buffer buffer)
   305 {
   306   AWL awl;
   307   AWLGroup group;
   308   Addr segBase;
   309   Index i, j;
   310 
   311   AVERT(Pool, pool);
   312   AVERT(Buffer, buffer);
   313 
   314   awl = PoolPoolAWL(pool);
   315   AVERT(AWL, awl);
   316   group = (AWLGroup)SegP(BufferSeg(buffer));
   317   AVERT(AWLGroup, group);
   318 
   319   segBase = SegBase(PoolSpace(pool), BufferSeg(buffer));
   320 
   321   i = AddrOffset(segBase, BufferGetInit(buffer)) >> awl->alignShift;
   322   j = AddrOffset(segBase, BufferLimit(buffer)) >> awl->alignShift;
   323   AVER(i <= j);
   324   if(i < j) {
   325     BTResRange(group->alloc, i, j);
   326   }
   327 }
   328 
   329 
   330 static Res AWLCondemn(Pool pool, Trace trace, Seg seg, Action action)
   331 {
   332   Count bits;
   333   AWL awl;
   334   AWLGroup group;
   335 
   336   /* all parameters checked by generic PoolCondemn */
   337 
   338   /* can only condemn for a single trace, */
   339   /* see design.mps.poolawl.fun.condemn */
   340   AVER(SegWhite(seg) == TraceSetEMPTY);
   341 
   342   /* Don't condemn buffered segments, to avoid allocating non-black */
   343   /* objects.  See change.dylan.sunflower.7.170467. */
   344   if(SegBuffer(seg) == NULL) {
   345     awl = PoolPoolAWL(pool);
   346     AVERT(AWL, awl);
   347     AVERT(Action, action);
   348     AVER(awl == ActionAWL(action));
   349 
   350     group = (AWLGroup)SegP(seg);
   351     AVERT(AWLGroup, group);
   352     bits = SegSize(PoolSpace(pool), seg) >> awl->alignShift;
   353     
   354     BTResRange(group->mark, 0, bits);
   355     BTResRange(group->scanned, 0, bits);
   356     group->rememberedSummary = SegSummary(seg);
   357     SegSetWhite(seg, TraceSetAdd(SegWhite(seg), trace->ti));
   358   }
   359   
   360   return ResOK;
   361 }
   362 
   363 static void AWLGrey(Pool pool, Trace trace, Seg seg)
   364 {
   365   /* parameters checkd by generic PoolGrey */
   366   if(!TraceSetIsMember(SegWhite(seg), trace->ti)) {
   367     AWL awl;
   368     AWLGroup group;
   369     Count bits;
   370 
   371     awl = PoolPoolAWL(pool);
   372     AVERT(AWL, awl);
   373     group = (AWLGroup)SegP(seg);
   374     AVERT(AWLGroup, group);
   375 
   376     TraceSegGreyen(PoolSpace(pool), seg, TraceSetSingle(trace->ti));
   377     bits = SegSize(PoolSpace(pool), seg) >> awl->alignShift;
   378     BTSetRange(group->mark, 0, bits);
   379     BTResRange(group->scanned, 0, bits);
   380   }
   381 }
   382 
   383 
   384 /* Returns the linked object (or possibly there is none) */
   385 static Bool AWLDependentObject(Addr *objReturn, Addr parent)
   386 {
   387   Word *object;
   388   Word *wrapper;
   389   Word fword;
   390   Word fl;
   391   Word ff;
   392 
   393   AVER(objReturn != NULL);
   394   AVER(parent != (Addr)0);
   395 
   396   object = (Word *)parent;
   397   wrapper = (Word *)object[0];
   398   AVER(wrapper != NULL);
   399   /* check wrapper wrapper is non-NULL */
   400   AVER(wrapper[0] != 0);
   401   /* check wrapper wrapper is wrapper wrapper wrapper */
   402   AVER(wrapper[0] == ((Word *)wrapper[0])[0]);
   403   fword = wrapper[2];
   404   ff = fword & 3;
   405   /* Traceable Fixed part */
   406   AVER(ff == 1);
   407   fl = fword & ~3uL;
   408   /* At least one fixed field */
   409   AVER(fl >= 1);
   410   if(object[1] == 0) {
   411     return FALSE;
   412   }
   413   *objReturn = (Addr)object[1];
   414   return TRUE;
   415 }
   416 
   417 
   418 static Res AWLScan(ScanState ss, Pool pool, Seg seg)
   419 {
   420   Addr base, limit;
   421   Addr p;
   422   AWL awl;
   423   AWLGroup group;
   424   Bool finished;
   425   Count bits;
   426   Space space;
   427 
   428   /* parameters checked by generic PoolScan */
   429 
   430   group = (AWLGroup)SegP(seg);
   431   AVERT(AWLGroup, group);
   432 
   433   awl = PoolPoolAWL(pool);
   434   AVERT(AWL, awl);
   435 
   436   space = PoolSpace(pool);
   437   bits = SegSize(space, seg) >> awl->alignShift;
   438   base = SegBase(space, seg);
   439   limit = SegLimit(space, seg);
   440 
   441 notFinished:
   442   finished = TRUE;
   443   p = base;
   444   while(p < limit) {
   445     Index i; /* the index into the bit tables corresponding to p */
   446     Addr objectEnd;
   447 
   448     AVER(AddrIsAligned(p, pool->alignment));
   449 
   450     /* design.mps.poolawl.fun.scan.buffer */
   451     if(SegBuffer(seg)) {
   452       Buffer buffer = SegBuffer(seg);
   453       /* Only skip the buffer area if it is non-zero in length. */
   454       /* See change.dylan.sunflower.7.170463. */
   455       if(p == BufferScanLimit(buffer) &&
   456          BufferScanLimit(buffer) != BufferLimit(buffer)) {
   457  p = BufferLimit(buffer);
   458  continue;
   459       }
   460     }
   461     i = AddrOffset(base, p) >> awl->alignShift;
   462     /* design.mps.poolawl.fun.scan.free */
   463     if(!BTGet(group->alloc, i)) {
   464       p = AddrAdd(p, pool->alignment);
   465       continue;
   466     }
   467     /* design.mps.poolawl.fun.scan.object-end */
   468     objectEnd = awl->format->skip(p);
   469     /* design.mps.poolawl.fun.scan.scan */
   470     if(BTGet(group->mark, i) && !BTGet(group->scanned, i)) {
   471       Addr dependentObj;
   472       Seg dependentSeg;
   473       Bool dependent;
   474       Res res;
   475 
   476       finished = FALSE;
   477       BTSet(group->scanned, i);
   478       /* is there a dependent object that needs exposing? */
   479       dependent = AWLDependentObject(&dependentObj, p);
   480       if(dependent) {
   481  Bool b;
   482 
   483  b = SegOfAddr(&dependentSeg, space, dependentObj);
   484  if(b == TRUE) {
   485    ShieldExpose(space, dependentSeg);
   486    TraceSetSummary(space, dependentSeg, RefSetUNIV);
   487  } else {
   488    dependent = FALSE;
   489  }
   490       }
   491       res = awl->format->scan(ss, p, objectEnd);
   492       if(dependent) {
   493         ShieldCover(space, dependentSeg);
   494       }
   495       if(res != ResOK) {
   496         return res;
   497       }
   498     }
   499     p = objectEnd;
   500   }
   501   if(!finished)
   502     goto notFinished;
   503   
   504   return ResOK;
   505 }
   506 
   507 
   508 static Res AWLFix(Pool pool, ScanState ss, Seg seg, Ref *refIO)
   509 {
   510   Ref ref;
   511   Index i;
   512   AWL awl;
   513   AWLGroup group;
   514   Space space;
   515 
   516   AVERT(Pool, pool);
   517   AVERT(ScanState, ss);
   518   AVERT(Seg, seg);
   519   AVER(TraceSetInter(SegWhite(seg), ss->traces) != TraceSetEMPTY);
   520   AVER(refIO != NULL);
   521 
   522   awl = PoolPoolAWL(pool);
   523   AVERT(AWL, awl);
   524   group  = (AWLGroup)SegP(seg);
   525   AVERT(AWLGroup, group);
   526 
   527   space = PoolSpace(pool);
   528   AVERT(Space, space);
   529 
   530   ref = *refIO;
   531   i = AddrOffset(SegBase(space, seg), ref) >> awl->alignShift;
   532   
   533   ss->wasMarked = TRUE;
   534 
   535   switch(ss->rank) {
   536   case RankAMBIG:
   537     /* not a real pointer if not aligned or not allocated */
   538     if(!AddrIsAligned((Addr)ref, pool->alignment) ||
   539        !BTGet(group->alloc, i)) {
   540       return ResOK;
   541     }
   542   /* falls through */
   543   case RankEXACT:
   544   case RankFINAL:
   545   case RankWEAK:
   546     if(!BTGet(group->mark, i)) {
   547       ss->wasMarked = FALSE;
   548       if(ss->rank == RankWEAK) {
   549  *refIO = (Ref)0;
   550       } else {
   551  TraceSetSummary(space, seg,
   552                         RefSetUnion(group->rememberedSummary,
   553                SegSummary(seg)));
   554  TraceSegGreyen(space, seg, ss->traces);
   555  BTSet(group->mark, i);
   556       }
   557     }
   558     break;
   559   
   560   default:
   561     NOTREACHED;
   562     return ResUNIMPL;
   563   }
   564 
   565   return ResOK;
   566 }
   567 
   568 
   569 static void AWLReclaim(Pool pool, Trace trace, Seg seg)
   570 {
   571   Addr base;
   572   AWL awl;
   573   AWLGroup group;
   574   Count bits;
   575   Index i;
   576   Space space;
   577 
   578   /* parameters checked by generic PoolReclaim */
   579 
   580   awl = PoolPoolAWL(pool);
   581   AVERT(AWL, awl);
   582   group = (AWLGroup)SegP(seg);
   583   AVERT(AWLGroup, group);
   584 
   585   space = PoolSpace(pool);
   586   AVERT(Space, space);
   587 
   588   bits = SegSize(space, seg) >> awl->alignShift;
   589   base = SegBase(space, seg);
   590 
   591   i = 0;
   592   while(i < bits) {
   593     Addr p;
   594     Index j;
   595     if(!BTGet(group->alloc, i)) {
   596       ++i;
   597       continue;
   598     }
   599     p = AddrAdd(base, i << awl->alignShift);
   600     if(SegBuffer(seg) != NULL) {
   601       Buffer buffer = SegBuffer(seg);
   602       AVER(p <= BufferScanLimit(buffer) ||
   603            BufferLimit(buffer) <= p);
   604       /* Only skip the buffer area if it is non-zero in length. */
   605       /* See change.dylan.sunflower.7.170463. */
   606       if(p == BufferScanLimit(buffer) &&
   607          BufferScanLimit(buffer) != BufferLimit(buffer)) {
   608         p = BufferLimit(buffer);
   609  i = AddrOffset(base, p) >> awl->alignShift;
   610         continue;
   611       }
   612     }
   613     j = AddrOffset(base, awl->format->skip(p)) >>
   614         awl->alignShift;
   615     AVER(j <= bits);
   616     if(!BTGet(group->mark, i)) {
   617       BTResRange(group->alloc, i, j);
   618     }
   619     i = j;
   620   }
   621   AVER(i == bits);
   622 
   623   BTResRange(group->mark, 0, bits);
   624   group->rememberedSummary = RefSetEMPTY;
   625   SegSetWhite(seg, TraceSetDel(SegWhite(seg), trace->ti));
   626 }
   627 
   628 static Res AWLTraceBegin(Pool pool, Trace trace, Action action)
   629 {
   630   AWL awl;
   631 
   632   AVERT(Pool, pool);
   633   awl = PoolPoolAWL(pool);
   634   AVERT(AWL, awl);
   635   AVERT(Trace, trace);
   636   AVERT(Action, action);
   637   AVER(awl == ActionAWL(action));
   638 
   639   awl->lastCollected = PoolSpace(pool)->allocTime;
   640   return ResOK;
   641 }
   642 
   643 /* @@@@ completely made-up benefit calculation: each AWL pool gradually
   644  * becomes a better candidate for collection as allocation goes
   645  * by. Starting a trace on a pool makes it a bad candidate. nickb
   646  * 1997-06-19 */
   647 
   648 static double AWLBenefit(Pool pool, Action action)
   649 {
   650   AWL awl;
   651 
   652   AVERT(Pool, pool);
   653   awl = PoolPoolAWL(pool);
   654   AVERT(AWL, awl);
   655   AVERT(Action, action);
   656   AVER(awl == ActionAWL(action));
   657 
   658   return (PoolSpace(pool)->allocTime - awl->lastCollected) - 
10*1024*1024.0;
   659 }
   660 
   661 
   662 struct PoolClassStruct PoolClassAWLStruct = {
   663   PoolClassSig,
   664   "AWL",
   665   sizeof(AWLStruct),
   666   offsetof(AWLStruct, poolStruct),
   667   AttrFMT | AttrSCAN | AttrBUF | AttrBUF_RESERVE | AttrGC | AttrINCR_RB,
   668   AWLInit,
   669   AWLFinish,
   670   PoolNoAlloc,
   671   PoolNoFree,
   672   PoolTrivBufferInit,
   673   AWLBufferFill,
   674   AWLBufferEmpty,
   675   PoolTrivBufferFinish,
   676   AWLTraceBegin,
   677   AWLCondemn,
   678   AWLGrey,
   679   AWLScan,
   680   AWLFix,
   681   AWLReclaim,
   682   PoolTrivTraceEnd,
   683   AWLBenefit,
   684   PoolTrivDescribe,
   685   PoolClassSig
   686 };
   687 
   688 
   689 mps_class_t mps_class_awl(void)
   690 {
   691   return (mps_class_t)&PoolClassAWLStruct;
   692 }
   693 
   694 
   695 static Bool AWLCheck(AWL awl)
   696 {
   697   CHECKS(AWL, awl);
   698   CHECKD(Pool, &awl->poolStruct);
   699   CHECKL(awl->poolStruct.class == &PoolClassAWLStruct);
   700   CHECKL(1uL << awl->alignShift == awl->poolStruct.alignment);
   701   CHECKD(Action, &awl->actionStruct);
   702   CHECKL(awl->poolStruct.space->allocTime >= awl->lastCollected);
   703   return TRUE;
   704 }
   705 
   706 
   707 static Bool AWLGroupCheck(AWLGroup group)
   708 {
   709   CHECKS(AWLGroup, group);
   710   CHECKL(SegCheck(group->seg));
   711   CHECKL(group->mark != NULL);
   712   CHECKL(group->scanned != NULL);
   713   CHECKL(group->alloc != NULL);
   714   /* Can't check rememberedSummary because it's a RefSet */
   715   CHECKL(group->rememberedSummary == RefSetEMPTY ||
   716          SegWhite(group->seg) != TraceSetEMPTY);
   717   return TRUE;
   718 }

