         IMPLEMENTATION OF THE AUTOMATIC WEAK LINKED POOL CLASS
                             impl.c.poolawl
                               draft doc
                           richard 1997-07-23

     1 /* impl.c.poolawl: AUTOMATIC WEAK LINKED POOL CLASS
     2  *
     3  * $HopeName: MMsrc!poolawl.c(trunk.11) $
     4  * Copyright (C) 1997 The Harlequin Group Limited.  All rights reserved.
     5  *
     6  * READERSHIP
     7  *
     8  * .readership: Any MPS developer.
     9  *
    10  * DESIGN
    11  *
    12  * .design: design.mps.poolawl
    13  */
    14 
    15 
    16 #include "mpm.h"
    17 #include "mpscawl.h"
    18 
    19 SRCID(poolawl, "$HopeName: MMsrc!poolawl.c(trunk.11) $");
    20 
    21 
    22 #define AWLSig ((Sig)0x519b7a37) /* SIGPooLAWL */
    23 
    24 /* design.mps.poolawl.poolstruct */
    25 typedef struct AWLStruct {
    26   PoolStruct poolStruct;
    27   Format format;
    28   Shift alignShift;
    29   ActionStruct actionStruct;
    30   double lastCollected;
    31   Sig sig;
    32 } AWLStruct, *AWL;
    33 
    34 #define AWLGroupSig ((Sig)0x519a379b) /* SIGAWLGrouP */
    35 
    36 /* design.mps.poolawl.group */
    37 typedef struct AWLGroupStruct {
    38   Sig sig;
    39   Seg seg;
    40   BT mark;
    41   BT scanned;
    42   BT alloc;
    43   Count grains;
    44 } AWLGroupStruct, *AWLGroup;
    45 
    46 
    47 static Bool AWLCheck(AWL awl);
    48 static Bool AWLGroupCheck(AWLGroup group);
    49 
    50 
    51 /* PoolPoolAWL -- convert generic Pool to AWL */
    52 
    53 #define PoolPoolAWL(pool) \
    54   PARENT(AWLStruct, poolStruct, (pool))
    55 
    56 /* ActionAWL -- converts action to the enclosing AWL */
    57 
    58 #define ActionAWL(action) PARENT(AWLStruct, actionStruct, action)
    59 
    60 
    61 static void AWLGroupDestroy(AWLGroup group)
    62 {
    63   AWL awl;
    64   Pool pool;
    65   Seg seg;
    66   Size tableSize;
    67   Space space;
    68   Count segGrains;
    69 
    70   AVERT(AWLGroup, group);
    71   seg = group->seg;
    72   pool = SegPool(seg);
    73   AVERT(Pool, pool);
    74   awl = PoolPoolAWL(pool);
    75   AVERT(AWL, awl);
    76   space = PoolSpace(pool);
    77   AVERT(Space, space);
    78 
    79   /* This is one of the few places where it is easy to check */
    80   /* group->grains, so we do */
    81   segGrains = SegSize(space, seg) >> awl->alignShift;
    82   AVER(segGrains == group->grains);
    83   tableSize = BTSize(segGrains);
    84   PoolSegFree(pool, seg);
    85   SpaceFree(space, (Addr)group->alloc, tableSize);
    86   SpaceFree(space, (Addr)group->scanned, tableSize);
    87   SpaceFree(space, (Addr)group->mark, tableSize);
    88   group->sig = SigInvalid;
    89   SpaceFree(space, (Addr)group, sizeof *group);
    90 }
    91   
    92  
    93 static Res AWLGroupCreate(AWLGroup *groupReturn,
    94                           Buffer buffer, Pool pool, Size size)
    95 {
    96   AWL awl;
    97   Seg seg;
    98   AWLGroup group;
    99   void *v;
   100   Count bits; /* number of grains */
   101   Res res;
   102   Size tableSize;
   103   Space space;
   104 
   105   AVER(groupReturn != NULL);
   106   AVERT(Buffer, buffer);
   107   AVERT(Pool, pool);
   108   AVER(size > 0);
   109 
   110   awl = PoolPoolAWL(pool);
   111   AVERT(AWL, awl);
   112 
   113   space = PoolSpace(pool);
   114   AVERT(Space, space);
   115 
   116   size = SizeAlignUp(size, ArenaAlign(space));
   117   /* beware of large sizes overflowing upon rounding */
   118   if(size == 0) {
   119     return ResMEMORY;
   120   }
   121   res = PoolSegAlloc(&seg, SegPrefDefault(), pool, size);
   122   if(res != ResOK)
   123     goto failSegAlloc;
   124   res = SpaceAlloc(&v, space, sizeof *group);
   125   if(res != ResOK)
   126     goto failSpaceAlloc0;
   127   group = v;
   128   bits = size >> awl->alignShift;
   129   tableSize = BTSize(bits);
   130   res = SpaceAlloc(&v, space, tableSize);
   131   if(res != ResOK)
   132     goto failSpaceAllocMark;
   133   group->mark = v;
   134   res = SpaceAlloc(&v, space, tableSize);
   135   if(res != ResOK)
   136     goto failSpaceAllocScanned;
   137   group->scanned = v;
   138   res = SpaceAlloc(&v, space, tableSize);
   139   if(res != ResOK)
   140     goto failSpaceAllocAlloc;
   141   group->alloc = v;
   142   group->grains = bits;
   143   BTResRange(group->mark, 0, bits);
   144   BTResRange(group->scanned, 0, bits);
   145   BTResRange(group->alloc, 0, bits);
   146   SegSetSummary(seg, RefSetUNIV);
   147   SegSetRankSet(seg, BufferRankSet(buffer));
   148   SegSetP(seg, group);
   149   group->seg = seg;
   150   group->sig = AWLGroupSig;
   151   AVERT(AWLGroup, group);
   152   *groupReturn = group;
   153   return ResOK;
   154 
   155 failSpaceAllocAlloc:
   156   SpaceFree(space, (Addr)group->scanned, tableSize);
   157 failSpaceAllocScanned:
   158   SpaceFree(space, (Addr)group->mark, tableSize);
   159 failSpaceAllocMark:
   160   SpaceFree(space, (Addr)group, sizeof *group);
   161 failSpaceAlloc0:
   162   PoolSegFree(pool, seg);
   163 failSegAlloc:
   164   return res;
   165 }
   166 
   167 
   168 static Bool AWLGroupAlloc(Addr *baseReturn, Addr *limitReturn,
   169                           AWLGroup group, AWL awl, Size size)
   170 {
   171   Count n; /* number of grains equivalent to alloc size */
   172   Index i, j;
   173   Space space;
   174 
   175   AVER(baseReturn != NULL);
   176   AVER(limitReturn != NULL);
   177   AVERT(AWLGroup, group);
   178   AVERT(AWL, awl);
   179   AVER(size > 0);
   180 
   181   space = PoolSpace(&awl->poolStruct);
   182   AVERT(Space, space);
   183 
   184 
   185   if(size > SegSize(space, group->seg)) {
   186     return FALSE;
   187   }
   188   n = size >> awl->alignShift;
   189   if(!BTFindLongResRange(&i, &j, group->alloc, 0, group->grains, n)) {
   190     return FALSE;
   191   }
   192   *baseReturn = AddrAdd(SegBase(space, group->seg), i << 
awl->alignShift);
   193   *limitReturn = AddrAdd(SegBase(space, group->seg), j << 
awl->alignShift);
   194   return TRUE;
   195 }
   196 
   197 
   198 static Res AWLInit(Pool pool, va_list arg)
   199 {
   200   AWL awl;
   201   Format format;
   202 
   203   /* weak check, as half way through initialization */
   204   AVER(pool != NULL);
   205 
   206   awl = PoolPoolAWL(pool);
   207 
   208   format = va_arg(arg, Format);
   209 
   210   AVERT(Format, format);
   211   awl->format = format;
   212   awl->alignShift = SizeLog2(pool->alignment);
   213   ActionInit(&awl->actionStruct, pool);
   214   awl->lastCollected = PoolSpace(pool)->allocTime;
   215   awl->sig = AWLSig;
   216 
   217   AVERT(AWL, awl);
   218 
   219   return ResOK;
   220 }
   221 
   222 
   223 static void AWLFinish(Pool pool)
   224 {
   225   AWL awl;
   226   Ring ring, node;
   227 
   228   AVERT(Pool, pool);
   229 
   230   awl = PoolPoolAWL(pool);
   231   AVERT(AWL, awl);
   232 
   233   ring = &pool->segRing;
   234   node = RingNext(ring);
   235   while(node != ring) {
   236     Ring next = RingNext(node);
   237     Seg seg = SegOfPoolRing(node);
   238     AWLGroup group;
   239 
   240     AVERT(Seg, seg);
   241     group = (AWLGroup)SegP(seg);
   242     AVERT(AWLGroup, group);
   243     AWLGroupDestroy(group);
   244     node = next;
   245   }
   246   ActionFinish(&awl->actionStruct);
   247 }
   248 
   249 
   250 static Res AWLBufferFill(Seg *segReturn, Addr *baseReturn, Addr 
*limitReturn,
   251                          Pool pool, Buffer buffer, Size size)
   252 {
   253   Addr base, limit;
   254   AWLGroup group;
   255   AWL awl;
   256   Res res;
   257   Ring node;
   258   Space space;
   259 
   260   AVER(segReturn != NULL);
   261   AVER(baseReturn != NULL);
   262   AVER(limitReturn != NULL);
   263   AVERT(Pool, pool);
   264   AVERT(Buffer, buffer);
   265   AVER(size > 0);
   266 
   267   space = PoolSpace(pool);
   268 
   269   awl = PoolPoolAWL(pool);
   270   AVERT(AWL, awl);
   271 
   272   RING_FOR(node, &pool->segRing) {
   273     Seg seg;
   274 
   275     seg = SegOfPoolRing(node);
   276     AVERT(Seg, seg);
   277     group = (AWLGroup)SegP(seg);
   278     AVERT(AWLGroup, group);
   279 
   280     if(AWLGroupAlloc(&base, &limit, group, awl, size))
   281       goto found;
   282   }
   283 
   284   /* No free space in existing groups, so create new group */
   285 
   286   res = AWLGroupCreate(&group, buffer, pool, size);
   287   if(res != ResOK) {
   288     return res;
   289   }
   290   base = SegBase(space, group->seg);
   291   limit = SegLimit(space, group->seg);
   292 
   293 found:
   294   {
   295     Index i, j;
   296     i = AddrOffset(SegBase(space, group->seg), base) >> awl->alignShift;
   297     j = AddrOffset(SegBase(space, group->seg), limit) >> awl->alignShift;
   298     BTSetRange(group->alloc, i, j);
   299   }
   300   *segReturn = group->seg;
   301   *baseReturn = base;
   302   *limitReturn = limit;
   303   return ResOK;
   304 }
   305 
   306 
   307 static void AWLBufferEmpty(Pool pool, Buffer buffer)
   308 {
   309   AWL awl;
   310   AWLGroup group;
   311   Addr segBase;
   312   Index i, j;
   313 
   314   AVERT(Pool, pool);
   315   AVERT(Buffer, buffer);
   316 
   317   awl = PoolPoolAWL(pool);
   318   AVERT(AWL, awl);
   319   group = (AWLGroup)SegP(BufferSeg(buffer));
   320   AVERT(AWLGroup, group);
   321 
   322   segBase = SegBase(PoolSpace(pool), BufferSeg(buffer));
   323 
   324   i = AddrOffset(segBase, BufferGetInit(buffer)) >> awl->alignShift;
   325   j = AddrOffset(segBase, BufferLimit(buffer)) >> awl->alignShift;
   326   AVER(i <= j);
   327   if(i < j) {
   328     BTResRange(group->alloc, i, j);
   329   }
   330 }
   331 
   332 
   333 static Res AWLCondemn(Pool pool, Trace trace, Seg seg, Action action)
   334 {
   335   AWL awl;
   336   AWLGroup group;
   337 
   338   /* all parameters checked by generic PoolCondemn */
   339   AVERT(Pool, pool);
   340   AVERT(Trace, trace);
   341   AVERT(Seg, seg);
   342   AVERT(Action, action);
   343 
   344   /* can only condemn for a single trace, */
   345   /* see design.mps.poolawl.fun.condemn */
   346   AVER(SegWhite(seg) == TraceSetEMPTY);
   347 
   348   awl = PoolPoolAWL(pool);
   349   AVERT(AWL, awl);
   350   AVER(awl == ActionAWL(action));
   351 
   352   group = (AWLGroup)SegP(seg);
   353   AVERT(AWLGroup, group);
   354   
   355   BTResRange(group->mark, 0, group->grains);
   356   BTResRange(group->scanned, 0, group->grains);
   357   SegSetWhite(seg, TraceSetAdd(SegWhite(seg), trace->ti));
   358   
   359   return ResOK;
   360 }
   361 
   362 static void AWLGrey(Pool pool, Trace trace, Seg seg)
   363 {
   364   AVERT(Pool, pool);
   365   AVERT(Trace, trace);
   366   AVERT(Seg, seg);
   367 
   368   if(!TraceSetIsMember(SegWhite(seg), trace->ti)) {
   369     AWL awl;
   370     AWLGroup group;
   371 
   372     awl = PoolPoolAWL(pool);
   373     AVERT(AWL, awl);
   374     group = (AWLGroup)SegP(seg);
   375     AVERT(AWLGroup, group);
   376 
   377     SegSetGrey(seg, TraceSetAdd(SegGrey(seg), trace->ti));
   378     ShieldRaise(trace->space, seg, AccessREAD);
   379     BTSetRange(group->mark, 0, group->grains);
   380     BTResRange(group->scanned, 0, group->grains);
   381   }
   382 }
   383 
   384 
   385 /* Returns the linked object (or possibly there is none) */
   386 /* see design.mps.poolawl.fun.dependent-object, and */
   387 /* analysis.mps.poolawl.improve.dependent.abstract */
   388 static Bool AWLDependentObject(Addr *objReturn, Addr parent)
   389 {
   390   Word *object;
   391   Word *wrapper;
   392   Word fword;
   393   Word fl;
   394   Word ff;
   395 
   396   AVER(objReturn != NULL);
   397   AVER(parent != (Addr)0);
   398 
   399   object = (Word *)parent;
   400   wrapper = (Word *)object[0];
   401   AVER(wrapper != NULL);
   402   /* check wrapper wrapper is non-NULL */
   403   AVER(wrapper[0] != 0);
   404   /* check wrapper wrapper is wrapper wrapper wrapper */
   405   AVER(wrapper[0] == ((Word *)wrapper[0])[0]);
   406   fword = wrapper[2];
   407   ff = fword & 3;
   408   /* Traceable Fixed part */
   409   AVER(ff == 1);
   410   fl = fword & ~3uL;
   411   /* At least one fixed field */
   412   AVER(fl >= 1);
   413   if(object[1] == 0) {
   414     return FALSE;
   415   }
   416   *objReturn = (Addr)object[1];
   417   return TRUE;
   418 }
   419 
   420 
   421 static Res AWLScan(ScanState ss, Pool pool, Seg seg)
   422 {
   423   Addr base, limit;
   424   Addr p;
   425   AWL awl;
   426   AWLGroup group;
   427   Bool finished;
   428   Space space;
   429 
   430   /* parameters checked by generic PoolScan */
   431   AVERT(ScanState, ss);
   432   AVERT(Pool, pool);
   433   AVERT(Seg, seg);
   434 
   435   group = (AWLGroup)SegP(seg);
   436   AVERT(AWLGroup, group);
   437 
   438   awl = PoolPoolAWL(pool);
   439   AVERT(AWL, awl);
   440 
   441   space = PoolSpace(pool);
   442   base = SegBase(space, seg);
   443   limit = SegLimit(space, seg);
   444 
   445 notFinished:
   446   finished = TRUE;
   447   p = base;
   448   while(p < limit) {
   449     Index i; /* the index into the bit tables corresponding to p */
   450     Addr objectEnd;
   451     /* design.mps.poolawl.fun.scan.buffer */
   452     if(SegBuffer(seg)) {
   453       if(p == BufferScanLimit(SegBuffer(seg))) {
   454  p = BufferLimit(SegBuffer(seg));
   455  continue;
   456       }
   457     }
   458     i = AddrOffset(base, p) >> awl->alignShift;
   459     /* design.mps.poolawl.fun.scan.free */
   460     if(!BTGet(group->alloc, i)) {
   461       p = AddrAdd(p, pool->alignment);
   462       continue;
   463     }
   464     /* design.mps.poolawl.fun.scan.object-end */
   465     objectEnd = awl->format->skip(p);
   466     /* design.mps.poolawl.fun.scan.scan */
   467     if(BTGet(group->mark, i) && !BTGet(group->scanned, i)) {
   468       Addr dependentObj;
   469       Seg dependentSeg;
   470       Bool dependent;
   471       Res res;
   472 
   473       finished = FALSE;
   474       /* is there a dependent object that needs exposing? */
   475       dependent = AWLDependentObject(&dependentObj, p);
   476       if(dependent) {
   477  Bool b;
   478 
   479  b = SegOfAddr(&dependentSeg, space, dependentObj);
   480  if(b == TRUE) {
   481    ShieldExpose(space, dependentSeg);
   482    TraceSetSummary(space, dependentSeg, RefSetUNIV);
   483  } else {
   484    dependent = FALSE;
   485  }
   486       }
   487       res = awl->format->scan(ss, p, objectEnd);
   488       if(dependent) {
   489         ShieldCover(space, dependentSeg);
   490       }
   491       if(res != ResOK) {
   492         return res;
   493       }
   494       BTSet(group->scanned, i);
   495     }
   496     p = AddrAlignUp(objectEnd, pool->alignment);
   497   }
   498   if(!finished)
   499     goto notFinished;
   500   
   501   return ResOK;
   502 }
   503 
   504 
   505 static Res AWLFix(Pool pool, ScanState ss, Seg seg, Ref *refIO)
   506 {
   507   Ref ref;
   508   Index i;
   509   AWL awl;
   510   AWLGroup group;
   511   Space space;
   512 
   513   AVERT(Pool, pool);
   514   AVERT(ScanState, ss);
   515   AVERT(Seg, seg);
   516   AVER(TraceSetInter(SegWhite(seg), ss->traces) != TraceSetEMPTY);
   517   AVER(refIO != NULL);
   518 
   519   awl = PoolPoolAWL(pool);
   520   AVERT(AWL, awl);
   521   group  = (AWLGroup)SegP(seg);
   522   AVERT(AWLGroup, group);
   523 
   524   space = PoolSpace(pool);
   525   AVERT(Space, space);
   526 
   527   ref = *refIO;
   528   i = AddrOffset(SegBase(space, seg), ref) >> awl->alignShift;
   529   
   530   ss->wasMarked = TRUE;
   531 
   532   switch(ss->rank) {
   533   case RankAMBIG:
   534     /* not a real pointer if not aligned or not allocated */
   535     if(!AddrIsAligned((Addr)ref, pool->alignment) ||
   536        !BTGet(group->alloc, i)) {
   537       return ResOK;
   538     }
   539   /* falls through */
   540   case RankEXACT:
   541   case RankFINAL:
   542   case RankWEAK:
   543     if(!BTGet(group->mark, i)) {
   544       ss->wasMarked = FALSE;
   545       if(ss->rank == RankWEAK) {
   546  *refIO = (Ref)0;
   547       } else {
   548  BTSet(group->mark, i);
   549  TraceSegGreyen(space, seg, ss->traces);
   550       }
   551     }
   552     break;
   553   
   554   default:
   555     NOTREACHED;
   556     return ResUNIMPL;
   557   }
   558 
   559   return ResOK;
   560 }
   561 
   562 
   563 static void AWLReclaim(Pool pool, Trace trace, Seg seg)
   564 {
   565   Addr base;
   566   AWL awl;
   567   AWLGroup group;
   568   Index i;
   569   Space space;
   570 
   571   AVERT(Pool, pool);
   572   AVERT(Trace, trace);
   573   AVERT(Seg, seg);
   574 
   575   awl = PoolPoolAWL(pool);
   576   AVERT(AWL, awl);
   577   group = (AWLGroup)SegP(seg);
   578   AVERT(AWLGroup, group);
   579 
   580   space = PoolSpace(pool);
   581   AVERT(Space, space);
   582 
   583   base = SegBase(space, seg);
   584 
   585   i = 0;
   586   while(i < group->grains) {
   587     Addr p;
   588     Index j;
   589     if(!BTGet(group->alloc, i)) {
   590       ++i;
   591       continue;
   592     }
   593     p = AddrAdd(base, i << awl->alignShift);
   594     if(SegBuffer(seg) != NULL) {
   595       Buffer buffer = SegBuffer(seg);
   596 
   597       if(p >= BufferScanLimit(buffer)) {
   598         AVER(p == BufferScanLimit(buffer));
   599  i = AddrOffset(base, BufferLimit(buffer)) >> awl->alignShift;
   600  continue;
   601       }
   602     }
   603     j = AddrOffset(base, awl->format->skip(p)) >>
   604         awl->alignShift;
   605     AVER(j <= group->grains);
   606     if(!BTGet(group->mark, i)) {
   607       BTResRange(group->alloc, i, j);
   608     }
   609     i = j;
   610   }
   611   AVER(i == group->grains);
   612 
   613   BTResRange(group->mark, 0, group->grains);
   614   SegSetWhite(seg, TraceSetDel(SegWhite(seg), trace->ti));
   615 }
   616 
   617 static Res AWLTraceBegin(Pool pool, Trace trace, Action action)
   618 {
   619   AWL awl;
   620 
   621   AVERT(Pool, pool);
   622   awl = PoolPoolAWL(pool);
   623   AVERT(AWL, awl);
   624   AVERT(Trace, trace);
   625   AVERT(Action, action);
   626   AVER(awl == ActionAWL(action));
   627 
   628   awl->lastCollected = PoolSpace(pool)->allocTime;
   629   return ResOK;
   630 }
   631 
   632 /* @@@@ completely made-up benefit calculation: each AWL pool gradually
   633  * becomes a better candidate for collection as allocation goes
   634  * by. Starting a trace on a pool makes it a bad candidate. nickb
   635  * 1997-06-19 */
   636 
   637 static double AWLBenefit(Pool pool, Action action)
   638 {
   639   AWL awl;
   640 
   641   AVERT(Pool, pool);
   642   awl = PoolPoolAWL(pool);
   643   AVERT(AWL, awl);
   644   AVERT(Action, action);
   645   AVER(awl == ActionAWL(action));
   646 
   647   return (PoolSpace(pool)->allocTime - awl->lastCollected) - 
10*1024*1024.0;
   648 }
   649 
   650 
   651 struct PoolClassStruct PoolClassAWLStruct = {
   652   PoolClassSig,
   653   "AWL",
   654   sizeof(AWLStruct),
   655   offsetof(AWLStruct, poolStruct),
   656   AttrFMT | AttrSCAN | AttrBUF | AttrBUF_RESERVE | AttrGC | AttrINCR_RB,
   657   AWLInit,
   658   AWLFinish,
   659   PoolNoAlloc,
   660   PoolNoFree,
   661   PoolTrivBufferInit,
   662   AWLBufferFill,
   663   AWLBufferEmpty,
   664   PoolTrivBufferFinish,
   665   AWLTraceBegin,
   666   AWLCondemn,
   667   AWLGrey,
   668   AWLScan,
   669   AWLFix,
   670   AWLReclaim,
   671   PoolTrivTraceEnd,
   672   AWLBenefit,
   673   PoolTrivDescribe,
   674   PoolClassSig
   675 };
   676 
   677 
   678 mps_class_t mps_class_awl(void)
   679 {
   680   return (mps_class_t)&PoolClassAWLStruct;
   681 }
   682 
   683 
   684 static Bool AWLCheck(AWL awl)
   685 {
   686   CHECKS(AWL, awl);
   687   CHECKD(Pool, &awl->poolStruct);
   688   CHECKL(awl->poolStruct.class == &PoolClassAWLStruct);
   689   CHECKL(1uL << awl->alignShift == awl->poolStruct.alignment);
   690   CHECKD(Action, &awl->actionStruct);
   691   CHECKL(awl->poolStruct.space->allocTime >= awl->lastCollected);
   692   return TRUE;
   693 }
   694 
   695 
   696 static Bool AWLGroupCheck(AWLGroup group)
   697 {
   698   CHECKS(AWLGroup, group);
   699   CHECKL(SegCheck(group->seg));
   700   CHECKL(group->mark != NULL);
   701   CHECKL(group->scanned != NULL);
   702   CHECKL(group->alloc != NULL);
   703   /* Can't do any real check on ->grains */
   704   CHECKL(group->grains > 0);
   705   return TRUE;
   706 }

