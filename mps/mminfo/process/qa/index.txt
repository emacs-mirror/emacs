                     THE QUALITY ASSURANCE PROCESS
                               process.qa
                             incomplete doc
                             rit 1996-10-07

AIMS

To improve the eventual product in reliability, performance
[and perhaps in functionality] by:

 1. finding errors or infelicities in specification, software,
    and, if available, documentation so that they may be corrected;
 2. providing the development group with resources to help them
    reduce such errors in future versions.

The main activity in QA is of course _testing_, which requires
_test_planning_.


TEST PLANNING

In test planning, we must (not necessarily in this order):
 - analyze the specification to identify classes of test-cases
 - adopt a testing strategy: what to test when and to what depth
 - create or obtain tools to assist, control and automate testing
 - establish ways to communicate with the developers in order to
   explain the testing and results, and to obtain the information
   necessary to plan it and carry it out.
[from Kaner, Falk, Nguyen: "Testing Computer Software", p.203]

It's not simply a case of "create plan, then follow it". The product
requirements may change. Testing will identify areas which are
particularly unreliable and hence more concentrated testing. New
releases of the product will need to be tested as they are produced.
Developers may ask for different kinds of information from the testing
process. For all these reasons, the process of planning and testing is
one that will have to be repeated. Even within a single iteration,
some aspects of the test plan may need to be revised once testing
is underway.


THE SPECIFICATION

...must include:

 - functional requirements (what the product is supposed to do)
 - attribute requirements  (under what circumstances and how well
                            it's supposed to do it)
 - interface specification (what the user is supposed to do)

It's important that the interface specification says not just how the user
should behave, but what will happen if the user doesn't behave as they should.
If the answer is "anything can happen", the interface should explicity warn
the user of this unpleasant fact.

Formulating a precise specification is very difficult, so it's likely that,
in the course of analyzing it, it will become apparent that, in places, the
specification is incomplete, ambiguous, or even erroneous. QA must talk to
the development group to clarify the specification when this happens.


%
% rough notes only from here onwards
%

identify from spec what resources are needed:
   -- h/w, s/w platforms
   -- other software it must work with
   -- particular test suites

For each requirement, identify test-case classes (by boundary analysis,
minimal pair, cause-effect, random, or as appropriate) considering
specified output as well as input. Development group should already
have done code-inspection, so black-box testing will be
more useful than white-box.

Consider variation in external factors not mentioned in the spec
(e.g. other processes running, change of millenium)

Think about which requirements are independent, and when combinations of
test-case classes must be considered? It will not be feasible to test
every combination of circumstances on every platform, so compromises
may have to be made. c.f. cause-effect graphing.

Specify required result for test-case classes. There are two kinds of
result:
 -- required result is to generate an error
 -- required result is not to generate an error
They require slightly different approaches because of masking effects.

By this stage, gaps in the spec will have become apparent -- consult
with development group (& with users if possible) to clarify.

Choose test-cases which cover many test-case classes at once to
reduce total number of cases needed. As a corollary of this:

Test core functionality (when there are multiple products)
in addition to testing each product,

BUT don't combine two cases each of which has an error as the
expected output (masking). Or that is, you can combine them, but
you also have to test all error cases separately. This applies mainly
to interface stuff.

Be sure to test for errors which were found in previous releases,
especially for errors found by users, even if since corrected, because:
 -- they might come back, if they are easy mistakes to make
 -- users will be more annoyed if the product seems to be regressing
 -- such testing is likely to be cheap, 'cos the tests are already available

Inspect results carefully. Was an unsuccessful test really unsuccessful
(it's easy not to notice an error if it's hidden in a page of information)?
Was a successful test really successful (or could there be an error in
the test program)?

Perform more tests on areas where errors are found, because mistakes
tend to cluster rather than be evenly spread throughout software.

Prioritize tests -- some reqs are more important than others. Some tests
unavoidably take longer than others (e.g. long-term durability, mean time
to failure, &c).

Report results to development group, with details of
 -- successful tests (i.e. errors found) (provide instances and/or classes)
 -- borderline cases (for attribute requirements)
 -- tests which find no error -- how thorough were they? (ditto)
[-- cases where there are vast amounts of leeway? ]

 & of how adequate testing was for different requirements --
   in particular, were there some which couldn't be adequately tested?

 & of differences between this and other (previous) releases.

Report results to users?

Do not ever throw away tests or results.


