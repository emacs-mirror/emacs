         REGRESSION TESTING, DARWIN, AND LESSONS FROM PARASOLID
                         meeting.qa.1997-01-09
                               draft doc
                             rit 1997-01-09

Dramatis personae: <g> gavinm, <r> rit, <d> drj

<g> A requirement is that tests cover as much as possible, and certainly all 
customer bugs. When we find a customer bug which isn't covered by any existing 
test, we regard it as a problem. 

2 ways of meeting this requirement:
- make sure some test covers each customer bug (perhaps by extending an 
existing test)
- have a suite of individul regression tests, each of which addresses one 
customer bug.

The second is more work to set up and maintain, but provides an important 
advantage: extending tests makes them into tests of many things at once, which 
means that when they fail some things may be left untested.

So far, our approach has been to (try to) pass all tests at release time (we 
currently have half a dozen automated tests), but this will become increasingly 
impractical. If a bug caused one of 8 tests to fail, then that's an eighth of 
our testing gone.

<r> My approach would be to only combine things in a single test when they are 
all essential requirements and if any of them fail, it's serious enough to 
demand attention.

<g> So we only test showstoppers?

<r> No, we have other tests too, but we can have a number of large tests which 
combine showstoppers together, and lots more small tests which check for 
individual aspects.

<g> I agree that there's a place for large stress tests--stress tests are good. 
But a key advantage of an automated regression test system is that we can tie 
it in to our product evolution database, for example, to compare expected 
status with actual status.

<r> I agree. It's important to know exactly which regression tests succeed and 
which fail, and for that you have to separate them. Automation isn't absolutely 
essential...

<g> ...but tying it into your PrEv database would be a good thing. Manual 
testing is only a short-term time-saver.

<g> Regression testing: the philosophy we had in PARASOLID was that we tried to 
reproduce exactly what the customer did when it went wrong. It took a long time 
to do all of the regression tests.

<r> Since people are using the MM in their own programs, wouldn't that mean 
that we'd have to do exactly what every user program did?

<g> No. The customer would say: "I intersected these two bodies and it didn't 
work". In debugging you'd reduce it to a small example which didn't work, but 
then the regression test was to try the bodies used by the customer.

<d> So if Tony comes in with a problem with Dylan, the regression test would be 
to run his program. I'd need the object file and the linker. For a customer bug 
it would be even worse.

<g> I must caution you against rampant bug-whittling, i.e. reducing a test to 
the minimal case. A regression test should have a close link to a fix request 
which should have a close link to what the customer tried to do.

<d> But sometimes the bug as produced by customer is not reproducible or 
deterministic. I think you want both tests: wide and narrow. The narrow test is 
a fast way to see whether the bug exists or not.

<g> In PARASOLID, we did have customer reports of operations that continued to 
fail even though we'd 'fixed' them. The lisp reproducing the bug had been 
oversimplified, and although it fixed the bug, it didn't ensure that the whole 
operation succeeded. We had a lot of problems with the sheer size and CPU 
requirements of our regression tests. Our requirement was that we be as sure as 
possible that customer bugs were fixed. Typically the regression tests were of 
the form: load in these one or two huge bodies and do some fairly simple 
operation on them. e.g.
  (modeller start)
  ((define b0 body) receive "flt12345")
  ((define e0 edge) tag (idenid (b0 tag) 'TYTOED 37))
  (blecre (e0 tag) 1 1 'TYBLRB)
  (blefix (b0 tag))
  (b0 check)
Typically we tested on HP/UX. Platform dependency was fortunately relvatively 
rare.

<d> But you need smaller tests too for your developers to use.

<g> This is a subtle point. Let's leave it for now. I suggest we see what 
experience tell us.

---

<g> Would you cover feature requests in a similar way to fix requests, i.e. by 
regression tests?

<r> I think so, yes, because they have to be checked in the same way when the 
release comes out, and they shouldn't go wrong in future versions. But they 
probably have a lower priority.

<g> In PARA, testing a feature was more a case of planning a wide range of 
tests to really try out many possibilities. You'd want several different tests 
to test a feature.
We had a problem with regression tests failing when the interface changed. We 
addressed this by having a (object-oriented) layer on top of the interface that 
almost never changed: when you wrote a regression test, you wrote all the setup 
code in that, and only the actual functions being tested in the lower layer. 
The oo layer was part of our test harness.

----
Testing and Darwin

<g> 2 points
1. Requirement for regression tests

We need to model the requirement that we have regression tests for requests in 
Darwin.
<r> Yes. That's almost implicit in Darwin anyway, because of the QA phase.
<g> But I'm also talking about testing within development. In Parasolid, there 
was an image build every night, and by 9 the next morning, a list of regression 
test results was mailed to various people. That's very useful. We had 50 
developers; if someone released code that broke a customer bug, we'd know about 
it the following morning and could investigate. One day's worth of releases for 
50 people is typically 10-20 changes.
<r> It sounds like it might not be so necessary for a group with only 5 
developers and a smaller product.
<g> True. It was difficult for developers to run the regression tests 
themselves (they could run module tests more easily, and typically did when 
changing anything within a unit) 'cos they wouldn't know which ones to run. We 
might only need to do it weekly, but it's certainly useful to have it done more 
frequently than releases.
<r> Do you think there's a need for developers to have easy access to the test 
suites and to be able to run tests whenever they want?
<g> Yes. There are three uses of (regression) tests:
a. bug fixing
Parasolid model is: you write the test lisp as a statement of the reproduction 
of the bug (the program itself checks whether it works). That lisp is what the 
maintenance developer will look at when finding and fixing the bug.

b. change testing
You've made some changes to code and you want to make sure that you haven't 
broken anything inadvertently. Regression tests aren't ideal for this---you 
really want functional coverage tests and unit tests. Unit tests are more 
important, because you can't run every possible test, you'd like to run only 
the tests which are appropriate.
<r> In fact you might be able to develop classes of non-unit tests which are 
nevertheless designed to test individual units.
<g> Parasolid has unit tests which use (secret) unit interfaces. It also has 
what it calls 'system tests' which test areas of functionality. If you, for 
example, implement blending, then you write tests to give blending a 
workthrough. In fact most of blending is in the BLE module, so there is 
sometimes some duplication there.

c. regression tracking
Checking that things are fixed when they're supposed to be. Noticing if they 
get fixed 'serendipitously'. Seeing if they break subsequently.

2. Need to compare test results with darwin status

<g> QA will do this in the QA phase, and generate reworks etc from it. However, 
at the risk of eroding the concept of a relase, it is useful for development to 
be able to do this more often. E.g. weekly.
<r> 2 kinds of comparison you might want to do on an e.g weekly basis:
a. How does it compare with the requirements for the release I'm working on? 
i.e. compare test results with darwin.
b. How does it compare with last week's version? i.e. compare 2 sets of test 
results.
<g> 2 things that darwin will tell you: the planned state for a request in a 
release (i.e. whether it will be done) and the development status of a change 
(i.e. whether it has been done). With 'sufficient automation', the QA status 
could be calculated automatically by running a test depending on the 
development status.


-----------

<r> A standard procedure for QA should be to look at changes in darwin which 
have been assigned to developers, and (in cooperation with the developers) to 
write tests for these changes. The tests will be used to check the release when 
it is made, but some of them can also be given to the developers to help them 
in carrying out the change.

