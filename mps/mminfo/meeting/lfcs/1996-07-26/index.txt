        NOTES ON MM/LFCS BRIEFING MEETING, CAMBRIDGE, 1996-07-26
                        meeting.lfcs.1996-07-26
                             incomplete doc
                           brianm 1996-07-27

[ Richard, I'm not sure that this is `well-tagged' - I looked at your
  docs. and I think it's OK.  But please complain if not.

  When I do meeting minutes, I try to present my understanding of the
  meeting.  This means that I give an interpretation of events.  So
  it isn't a verbatim record - it sometimes fills in with my thoughts
  - that I reached after the fact.  This may or may not be appropriate - so
  please adjust these notes accordingly ... and let me know for future
  reference.

  There is at least one point which needs your clarification - such
  points are marked as ***.

  Please delete this comment if/when it becomes redundent.
]

INTRODUCTION 

.intro The purpose for this meeting was for Richard to brief me on
technical aspects of the MM-LFCS collaboration.  I had attended one
group meeting previously (at Longstanton House) - I have also spent
a day talking to people at LFCS.

I am due to visit Healf Goguen at LFCS again on Friday, 9th Aug -
and I need to have the technical material for discussion sent to Healf
by close of Wednesday 7th August.

TOPICS

1.  Goals + requirements of modelling activity
2.  The Tracer
3.  ML model of the tracer


1. Goals + requirements of the modelling activity
-------------------------------------------------
.goals  The main idea is to construct `models' of the MM work in ML.
There are several purposes and benefits of doing this:


  a.  This provides a mechanism for acheiving improvement of
      functional quality of product and reliability.  Modelling
      helps develop the case for functional integrity of the product.

      The model provides technical doumentation of the operating
      principles of the system and in an executable framework.  In
      particular, it maybe possible to use the model to validate
      tests and obtain data by direct execution.


  b.  Provides an opportunity to study the model and compare it to the
      `live' code.  The model's main purpose is to maintain a handle on
      functional aspects of this code.

      Although performance isn't immeadiately deducible, the
      archetectures that the models describe generally represent design
      trade-offs and these may well have performance implications.  So,
      models don't say a great deal directly about performance aspects -
      but there could be some results obtainable by indirect means.
      

  c.  Live code needs to do ``clever tricks'' that exploit all sorts of
      properties of the actual representation in order to obtain
      performance.  There is a great temptation to use code that is more
      abstract than it needs to be - the worry being that the code
      would become too opaque if all these clever tricks were applied one
      upon the other (i.e. compromising functional integrity of the system).

      Modelling provides a way of _justifying_ the application of all
      the `clever tricks' that are needed in live code for performance
      reasons.

      We can do this by taking each of the `clever tricks' and then giving
      a clean representation in terms of these models.  Each time these
      tricks are used, the code should contain a tag comment pointing back
      the appropriate `constructive justification' in the model or model(s).

      This is also useful in reviews since obscure untagged code will often not
      map back onto (combinations of) the known tricks.  The options then are
      to rewrite the code so that such a mapping can be stated -or- to
      add new tricks into the model and than add the mapping info. for
      the code.


  d.  The MM system needs to be:

      - Adaptable : changeable to meet different low-level requirements
                    (i.e. system platform requirements).

      - Flexible  : ability to meet different user requirements on the
                    fly (i.e. change of parameters) (see .utility).

      Because the product will be used in a wide range of circumstances,
      `absolute' performance isn't the issue (although performance trade-offs
      are important in each of the places the system is used).

      This degree of required adapatability means that the `clever
      tricks' needed for performance cannot be so widely applied - at least
      in a naive direct way.

      Modelling may help us get a handle on how to acheive
      adaptabilty of the code whilst also making use of `clever
      tricks'.


  e.  Potential wider economic benefit to software development practice in the
      company (and beyond).  The use of modelling here could provide a
      concrete example of how abstraction hierarchies and barriers can
      be useful when developing complex code (e.g. exploring trade-offs
      in archetectures - knock-on effects of design decisions).


  f.  Provides something to act as an abstraction barrier between LFCS
      and Harlequin.  Advantages are:

         - An abstract model provides a `lingua franca' interface between
           the two groups, thus provides a standard channel for
           communication and exchange of technical information.

         - By adding an explicit intermediary, each group can make
           progress independently of the other.

         - Resynchronisation can be expressed in terms of
           characteristics of the model and of how it could be
           updated/extended to reflect new understandings.

         - From LFCS point of view, they may see how we use their input
           to strengthen/reinforce the implementation - so the model
           provides a `bridge' between verification and code
           development.


.goals.concrete  The LFCS team is looking at GC itself - Paul is looking
at a pure read barrier collector, whilst Healf is looking at the 5 stage
collector.


2.  The Tracer
--------------
.tracer  Richard described the current thinking concerning the Graph
Tracer (GT).  This is concerned with the connectivity of nodes reachable
from the root.  What follows is a brief summary of the conversation
(Richard wrote this mainly as a sequence of diagrams within my notebook -
I hope to convey their essence :-) ).


   - partitions on graphs:

     Let P be any property of nodes (i.e. extensionally a subset of
     nodes).  We introduce colouring as (an `approximation') attribute to
     nodes as follows:

         white (P)  : nodes currently _known_ to have property P

         grey (P)   : nodes which may contain a pointer to white nodes.

         black (P)  : nodes which are currently neither white nor grey.

    Note that the colouring represents a dynamiclly changing situation
    concerning what is known about nodes wrt a given (but fixed)
    property, P.  Any given property P will be fixed whilst colouring, even 
though
    the colour of nodes may change.  The act of colouring is to
    determine 

    At any one moment, a partition may be considered to be a triple of sets
    of nodes:

            ( White , Grey , Black )

    whose composition changes as the colouring of nodes changes.  Note
    that a node can only have one colour - none of White, Grey or Black
    intersect (disjointness).


 -  A _reference partition_ is a partition where no black node contains
    pointers to white objects.  Black objects can of course point at
    grey objects and other black objects.


*** Richard, in the diagram, the following was also true:

       - Grey objects only point at other grey objects and also
         point at white objects.

       - White objects only point at other white objects.

    Is this really required to be true?  I suspect that it may be an
    accident of the diagram.  However if we do really need this after all
    then it isn't derivable from the graph invariant.  It is trivial to
    construct a simple example satisfying the original invariant,
    but where some white nodes refer `back' to black nodes.

    To formalise the extended invariant inferred above, one may consider graph
    morphisms from graphs to the following partially ordered 3-point set :

         W << G << B

    We also add the `grouping' constraint that Black nodes may map to anything,
    Grey nodes may map to either G or W and White nodes may only map to W.

    Now, graph morphisms preserve connectivity - the edge relation in the
    original graph.  The invariant property is then that there exists a graph
    morphism satisfying the above constraints.

    [ This is interesting.  Even if we don't need this stronger invariant - it 
is
      instructive to _say_ why we don't.  If we do need the stronger invariant, 
then
      I suggest labelling the current invariant something and giving a different
      label to the stronger invariant. ]
   
 -  In terms of GC, one may imagine starting with some initial
    reference partition and ending up with a partition in which there were no
    Grey nodes left.  In GC algorithms, the White set will be
    the condemned set of nodes - once a node is coloured white, it
    remains so.  Also, the black set will then be the `live' nodes.

 -  The property P determines the set White.  However, note how this
    set isn't the one we ultimately want - this set contains those
    nodes we shall condemn and eventually reject by disposing of them.  This
    is a common and frequently occurring pattern - often useful properties
    will be negatives (something good isn't true).

 -  Root nodes are selected nodes not satisfying the .

*** In the abstract case, are all the roots known _not_ to satisfy property P?
    Certainly, with GC, this is true - but is it valid in general?

 -  A mutator process is one that applies surgery operations to the graph:
    Mutators are always anchored to a `root node' and apply operations to
    its descendents.  The operations look like the following:


     Root nodes = >O


     .load      >O                  >O
                 |                   |\
                 |         ====>     | \
                 |                   |  \
                 V                   V   \
                 O--> O              O--> O
                 
     This operation `pulls' nodes towards the root
     (i.e. sibling siblings become siblings).

   ---------------

     .store     >O                  >O
                 |\                  |\
                 | \       ====>     | \
                 |  \                |  \ 
                 V   \               V   \
                 O    O              O--> O

     This adds connections between node siblings.

   ---------------


     .drop      >O                  >O
                 |
                 |         ====>
                 |
                 V
                 O                   O

     This disconnects siblings from root (i.e. disinheriting).
***  Can the sibling node have any descendents?  Does this
     matter?

   ---------------

     .splat     >O                  >O
                 |                   |
                 |         ====>     |
                 |                   | 
                 V                   V
                 O--> O              O   O

     This disconnects siblings from their neighbours. 

-----------------------------------------------------------------------

 -  In any graph, it is claimed that several mutators can act upon the graph
    independently and simultaneously.  This assumes that the above
    operations are atomic and that nodes may be legitimately joined to
    more than one root, (where this arises of course).

 -  We can infer a further property - since GC must be semantics
    preserving (otherwise - what's the point :-) ?), the connections that
    are added or deleted are not (eventually) semantics bearing.  That is, these
    `connections' can be manipulated arbitrarily _during_ GC - but whatever
    semantic significance they have will be restored at the end of GC.

 -  We now add colouring constraints to some operation from the above. 

    .write.barrier  This is an operation which repeatedly applies STORE to 
coloured nodes
    "as much as possible":

    WRITE BARRIER:     >g                  >g
                        |\                  |\
                        | \       ====>     | \
                        |  \                |  \ 
                        V   \               V   \
                        b    w              g--> w

    This operation `pulls' black nodes into the grey zone.

    ---------------


    .read.barrier  Dually to the Write Barrier, this operation repeatedly
    applies LOAD to coloured nodes "as much as possible":

    READ BARRIER:      >b                  >b
                        |                   |\
                        |         ====>     | \
                        |                   |  \
                        V                   V   \
                        g--> w              g--> g
                       
    This operation `pulls' white nodes into the grey zone.

    ---------------

    .termination  The termination condition is simply that neither read
    nor write barrier parts can do anymore useful work.  There are no more nodes
    to which these operations can be applied.  Consequently all surviving
    grey nodes may now be (conceptually) treated as black nodes.  This means
    that no black node (or grey node) points at a white node.  Hence the
    black and white nodes have been seperated, and the white nodes can
    be eliminated.

    .5phase.gc.framework The above operations represent the principle actions 
required to
    perform GC.  The outline GC framework is as follows:

       1. Start    - determine initial partition
       2. W-Trace  - write barrier moves (roots grey)
       3. Flip     - roots turn black
       4. R-Trace  - read barrier moves (roots black)
       5. Reclaim  - no grey left, recycle white

    The point here is that this is a genuine _framework_ - the above
    can be configured to give a spectrum of GC algorithms - with
    different cost/benefit profiles (trade-offs).

    Write Barrier collection algorithms :
      - as above but with step 4 switched out.
      - can't move (relocate) objects
      - Mark and Sweep

    Read Barrier collection algorithms :
      - as above but with step 2 switched out
      - can relocate objects (i.e. compaction)
      - copying GC


 -  .placement.policy  - where to place new objects in memory.
    It can be shown that every placement policy has an
    acheivable worst-case.  This means that:

        1) there is a worst case performance for placement, no matter
           what algorithm is chosen.

        2) it can always be acheived by some sequence of requests.

    The point is that the worst-case behaviour is different for
    different algorithms.  Also, different algorithms for placement will
    do better than others
    
*** Richard, you promised me a precise statement of this and a reference
    to a proof of the attainment of the worst-case ... but I'm beginning
    to see how this might go (i.e. an adversarial argument that forces
    all searches to their limits).

    Placement policies will use characteristics of the object being allocated
    to decide where best to place them:
       - pairs might be allocated in their own region
         (frequently used and recycled - high frequency of requests)
         
       - large objects may be allocated in their own region
         (less frequently recycled - low frequency of requests)

 -  First-Fit algorithm:

    Storage regions are kept on a free-list.  When objects are freed, this
    creates free memory on the free-list.  To satisfy a request, scan
    the list for the first free region on the list thats large enough and
    assign it.  Otherwise, acquire/allocate memory.

    Worst case for this is as follows:

      0. - allocate many large objects

      1.  free a large object
          (i.e. makes a large hole)
        
      2.  request allocate a small object
          (i.e this falls into the hole, filling it up a little)
        
      3.  request allocate a large object
          (i.e. this forces new memory to ba allocated,
          since the hole isn't large enough now).

      <repeat steps 1 - 3 as much as you like - or can :-( >

    The effect of this is that memory use goes through the roof - and
    must eventually exceed the process limit and crashes the program.

    .initial.partitions  These partitions give an initial approximation
    to the desired GC'd partition.  The choice of initial partition
    affects efficiency and behaviour of the GC.

    For example, choosing an `all grey' initial partition means that no
    storage would ever be condemned - both read and write barriers never
    mark a grey node as a white node.  So the GC would terminate quickly
    - but it would have done no useful work!

    Another initial partition would be to mark all but the roots as
    white.  This does more work - but possibly too much in general.  The
    write barrier phase is ineffective and cannot engage.  All the work
    would be done by the read barrier phase (i.e. essentially a Mark/Sweep).

    Better choices involve having some `known' black nodes.  In fact an
    initial partition may be obtained from a _logical_ combination of two 
initial
    partitions such as:

                                  Pool 3
                             
                            B       G        W
                       +--------------------------+
                       |                          |
                    B  |    B       B        B    |
                       |                          |
                       |        ..................|
          Gen 1.    G  |    B   .   G        G    |
                       |        .                 |
                       |        .         ........|
                    W  |    B   .   G     .  W    |
                       |        .         .       |
                       +--------------------------+

    This is fairly efficient since it marks nodes as black as possible,
    thus minimising white.

    You may be amused to note that this is the three-valued logical
    disjunction devised by Lukasiewicz and Kleene (and used by Cliff
    Jones in his LPF).  [put Black as true, White as false and
    Grey as don't know]

    The pattern dual to the above is:

                                  Pool 3
                             
                            B       G        W
                       +--------------------------+
                       |        .        .        |
                    B  |    B   .   G    .   W    |
                       |.........        .        |
                       |                 .        |
          Gen 1.    G  |    G       G    .   W    |
                       |..................        |
                       |                          |
                    W  |    W       W        W    |
                       |                          |
                       +--------------------------+

    This maximises the choice of white nodes and minimises the choice of
    black.  Due to symmetry, my suspicion is that there are circumstances
    in which this combination pattern is preferable to the others given above.

    Naturally the above table corresponds to "three valued conjunction".

    .utility.function  It is a central objective that the GC framework be very
    flexible to allow it to be dynamiclly tuned to provide good performance in
    many particular and distinct applications.

    To do this, the GC system needs some information on which to basis
    this tuning.  This will be done by allowing the application
    programmer to provide a utility function.

    The point here is that. unlike conventional storage manager interfaces,
    the utility function operates purely in terms of attributes that the
    programmer knows and cares about.  For example, the programmer may
    specify how much time to spend GC'ing as function of space
    available:

                      A
        Time to spend |
        GC'ing        |
                      |
                      |
               100 %  |...................
                      |                   .
                      |                    .
                50 %  |                     .
                      |                      .
                      +------------------------------>
                     0             Space Available

    This allows the programmer to give precise information about what
    the ideal _profile_ for memory management might be for the application.

    So memory management involves a multi-dimensional optimisation of a
    number of qualities and attributes.  The users `utility.function' (also
    called the `objective function' in operations research) may not
    always be feasible.  The memory manager will therefore have to form
    strategies even in the presence of contradictory goals (i.e. use of some
    prioritisation or weighting scheme).

    In particular, there is a trade-off between spending time computing
    what an optimal response might be and simply finding some adequate and
    acceptable response.  The range of options available to the memory manager
    is _informed_ by the goals and requirements specified by the user.

    It is clear that obtaining optimal performance would require infinite
    lookahead in the form of omniscience concerning future requests.
    Sometimes the applications programmer already knows how all memory
    will be used for the application - but in general, this will only
    be known approximately.

[WRT meeting.lfcs.1996-07-26, clarification of graph tracer /
partitions. See mail.nickb.1996-04-19.09-39, and subsequent mails with
the same subject. -- Nick B]

3. Modelling in ML
------------------

    Richard described how he thought an ML model might go.  He hoped
    that this would take the form of a fully functorised assembly of
    components.

    Each component should play a conceptual role, allowing the
    structural form of the development to reflect relationships
    between the concepts represented.  This implies that the ML
    models will be manifested as a hierarchy of signatures and
    associated functors.

    Also, the execution efficiency of the model is not of paramount importance.
    Hence, simple and clear implementations should be chosen in preference
    to other implementations that are of greater complexity and possess
    unnecessary complication.
    
    In addition explicit dependences upon state objects are used, instead of 
ML's
    references and arrays.  Ths implies that such objects are explicitly passed
    into functions and returned from them when a state object is to be 
`updated'.
    This requires care since both pre/post state objects are accessable - and
    may get confused or muddled up.  Complication can arise due to having to 
manage
    too many explicitly named values.

    This policy may be relaxed when use of state can be entirely
    encapsulated inside a set of functions.  It is important to understand
    how components rely upon each other - explicitly given dependencies can
    (with care) convey this information without obscuring other aspects of the
    model.

    .state.combinators  To help with describing stateful systems in a
    functional style within ML, we may introduce some functional
    combinators.  These _apparently_ don't do a great deal - however,
    they can be useful in hiding some of the `plumbing':

       fun state (x) = x

       fun T (x, f) = f(x)

       fun cond p t f st = if p(st) then t(st) else f(st)

       infix 3 T

    To show how these apparently trivial definitions can be useful, consider
    the following example:

    Suppose we have the following types and functions:

    (* Here are some state types *)
        datatype my_state = ...
        
        datatype another_state = ...

        datatype yet_another_state = ...

    (* Now for some operations *)
        val myOpn : int -> (my_state -> another_state) = ...

        val anotherOpn : string * int -> (another_state -> yet_another_state) = 
...

        val yetAnotherOpn : int * string -> (yet_another_state -> my_state) = 
...

    The types of the operations are in the functional `State Transformer' style.
    The functions show how state information is computed from input state to
    yeild an output state.  Now we can write the following sort of
    `sequenceing' expression in definitions:

        fun compositeOpn (k) (init) =
              state (init)            T
              myOpn (k + 1)           T
              anotherOpn ("foo", 23)  T
              yetAnotherOpn (42, "bar")

    Note that compositeOpn is again a `State Transformer', this time with type
    int -> my_state -> my_state.

    The important point is that intermediate state values are not
    unnecessarily exposed and so we can be certain that they are used 
correctly.

