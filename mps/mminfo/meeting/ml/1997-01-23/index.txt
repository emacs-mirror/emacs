         NOTES FROM THE MM/ML REQUIREMENTS MEETING, 1997-01-23
                         meeting.ml.1997-01-23
                             incomplete doc
                           richard 1997-01-23

INTRODUCTION

.intro: Richard Brooksby <richard>, Gavin Matthews <gavinm>, and Nick Barnes 
<nickb> met Dave Berry <daveb>, Jo Brook <jkbrook>, and Jo Blishen <jblishen> 
to discuss MM/ML requirements.


NOTES

daveb: Float Arrays are implemented by using byte arrays and some new built-in 
operators.  

nickb: They can't be distinguished from byte arrays at run-time.

richard: We could use some spare secondary tags.

NickB: New collector is believed to have a better heuristic.  

DaveB: We found good results with stack reduction.  Did you try that?

NickB: I will.  We could also try having a mechanism for providing information 
based on times. This will be easy to add later.

DaveB: The only new requirement is for inter-operability, FFIs, and so on.  The 
current FFI is good but can't pass floats which is a definite requirement.

Richard: Need nore ability to pass values about.

DaveB: Also 32-bit ints.

JBlishen: How does this affect MM?

Richard: It affects how objects are represented.  In DYlan, any objects which 
might escape are registered so they don't move around.  The ML GC is less 
flexible.

JBlishen:  MLWorks doesn't use the MPS, and there are no plans to?

Richard: We will need to assess whether the ML requirements make it worthwhile.

NickB: A good example is if ML required incremental GC, it wouldn't be worth 
adding to the existing GC.

DaveB: We don't know what effects these things would have in the market.

Richard: The existing ML GC has been tuned over a long period.  In effect, it's 
wuite incremental already.

NickB: 32-bit ints.  There's no fundamental reason why we couldn't do them.

DaveB: They're all boxed.

NickB: We'd need non-GC'd values in stack frames.  All the machanism exists, 
including spills.

Richard: It's untested, but is all accounted for.  What arewe going to talk 
about today?  We haven't had much customer feedback.

DaveB: Library support is particuarly poor.  There are MM questions there.  
Linking DLLs with C.  Delivering components.  32-bit ints.  FFI advice.  Tail 
recusion, modulo cons, destination passing style, r4000 allocation sequence (an 
optimization that should be relatively easy to do, rate this one high for large 
effect) mips, 

NickB: Are there any porting plans?

DaveB: No.

Richard: Most important:
 - DLL support
 - 32-bit ints and unboxing
 - Possibly MIPS allocation sequence, should be easy

DaveB: We have Matthew working a a series of small optimizations.

NickB: Dynamic libraries in DYlan.  Eveny language has a dynamic library, 
including C.  When it is first loaded, it gets to run some code in the DL, 
which starts up the Dylan MM.

RIchard: It's not quite like that. We supply the MM as a library, and it 
becomes one part of their run-time support.

NickB: Can be done in M in much the same way.

Richard: There are issues like, what if we run with more than one MM?

NickB: Supposing two companies use MLWorks to make components, what happens 
when they're linked together.

RIchard: At the moment MLWokrks will trample all over the place, assuming it's 
the only GC.

DaveB: Couldn't we just have the MM as an independent compatible DL.

Richard: This is a QA nightmare, and prevents you changing your interfaces.  
Also, what if two independent components use the same MM but don't know about 
each other?

NickB: I hear there are problems with the run-time when you try to do this sort 
of thing that aren't MM, such as signal handling.

Richard: You're sharing resouces: threads, memory, signal handling.  Also FD, 
semaphores, X connections.

DaveB: Part of out FFI plans are to separate out things bound up in the 
run-time, such as the X library.

Richard: It's safe to say that you could never make everything transparent and 
working.  You could warn the use about potential problems with the signal 
handler, etc.  There will always be something.  For example, you can never call 
the next signal handler in the same way as the kernal.  On Windows, it's 
slightly better in some ways, and a lot worse in others.  There is an exception 
mechanism, as ML, sthrowing up the stack, but signals must be handled in one 
thread.  Different threads signal handlers don't interact well.  On the Mac, I 
don't know.  With memory, in win32 you can reserve chunks of address space, 
independently of reserving chunks of memory, and you're safe.  You can do this 
very we on Unix.  So you can have trouble with memory clashing.  We have a 
method, which is a hack, on some Unixes which involves mapping /etc/passwd into 
memory.  

JBlishen: Why?

Richard: The way we get memory is to map a file into memory using mmap.  We 
want to say, "Give us a space where we could get memory,, and not worry about 
other people getting it."  We therefore map in a file that we don't care 
about.  We then replace that with mappings to real memory.  Windows has 
explicit support for this.  

With MLWorks GC, it's designed, as LISPWORKS, that it is the master.  It 
assumes it uses certain addresses in memory.

NickB: Although this would be easy to change.

Richard: On Windows this is easier.

NickB: There is a related problem that even if we float the chunk of address 
space that we need, it's a big one, say MBs.  If we need a another one we might 
not have enough.  Signal handling is another problem.

Richard: We could get ten or so at the same time.  What does MLWorks do if it 
doesn't oen the signal.

NickB: We try to do the right thing nowadays.  If we can't identify it as being 
in ML, we try to pass it on, but we probably don't get the signal masks right.  
If we have multiple version of MLWorks, the inML variables will be distinct.

Richard: It's not clear what happens with global variables with the same name.

NickB: This is related to the problems of stripping.  Presumably a solution 
will be found.
If this gets stripped out before it gets to customers, then is won't be a 
problem.

Richard: A good test is to run two copies of the same thing, and see whether 
they interact as all.  What happens with the excpetion handler in Win32, are 
they delat with at all?

NickB: No, I don't thinks so.

Richard: We use signals on the Sparc.  If we can't pass them on, we can't GC.

NickB: If necessary, we could write a little piece of code to handle this sprt 
of things and change the way our signla handlers work.

Richard: This could be a major amount of work.

Essentially we're pushing these systems to do things they weren't designed to 
do.

DaveB: What about static code?

Richard: If the two pieces of code share the same MM then they'll share the 
same heap.  If they don't, they wont.

NickB: You're free to put code statically into the text segment.  If it isn't 
writable, then space profiling stops working.  

DaveB: I haven't looked into the DL so format al all.

RIchard: It's just ane executable format.

NickB: There are case where we ask which memory something is in, and that's 
going to start going wrong, we'll need to patch it up.

Richard: If you ship an ML library, what's going to happen when native threads 
call the library at the same time?  YOu could have a barrier on the way in to 
make it single threaded, but that could cause deadlock.  The MLWorks run-time 
system is single-threaded and non-renentrant.  That would be too hard to fix 
without rewriting it all.

If you call foreign code from multi-threaded ML, can that be re-entrant?

NickB: You only switch between threads at specific points in the ML run-time, 
so I don't think it's a problem.

RIcharD: Use of native threads on Windows, is a matter of getting memory.

NickB; Something to do with win16 trampolines.

Richard.  Thirdly, what if there are two ML threads with different memory 
manangers.  Yield and dispatch will only occur in one of them. There's a tree, 
or a set of lists. 

NickB: So far as each is concerned, the other is like a piece of foreign code.

DaveB: If we use native threads this will disappear.  Solaris threads are 
lighweight.

Richard: The controller in the run-time system for thread dispatch can only 
control threads it knws about.

NickB: If you are prepared to change the way MLworks uses native threads 
instead of our own threads then it would be a lot of work.

Richard: We did it on Windows, because it doesn't work if we don't, but they 
aren't really native threads.

NickB: It gets the system to allocate C stacks.

Richard: How much flexibility do we want to give to the customers?  One 
component, several components with restrictions, or anything they like. The 
last will cost more that we can afford.

NickB: We could design a migration path to end up where Dylan is.

Richard: There are many years of effort in that.  It's a marketting decision.  
Need to spec them for Jo to decide.  Don't need to discuss this now.

[Coffee break.]

Richard: Changing to the MPS will change many of these problems.  The MPS is 
not entirely finished and switching to the MPS is a major undertaking and may 
cause a lot of disruption.  Nick and I discussed hacking it in to see if it 
would limp along.  It would be useful to find out how well the MPS works in 
practice.  We don't get enough usage feedback from Dylan.  

If Nick and I get together for a week on a Socttish island, we can give a much 
better guess of how long it will take.

NickB: My guess is it's 4-6 weeks of Nick and/or Richard to get it working.

Richard: Plus the time to get the performance back up, which might take six 
months.

DaveB: We're also talking about performance on several platforms. When could 
you start this?

Richard: It depends on plans, I can't see it happening this year.  It would be 
worth our whle to spend the week in about six months.  There will be some 
duplication of effort working on the ML GC.

Fixing the memory hog problem, has been fixed partially on win32.

Inter-operating can be fixed for Unix in a fortnight.  

Making it less of a pig will take another fortnight for a simple solution and 
four weeks to do it better.

DaveB: signal handlers.  Is that a GC ISSUE or runtime.

RIchard: A mixture.  It's in the run-time part of the code, but it effects the 
GC.  It'sa couple of weeks work, but mainly ML's job.

NickB: Stephen should do it.  The two week solution is the co-operative, we all 
know what we're doing solution.

Richard: Add another week to write a spec for the situation.  Will never be 
completely transparent and clean. 

NickB: Users of components need to know that we need SIGENT [?] on the Sparc.  

Richard: Debuggin something with the MPS, we'll see a lot of signals; this 
could indicate faults in the program.  Documentation issue, possibly mechanism.

You can call the MPS from multiple threads and it will work.  It enforces 
single-threadedness within itself.

DaveB: Sounds like we need another person.

Richard: It depends how it all totals up.

DaveB: The maximum total here is ten weeks.

Anything else on DLs?

Richard: Have a chat to Dylan.  Their design started out with this in mind.  
David Gray, Tony, Andy SIzer.

NickB: Need to go away and think about how to fit ML code into a DL, and call 
it from elsewhere.  Then we can discuss the MM issues.

NickB: New GC.  The current GC copes with lots of generations, and creates them 
dynamically.  It creates new generations using figures it makes up in its 
head.  The standard ML NJ collector did this very badly. 

The new GC is about making these decsions better.  It has a notiion of time, 
governed by allocation. It measures each GC on a generation, mean age of 
objects, time taken for GC, and stores this in tables.  It uses these to 
predict effort and reward for GC for cost-benefit analysis.  Part of this 
involves pause restrictions.  That test is not performed at all by the current 
GC.  It can take arbitrarily long.

Richard: Nick's GC is essentially an adaptive system.

NickB: It regards recent information as more reliable than old.

Richard: We should talek to Loiuse about this.  I've talked to various people 
in Adaptive Systems about this and it's sound.

NickB: The other issue is whether you collect into the next generation, or 
create a new one.  The interaction between this and the aging is not right.

Richard: The old one had hard-wired decision making with one parameter that 
controlled eagerness.

NickB: I still have the design and the implementation that will have suffered a 
little bit-rot.  It will take a week to bring up to date, and week to fix the 
pathalogical behaviour, and a week to test and QA.  That makes a month in total.

Most of the total GC time (85%) is spent collecting the nursery.  The NJ GC 
doesn't bother counting this in its GC time.  

Measurements.  Almost all collections are "promote 0", so the overall GC time 
won't change much, but the long pauses will go away.

There's a 16M chunking behaviour.  This is a separate problem.

Benefits:
  - Better pause distributions
  - Small reduction in overall GC time
  - More use control
  - More ML-ification

Richard: Don't forget to measure space efficiency of new algorithm.  Probably 
no worse, but let's check.

DaveB: If we reduce the amount of allocation we do, how does that effect GC?

NickB: It will adapt because it's a learning algorithm.  There may be an 
initial period.  You can tweak the value in the run-time.

Richard: The fact we have more that one generation mitigates this.  Also you 
can adjust the creation space size, to allow for many temporary objects.

NickB: Other change is that creation space size is no longer hard-wired.

Richard: It's more flexible.

JLBrooks: Does it keep any record, or does it have to warm up evey time?

NickB: It could do, but doesn't at the moment.

Richard: The initial set will resemble current behaviour.  It's very hard to 
teach people about this.

JLBrooks: How do adaptive systems present their stuff?

Richard: Would take a chapter of the documentation.

DaveB: If unboxing changes allocation, then we can have a look at it.

NickB: I could add a couple of numbers to the four line report.

Richard: If you emit total GC time and longest pause, and ask to report large 
values.

DaveB: Would it make sense to use a non-copying generational collector?

Richard: It would increase your run-time by a factor of two.

NickB: It would make thinks much slower.

DaveB: [points] Don't document that; it's a stupid question.

NickB: You asked a question about reducing the spikes in the VM usage.  I call 
this dynamic two-space sizing; a solution.

What happens at the moment is that a generation of, say 10Mb, collection with 
copying needs 10Mb of swap (for no checking).  This can be bad in certain 
circumstances.  There are a few solutions:
  1- Tell the GC that generation shouldn't get so big;
  2- Put a check in the copy code --- will stretch runtime, but shouldn't be 
too bad (action: do some sums);
  3- Dynamically allocation swap as you go along by catching SIGSEGVs.  At the 
moment we alocate C stacks of 64k and running out is fatal.  We could could fix 
both at the same time.

DaveB: We have problems with stack extension.  There are two approaches: cope 
dynamically, or tell the use to run again with a larger stack size.

Richard: This latter is poor for a development environment.  It's bad for long 
term liability.

NickB: We have code to check for stack overflows even when we don't have stack 
extension. The determination of where this code need to be is not good.

DaveB: better now

NickB: Need to handle asynchronous [?]

NickB: (1) can be done by adjusting the minusLimit, switch on runtime.

(2) Computation to do; can do this afternoon.  Check in assembly on several 
platforms will take a week to get code in, a week to check.  Call it three.

(3) Fixing C stack may not work on Solaris 2.X because you get trap for 
stack-push from inside on SunOs 4, but this went away.  Three or four weeks.

Richard: Not clear that (3) will perform better.  We can do some calculations.

NickB: Hopefully you never trip the SEGV handler.

Richard: BOE: Signal handler takes 5-10 instructions, 300 for copy.  Win for 
2000-3000 object without tripping handler.  This sound best.  Exacerbates FFI 
problems.

NickB: Have restriction on FFI code that it can only push so much onto the 
stack.

DaveB: Reports of user not getting good performance. Didn't play with VM.

Richard: Ought to be at the front of the manual.

NickB: Ought to be able to detect VM from run-time, and adjust.

Richard: Find out whether it's possible in a day.

DaveB: 1 month heuristic improvement, 3-4 weeks for reducing spikes, DL support 
6 weeks MM work plus more ML.

Richard: MPS estimates are very uncertain.  

DaveB: FFI and 32-bit integers.  Stephen is working on the FFI, and is getting 
stern restrictions.  For each C type, you can create an ML structure, and pass 
it.  It's donw with .h parsing.

Richard: DYlan do this .h parsing as well.

NickB: I believe this is a disaster.  You can't parse windows.h, for example.

DaveB: We can't afford to have someone doing .h files for a year.

Questions arose about MM.

0:Can we automatically reclaim objects allocated in C?

Richard: I we do the MPS, maybe.  Boehm-style.

DaveB: Boehm put in a tweak to black-listing.

GavinM: It's in the glossary.

DaveB: All objects allocated by foreign code has to be freed manually.  Can it 
be safe?

NickB: Our implementation of malloc/free does this.

DaveB: Can they use their own malloc/free?

NickB: Not if you want the answer to the previous question to be "yes".

DaveB: Their own malloc/free may have extra features, debugging, or optimized.

Richard: This is tied into having a friendlier GC.  We're overriding 
malloc/free global resouces.

NickB: I'd rather not have our own malloc/free, but it does stop us going 
SEGV.  Removing this will cause ML interesting problems.  On IRIX libelf does a 
double free.  Some X library (MIPS?) with -g frees unmalloced block.  If 
malloc/free handles this, the that's fine.  Ours does.

Richard: We should allow other people to use their own malloc/free.

Estimate?  

NickB: Comes under multpile ML MMs.

Richard: A day for the symbol changes, then make sure we don't break -- same 
work as before (not hogging address space) plus 2-3 days.

NickB: Alternative is a quick hack to change the symbol and leave the first 
16Mb to system malloc.  A few days.  Dodgy.  Assumes that foreign code malloc 
uses sbrk() etc.

DaveB: Question 3. These are about whose responsible for different objects.

NickB: My opinion is that it's the responsibility of the programmer whose using 
the FI to decide.

Richard: I agree.  Give the programmer the flexibility to decide.

DaveB: Steven's example is the dbm database.  The ML programmer must call 
dbm_close.  Brian suggested that a co-operative model where objects are 
declared would be good.

NickB: Dangerous.  We shouldn't assume things about foreign objects at all.  
Leave it to the programmer.  If we have finalization we can give the programmer 
extra flexibility to cope.

DaveB: Steven has a general comment.  We need more examples to see how much of 
an issue manual freeing is.  It may be that it is the programmers 
responsibility but the problem is manageable (small).

Richard: I advise not treating memory specially.  All resources are similar 
(fds, windows, etc.) and I think they should be treated uniformly.

DaveB: Maybe.

DaveB: Question 4.  What happens to objects allocated in ML and passed to C, 
and lost by ML.  We need to make sure that ML doesn't recycle these objects.  C 
objects allocated in ML and passed to C.

Richard: Can you tell when you allocate them when you're going to pass them to 
C?

DaveB: Yes.

Richard: Then malloc them.

NickB: You'll get a memory leak.

Richard: You need to define who is reponsible for freeing them in this 
situation.  Again, give the programmer the responsibility.

NickB: The user of the foreign interface needs to specify, if there's an 
external allocator, what that allocator is.  They might have a debugging 
malloc, for example.

DaveB: Should expand the FI proposal (doc) to explain how this is done.

NickB: Do we give proper ML objects to C?

DaveB: I don't think so.  What would be the problem?  (A new question.)

Richard: I would advise against it.

NickB: In a version of Brian's foreign interface we did, and there was some 
kind of mechanism which we provided to the outside world for looking at ML 
objects and picking them apart (libml).  The solutions in there are the ones 
you'll need to pursue.

Richard: Never let C get direct hold of them, or you lose control.

DaveB: Question 5. Should we provide malloc and free in ML for C objects.  
Brian says this might encourage people to manage all their C memory from ML.

Richard: What does he mean?

NickB: If there's a foreign interface you can't prevent people from doing 
this.  Should we provide them specially?  Well, probably no.  The only question 
is which malloc and free.

DaveB: How do we create C objects from ML?

Richard: Suggestions: Don't call our malloc "malloc" to avoid customer 
confusion.  Also, why not just provide an interface to all of the C library, 
then it's clear where malloc comes from.

NickB: The user should be able to say how ML allocates C objects, if it needs 
to.

NickB: On the subject of our malloc/free.  I think they shouldn't be called 
"malloc/free" but we should keep them because it good to have control.  We 
should keep them for ourselves, even if motif etc. link with the C malloc and 
go SEGV we want to keep our own.

DaveB: Finalizers, but first lunch.

[Lunch]

NickB:Do you want finalization?

DaveB: We've been asked for it for FDs by one person.

Richard: The main purpose of finalization is to manage a resource. You have a 
proxy object and cler it up when you've finished with it.  FDs are a good 
example, as are C heap objects.  It gives programmers and extra way of dealing 
with resources.  I suggest you don't specify that it'd specific to FDs.

NickB: A related subject is weakness.  MLWorks already has them, but they 
aren't exposed to the language.  This could be done really easily.  We have a 
good undertsanding of the interaction between finalization, weakness, and GC 
from Dylan work.

DaveB: Can we avoid finalization ordering problems, and cycles?

Richard: We can supply something unordered.  It is possible to do ordering, but 
it's expensive, not generally useful, and nothing to do with GC.  For example, 
child and parent windows.  Finalizers on windows are a bad idea.

We don't understand this well enough, and can't guarantee to provide ordering.  
You can generally do it yourself by designing your data structures properly.

How much work is it?

NickB: We need to sort out the weakness/finalization relationship.  We could:
  - Make weakness a visible property
  - When something becomes dead in a weak array, it gets finalized.

Richard: Problem with referring to object in finalizer.  Alternative is to 
distinguish weak and final, and store final messages, and do what you like 
later.  The object stays alive.  A final reference notes that the object would 
have died without the reference.

DaveB: MLWorks sees something that creates a final ref, and then voids it.  

Richard: Have final array with anything in it, and with the asychronous event 
mechanism, you have a single function which handles all finalizers and 
dispatches. The user might see a pair of vector and function.  If you have a 
constructive type, you have a non-value carrying value of that type.  No need 
for magic value.

DaveB: Seems a bit odd to show to the user.

Richard: Messages get back through AEM like signals or whatever, and get 
received at an appropriate point.

There's another concept.  Weakness and message are orthogonal.  Could have 
message for weak reference without being able to look at it.  Say which you 
want when you allocate.  Could be done in two weeks.

DaveB: Nick talked about a scheme for removing entry lists.  

Richard: This is related to the cons of tail.  Vocative [?]

Have to look at this carefully.  Cost of GC could outweight it.

This scheme is to replace them with an entry buffer/table.  It would allow any 
object to be updated, not just arrays, and would make array update quicker.  
Arrays would probably no longer need to be special in representation.  The main 
problem is that, if you update an array a lot, then you will enter GC without 
necessarily generating garbage.  There may be a way round the problem.  By 
keeping a slightly special representation of an array, like a flag, then we 
could solve it, but it would still be a problem for tuples.  Array updates 
might be a little slower.  

Updates are done on the side with a special buffer.  

NickB: Need ML support, plus MM four weeks.

Richard: We need to be careful, say 8 weeks, with code review

NickB: Locatives.  My favourite is for a locative, i.e. a pointer into the 
interior of an object, to have a special primary tag, or which we have a spare 
one.  We need to change fixReg.  They only ever appear on the stack:
  - Allocate object
  - FIll in all but one slot
  - Put nil in slot
  - Take pointer to slot
  - Pass pointer to another function
You know pointer is interior, and you can scan back for header.  Update through 
the locative, causes entry lists problems.

RIchard: Can only do this with records, and certainly not with pairs, because 
you can't scan back for a header.  Scanning back in a code vector is not really 
scanning back. Scanning back in a closure is special.  Scanning back in a 
general record has never been done, but should be possible, except for pairs.

Could put the back pointer offset in the slot.  Once you've written a locative, 
you can't find the header again.  It would be a one-shot update.

NickB: The problem is updating an old object with a pointer to a new object.

DaveB: They keep a list of things that may have locatives into them.  They 
argue that most of them never get out of the creation space.  The inefficiency 
is overwhelmed by the other improvements.  

NickB: Only writes to locatives that have survived one GC.

Richard: Can spot this magic back pointer when we copy.  Make our fixup code 
more complex.  This is an example of what we talked about where you have both 
mechanisms side-by-side.  If you update an array so it only points at itself, 
it will be collected.

If we do locatives, it will be a special case of something we can generalise.

Estimate:
  - New primary tag on stack only -- 
  - Compiler representation of back-pointers in records -- 

DaveB: The common case is create a locative; recurse on the stack and rplacd.

NickB: The next most common [?]

Richard: Could do generation check on locative update.

NickB: Means ML code needs to know how generation code works.  

Richard: Would mean that array updates wouldn't need to go on list.  Would help 
for locatives.

NickB: No point in doing locatives if it's going to take 20 instructions.

Richard: Locative is like a reference to geneneration -1 or 0.

NickB: Only one locative in each object.  

Richard: Keep locative and fix one end.

Locatives need to be one-shot if we are two have the back pointer mechanism.

The worst case is half the size of the heap.  

Estimate --   
  - design: 1 week
  - new representation: 
  - change fix, fixreg: 
  - compiler changes: all 1 week
  - entry table: 2 weeks
    o Managing it: 
    o Doing the right thing in scanning: 
 - testing: 2 weeks

Total: 6-8 weeks MM

DaveB: Part of the gain is potentially clearer code.  It would be interesting 
to see if B-Tree could be written in a non-constructor style, in CPS.

Richard: To get rid of the entry lists.  4 weeks tops on top of that.  2-4 weeks

NickB: Sparc assembly is ghastly.  Trapping allocation sequences:
  - decide sequence
  - make compiler change
  - write signal handler to store PC and call magic assemblar
  - write magic assemblar to store other state and call GC
  - write code to fix up as if allocation had succeeded
  - change space profiler to recognize allocation code
  - change assemblar run in space profiler at allocation sequence
There are leaf and non-leaf cases for all of these, but will be very similar 
and might be the same.

Trap handler stores PC and register to store value into.

On Sparc, size of object is either inline in allocation instruction or in 
register.  Signal handler figures it out, and sticks it in a slot.  This is all 
MM except the compiler change.  Estimate: 1 month  mostly design, and staring 
at register layouts

The MIPS is a simpler model because it doesn't have register windows, and has a 
spare register.  

Can measure expected saving in design stage.  

DaveB: Unboxing and region inference.  Would like to improve support for 32-bit 
unboxed integers, for inter-operability.  

Richard: We already have support, but the performance is not good enough.  We 
are just sticking them in strings?

DaveB: Byte arrays, because strings are updatable.  Integers are not.

Would like default integer type to be 32-bit.

Richard: If we can get the analysis good enough.

DaveB: Makes calling C easier.

Requires calling non-GC registers.  

Richard: Need to get that working. Nothing or some work.

DaveB: Unboxed integers on the heap.  Conservative GC is a radical approach.  
May have too high overheads or development costs.

NickB: MPS would solve this.

Richard: MPS will cope with arbitrary mixtures of ambiguity and weakness.

DaveB: Generate type information and records it for values on the stack.  

NickB: Once you know the type of your root set, you know the type of everything.

When you have a pair of a 32-bit integer and a float, how does that look?  12 
bytes?

DaveB: Don't know.
[...]
Richard: A Cheney scan would forget the type of the objects being scanned.  We 
can find out how this works.  

On the Sparc, there is very little overhead in the tagging.  

NickB: It's a question of how many pointers you have to dereference.

DaveB: Arrays of floats become float arrays.

NickB: It's a big win in locality.

Action to find out what TIL does.

A related issue is what several other people do, but we don't, is to improve 
stack scanning by only scanning things that are live.  E.g. the compiler knows 
when registers are no longer alive.  Stack-frame liveness mapping.

This doesn't make your GC go much faster, but supposedly improves your space 
efficiency.  On the Sparc, it makes function entry faster, because registers 
don't need to be cleared or spilled.  Compiler must map from PC to register 
liveness.  PC offset within parent code vector.  

NickB: Would be a bit of an experiment.

Richard: Could simulate effect by changing code generator to smash old values.  
This would give the effect on the heap.  If nothing happens, we don't get a 
heap win.  The next thing is to scan an image, looking at entry sequences and 
count slot blits.  Multiply by call counts.

NickB: Experiment 3 is do profile and do statistical analysis of slot blatting.

Net GC effect will be small.

Richard: Almost certainly a win.  Question is how much.

Estimate a week of fiddling round with stack scanner.  Most work in compiler.

Richard: Dylan unboxing.  They have quite different representation of objects.  
GC uses wrapper.  [ Dylan container format ]

Original design didn't have an out of line format and had a fast scanner like 
ML.  Pattern every 32 slots.

DaveB: Ideas.  Monomorphic arrays are always flat. [?]

Can treat 32-bit integers are special representation.

Records are harder.  When a record is an argument, instead of having a pair, 
combine tag with header and value.  This is not a valid top-level value.

f(Px)-> ... x ... has to do some magic.

Richard: This is why you can't do two byte conses.  Signature hiding.

Can probably fix it with partial evaluation of functors.

DaveB: If most tuples are either arguments to functions (optimised to 
registers), or arguments to constructors (optimised to form above), you could 
divide value into two areas with unboxed and boxed areas.

Richard: Dylan can't do this because they must support inheritance.  ML has 
enough type information to do this.  Probably don't need pattern stuff.

DaveB: if we have:
  datatype intfoo   Foo of (int * alpha)
it will be stored unboxed.  

Richard: Isn't the boxing independent of the constructor?

DaveB: You need to know the type.  

Richard: Separating boxed and unboxed areas can be done everywhere.

DaveB: I don't know about top-leve records.  You can't reorder the elements.  
Such as type records.

Richard: You almost always have enough information to make some sort of 
representation.  You must have the same level of knowledge everywhere.

DaveB: This often makes a performance improvement even within a module.

NickB: Assumes you don't pass large polymorphic stuff around with any frequency.

DaveB: Consider first.  Either need special cast when applying first, or ensure 
boxed at first level.

By top-level I mean record values that aren't part of a constructor, that you 
deal with directly.

Richard: But you can do the same with with functors.

DaveB: I thought most of these values were parts of constructors.

JLBrooks: I've read various papers on this.  Most of them are conservative and 
box.  

Richard: So they need complete local knowledge.  Is that enough?

DaveB: It gives good speedups.  But how should we store these things on the 
heap?

Richard: I'm not convinced this is simple.  If you're going to have a headless 
pair, it will be polymorphic.

DaveB: We can use 30-bit integers for constructor tags.

Pair in the sense of an 8-byte quantity.

  Pair 1: Float, 2 unboxed ints, tag x boxed int
  Pair 2: Tag x boxed value, unboxed int x boxed value
  Pair 3: Boxed value x boxed value

Richard: This won't work for scanning.  You might do this for lists _only_:

  cdr | 7
  32-bit car

and this for constructors:

  (contag << 6) | header7
  32-bit carried value.

Combining this with locatives requires overloading of pointer7 (when on stack) 
and a special form of backpointer for the case of a locative into the cdr field 
of a list.

Other options are BIBOP, including automatic BIBOP on promotion of headered 
pairs.  Whacky.

DaveB: Region inference.  Mads Tofte will release his region-inferencing 
compiler for whole of SML on 1997-04-01.  They delay compilation of functions 
until application.  They eliminate poly-equal by passing "dictionary functions" 
around.  They don't have run-time tags.

Richard: Compile times?

DaveB: Compile times are slow O(n^4).  They're aiming to combine region 
inference with GC.  They're producing a manual about programming with regions 
to explain what you have to do.

Richard: I claim that not all programs can be transformed this way.  It's a bit 
like programming with malloc/free (but nowhere near as extreme).  You have to 
change you program to be stack-like.  It will be a restriction on flexibility.

DaveB: There are faster algorithms which detect stack-like behaviour.  Their 
results are also impressive.

Richard: It's a tool in your toolbox.  You need to decide when to apply it.  
There are two things: 1. when to apply it, and 2. can it be toned down and 
combined with GC?  Maybe there's an O(n) analysis which will tell you whether 
to apply region inference.

What do you want to do about it?

DaveB: Not very much.  At this stage it's an optimisation to look at.  We 
should consider it.  If we _do_ go ahead we'll need some support from MM.  Mads 
says a region profiler is essential to see what's going in which region and 
optimise.  Useful to analysing the memory behaviour.  It's a lot of work to 
develop.

DaveB: What about stack allocation?

Richard: It doesn't win as big as you might think.  The fact that continuation 
allocation is almost good enough indicates that stack allocating activation 
records isn't even _that_ big a win.  We can estimate the benefits if you can 
analyse the objects, but the only way to really tell is to measure it on 
various platforms.

DaveB: How long for the special pair encodings?

Richard: About a week.  And for record splitting (tagged, untagged), about 
another week.  Debugging non-gc register stuff is about another week.  So 
unboxing support is quite cheap for MM.

DaveB: Our release schedue.  1.0r1 indended Immediate fix for UltraSPARC.  
1.0r2 is bug fixing with some performance improvements (floating point, in 
particular) and OpenWindows support.  Scheduled for March/April.  1.1 in 
August/September, GUI revised for Windows in particular, new FI, various 
optimisations, etc.  May drop support for old platforms, such as R3000.  Nick's 
new GC algorithm goes in here too.  We're also reviewing (considering redesign 
of) various things: build system, delivering libraries, design of debugger.

Richard: I need to tell you how much time you can have and you tell us what you 
want.  You have the estimates.  I guarantee you'll get three months of Nick 
before the end of the year.  We may be able to arrange some weeks before June.  
I will need to talk to Nick about the details when I get back to Cambridge on 
Tuesday [Nick's gone home to see his baby].  You get 11 weeks this year.



