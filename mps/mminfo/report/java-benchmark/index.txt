              THE JAVA BENCHMARK SUITE -- PROJECT SUMMARY
                         report.java-benchmark
                             incomplete doc
                             lth 1998-09-23

Overview

This is the project report for the Java Benchmarks project.  It describes what 
was done and left undone, and then moves on to discuss desirable aspects of 
benchmarks and performance measurements in general.  Finally it discusses 
future work, related work, and superficially critiques some of the benchmark 
programs.


1  WHAT WAS DONE

The purpose of the project was to create a benchmark suite for Java memory 
management functionality.

The project was initiated at a meeting between GavinM, RIT, PTW, and LTH which 
resulted in an analysis document (analysis.java-benchmark(1)) and subsequently 
a progress plan (plan.java-benchmark(1)).  The project proceeded in four phases.

1.1 Phase 1: Initial Design, Research

I spent several days searching the World-Wide Web for existing Java MM 
benchmarks; this search turned up little, and the ones found were very weak.  
Notably the "standard" Java benchmark suite "Caffeinemark" 
(http://www.webfayre.com/cm), which used to have a GC benchmark, does not have 
a GC benchmark in the most recent version.

I then created a list of MM features that it would be desirable to test, based 
in part on a list of suggestions from PTW.  This resulted in an informal 
design, some prototype benchmark programs, and finally in a design doucument 
(design.java-benchmark.arch(3)).  The design document was presented to the MM 
group as a RFC.

1.2 Phase 2: Initial Implementation

While the RFC was being reviewed, I implemented large parts of the initial 
design.  When comments on the RFC had been submitted by MM group members, the 
design and programs were reviewed and updated, resulting in an initial 
benchmark suite of 17 programs.  Of these, two were specific to Java 1.2 and 
not implemented, and two were translated from Scheme (these are variations on 
the classic Boyer benchmark).

1.3 Phase 3: Implementation and Design Review 

I ran the initial implementation of the benchmark programs on several Java 
systems and machines to see how they held up in practice.  In particular, in 
this phase it was important to determine whether the benchmark results were 
repeatable and believable.  As a result of this, a handful of programs were 
rejected as unsuitable (generally, the results were not repeatable).

1.4 Phase 4: Release, Review, Refinement

The fourth phase is ongoing.  The benchmark suite was released to the MM group 
on 1998-08-12; unfortunately feedback had at the time of writing (1998-09-18) 
not been forthcoming.  I have been refining the programs, fixing bugs, writing 
output postprocessors, writing documentation and design docs, and generally 
cleaning the programs up.


2  WHAT WAS NOT DONE

Several things were left unaccomplished in the project; some of these (missing 
programs, testing on more Java systems) were due to lack of time or other 
resources, others because they would be inappropriate without more feedback, 
review, and testing (glitz).

2.1 Core Functionality

Two aspects of the core of the project are currently not done.  

First, no benchmarks were designed or written for Java 1.2.  This is 
unfortunate because Java 1.2 adds several interesting memory management 
facilities: weak references, reference queues, and a memory-advice interface 
that allows a program to detect low-memory conditions before they become 
critical.

Second, while the benchmarks were tested on many Java implementations, no 
implementations that cost actual money were tested, nor were any 
implementations running on Macintosh.  That said, free evaluation versions of 
commercial products were used during the test (SuperCede, for example), and 
systems tested include high-quality implementations like Microsoft's jview.

2.2 Gilding

For the benchmarks to be useful or at least interesting to the general public, 
eg, someone who has a couple of Java system and want to find out how how they 
stack up against another, too much detail will likely be distracting.  While I 
have written a number of analysis programs, these too are too obscure and crude 
to be used by end-users.  The output needs to be condensed until it is simple, 
and preferably anything that can be shown graphically, should be.

I am still of the opinion that less processed results need to be available, as 
serious people will want to inspect them.

PTW and I have discussed the desirability of having a benchmark present a 
single number per test, where that number represents the benchmark result 
relative to the result on some reference platform.  No work has been done on 
designing such an output module.  Also, not all benchmarks perform measurements 
that can be reduced to a single number.


3  BENCHMARKS: REQUIREMENTS AND ARCHITECTURE

This section discusses at some length the requirements for the benchmark suite 
and how programs might be written or adapted to satisfy the requirements.  In 
particular, the difference between "real" programs (complete applications or 
close approximations thereof) and "synthetic" programs (abstract models of 
applications in the best case) is treated at some length.


3.1 Requirements

Initially, the following ambitious requirements for a Java MM benchmark suite 
were established (analysis.java-benchmark(1); this is a partial list):

.req.public: The benchmarks should be publicly available.  Implication: 
Proprietary programs cannot be used.

.req.impartial: The benchmarks should be impartial.  Implication: Programs 
cannot be tuned for a particular implementation.

.req.portable: The benchmarks should be portable across platforms and Java 
implementations, and should run both as Java Applets and stand-alone programs.  
Implication: The programs cannot rely on the availability of a file system, 
standard I/O, a network, or a GUI.

.req.comparison: The benchmarks should allow meaningful comparisons to be made 
between the memory management abilities of different JVMs.  This includes 
comparisons across platforms (these may require more human interpretation).  
Implication: Detailed information about the execution must be reported.

.req.reproducible: Results obtained from running the benchmarks should be 
reliably reproducible.  Implication: Programs cannot rely on random behavior or 
system-dependent functionality.

.req.scaleable: The benchmarks should scale up (and down) to systems of varying 
sizes.  Implication: Programs must have a non-fixed workload.

.req.java-version: The benchmarks should cover all versions of Java.  
Implication: There must be multiple versions of the benchmark suite.

.req.coverage: The benchmarks should cover all Java functionality that is 
relevant to memory management.  Implication: There must be multiple benchmark 
programs.

.req.reality-check: The benchmarks should, at least to some extent, reflect 
typical or likely behaviour of real-world Java programs.  Implication:  
Programs can't just be dumb loops.

.req.easy: The benchmarks should be easy to use.  Implication: Programs must be 
packaged in a ready-to-use form.

Of these requirements, .req.portable (second clause), .req.comparison, 
.req.scaleable, and .req.reality-check are particularly demanding, since they 
constrain the internal structure of the programs.


3.2 What are Suitable MM Benchmarks?

[ @@@ Needs work ]

It is interesting to entertain the idea that benchmarks could be adapted from 
existing, freely available programs, both because that would reduce the amount 
of work and because such programs would in principle be better predictors of 
real-world performance than synthetic benchmark programs (see discussion below).

In the context of Java, there are already some large, public programs 
available.  Many of these large programs seem to be Java compilers of one ilk 
or another (Kaffe, guavac) or development environments and similar interactive 
and/or proprietary systems.

When assessing the suitability of these programs as benchmarks, the following 
questions must be answered:

(1) How typical are these programs? 
(2) How suitable are they as _memory management_ benchmarks? 
(3) How much do they need to be adapted to be useable as applets, say? 
(4) Are programs whose object code form occupies several hundred KB really 
suitable as applets?

I have not spent time looking for large programs to use as benchmarks. I note 
that the SPEC benchmark suite (see section 7), which probably has fewer 
requirements than our benchmark suite, consists largely of benchmark "kernel" 
programs (see section 3.5).


3.3 Problems with Synthetic Benchmarks

Synthetic benchmarks have well-known problems with respect to how well they 
predict the performance of "real" programs: they tend to stress the allocator, 
garbage collector, operating system, and hardware in atypical ways.

First, synthetic programs tend to sharply increase the ratio of allocation to 
other computation, thereby usually increasing the collection frequency.  A 
collector that has good performance under moderate allocation may not do well 
under rapid allocation.  This is not simply a matter of collector overhead.  
It's legitimate for the GC designer to assume that e.g. a full GC happens only 
rarely, and design to this.  If the assumption is true for most programs, but 
not for the benchmark, then the benchmark may give the GC an unfairly poor 
showing.

Second, synthetic programs may have unrealistic object lifetime distributions, 
again thwarting reasonable design assumptions like relying on the weak 
generational hypothesis (see discussion below).

Third, synthetic programs may have unrealistic object size distributions, e.g., 
using only a single (often small) object size. This will give an advantage to 
certain kinds of allocators, like those that use Quick lists, again skewing the 
results.  It will also give an edge to collectors that are tuned for small 
objects but not for larger objects.

Fourth, synthetic programs have unrealistic locality characteristics, both for 
code and data: code is typically very small, object turnover is high, and the 
unusual size/lifetime distributions may result in peculiar data reference 
behavior.  Abnormal reference patterns affect the memory hierarchy but may also 
negatively impact collectors that use page protection, since the page 
protection exception rate may be higher than it would be in a "real" program.

Generally, research by Wilson et al [allocsurvey] has concluded that programs 
that model object lifetime and size distributions using traditional statistical 
techniques are not reliable predictors of the performance of real programs.  
The former programs tend to have object sizes and lifetimes drawn from some 
probability distribution while the latter often exhibit highly regular storage 
use that a storage manager can exploit effectively and somewhat mechanically.


3.4 Advantages of Synthetic Benchmarks

There are also advantages to writing synthetic benchmarks.  (But note that in 
all cases we may still be analyzing an unrealistic program, so it's important 
to emphasise whether a particular program is thought to predict real 
performance or simply exercise a particular piece of machinery.)

First, synthetic benchmark are controllable.  A benchmark can be written to 
exercise how the system reacts to particular types of programs, for example 
programs with particular lifetime distribution, allocation load, object size 
distribution, or reference pattern.

Second, synthetic benchmarks are analyzable.  The small size of these programs 
makes it straightforward to understand them and makes it at least potentially 
possible to relate behavior to construction.

Third, synthetic benchmarks are malleable.  A benchmark can be written to 
compute the same result in different ways, thereby highlighting how a system 
reacts to the variations (for example, differences between allocation of new 
object and reuse of existing objects).

Fourth, real programs do not always push a language implementation to its 
limits, but a synthetic benchmarks can be written to do so.


3.5 Kernel Benchmarks

A "kernel" is a synthetic benchmarks that is intended to represent a real 
program.  A kernel is the essence of a real program in the sense that it 
represents that part of the real program which accounts for most (or a large 
fraction) of the execution time of the real program.  In particular, object 
allocation patterns, object lifetime distributions, object size distributions, 
and reference patterns will be close to those of the real program.

Kernel programs are tractable approximations to real programs, and as such 
represent a point on the continuum between synthetic and real programs.  Kernel 
programs combine most of the advantages of both extremes, although they is not 
without their problems: 

* Programs are still small.

* Though similar to real programs, they are not real programs.  In particular, 
for a memory-management benchmark, the parts of the real program not present in 
the kernel program would likely be those that compute but do not allocate.

* Compilers may be able to optimize kernels more heavily than non-kernels, 
although this might be less of a problem for memory management benchmarks than 
for compute-bound benchmarks.


3.6 Architecture of the Benchmark Suite

For reasons that should be clear from the discussion in the preceding sections, 
a good benchmark suite should probably include a mix of synthetic and 
non-synthetic programs.  The synthetic programs can be used to characterize the 
system under study, and the non-synthetic programs can be used to predict 
performance on "real" programs.

For simplicity, and because one of the project goals is to provide a suite 
which characterizes the Java MM functionality, I have elected to use mostly 
synthetic benchmarks that test very specific MM features. Additionally there 
are two kernel benchmarks that test overall performance, memory consumption, 
and pause time behavior.

The document design.java-benchmark.arch(3) and the individual design documents 
for the benchmarks (design.java-benchmark.*) describe the benchmark programs 
fully.  In brief, they are:

* HeapSize determines whether the Java system grows and shrinks the heap as 
needed to accomodate the user program.

* ExplicitGC determines whether the user program can reliably force a garbage 
collection.

* HasJIT determines whether the Java system has a native-code compiler.

* IsPreemptive determines whether the Java system has preemptive threads.

* FastAlloc tests the garbage collector's ability to cope with rapid allocation 
of short-lived objects in the presence of long-lived objects.

* HashCode tests the memory manager's object identity hashing implementation.

* Serialize tests the language's object serialization feature.

* Reliability tests whether old garbage is reliably collected.

* PromptFinalize computes the time from death to finalization for a range of 
object ages.

* Boyer is a theorem-proving kernel benchmark; it comes in two variations that 
have different allocation and lifetime patterns.

* TextSearch is a server-like kernel benchmark with a large, stable database 
and a moderate amount of short-lived allocation.

The programs use a common infrastructure for code sharing and runtime 
environment independence; this infrastructure is described in 
design.java-benchmark.framework(0).


4  MEASURING AND INTERPRETING

4.1 Programs Abandoned

Several benchmarks were designed, implemented, evaluated on several Java 
systems, and subsequently discarded.  In most cases, the reason for discarding 
a program was that it did not reliably measure anything.  

For example, the ObjectSize benchmark was designed to compute the per-object 
allocation overhead, if any.  To do this, the program allocated N objects of 
known types, made a reasonable estimate of the space S required by the 
user-visible fields, computed the difference D between free space before 
allocation and free space after, and concluded that the per-object overhead was 
(D-S)/N.  Each test was run a number of times and the most typical answer was 
chosen; this trick was intended to factor out noise.  Finally the result was 
subjected to a reasonableness-test that set an upper bound for how large the 
per-object overhead could be expected to be.

However, in practice the results varied sufficiently to make me conclude that 
the test was unreliable.  The exact reasons for the unreliability are not 
known, but plausible explanations include garbage collection noise, allocation 
occurring during task scheduling, and system allocation interleaved with user 
allocation (for example, the system extending its handle space).

Another example is the FinalizeAlloc benchmark, which attempted to determine 
the time and space costs of using object finalizers.  The assumption was that 
if there were many finalizable but uncollectable objects, then space usage 
would be higher and garbage collection would take longer than if no finalizable 
objects were live.  Again, these  differences were not measurable.

The general lesson is that Java space and time measurement technology is quite 
crude and that it should not be used to measure subtle phenomena. More on this 
later.  More specifically, the system-level phenomena mentioned above, and 
others like it, might prevent accurate measurements, if the effects measured 
are small enough.

If I had recognized the crudeness of the measurement technology sooner, I would 
have spent less time inventing programs that eventually had to be discarded.  
Even so, in truth it is not clear that programs like ObjectSize could be 
expected to work in all but the simplest systems.


4.2 Java Weaknesses

Java's standard time and space measurement methods are not very sophisticated.  

The method System.currentTimeMillis returns the current time in milliseconds 
since a predetermined time, and is the only method available with which to 
measure time.  While the clock resolution may be lower than one millisecond (on 
NT it appears to be no better than 10 milliseconds and it may be worse), that 
should in itself not be a problem since it is not reasonable for a benchmark 
program to try to get elapsed-time readings on such a fine level.

There is no portable way to obtain the CPU time of the programs, on systems 
where that makes sense, nor the clock tick resolution.

The methods System.totalMemory and System.freeMemory return the total memory 
available for allocation in the system and the amount of memory that is not 
currently in use.  Unfortunately, that specification is loose enough that 
different systems interpret it differently; the following descriptions are 
based on observed behavior:

* Sun's JDK 1.1 interprets totalMemory as the total amount of heap space 
currently claimed from the operating system, and freeMemory as the amount  of 
that memory that's free.

* Microsoft's jview 4.79 interprets totalMemory as the garbage collector's 
current notion of heap space in use; the heap space in use is part of the 
memory currently claimed from the operating system.  (In other words, the 
memory allocated to the heap may be larger than the amount returned by 
totalMemory.)  Jview interprets freeMemory as the amount of totalMemory  that's 
free.

* Supercede 2.02 interprets totalMemory as the total amount of virtual memory 
available to the process in the system, and freeMemory as the amount of that 
memory that's free.

The key observation here is that only in JDK does totalMemory closely relate to 
the amount of memory that's occupied by the Java process.  In particular, in 
jview the value returned from totalMemory may decrease over a period of time, 
but memory no longer in use may not be returned to the operating system.

In all three systems, however, the amount of memory occupied by Java data can 
be computed by subtracting freeMemory from totalMemory.  The specification of 
the methods in the language definition appears to guarantee that this will 
always be the case.

There is no portable way to obtain the program's virtual and real memory 
footprints.


4.3 Non-portable Measurement code

It is possible to create a portable interface to non-portable measurement code 
-- with a portable dummy implementation -- and use the non-portable code when 
it's available.  

The code behind the interface can call non-standard Java code.  For example, a 
Java implementation may provide better measurement tools -- to measure CPU 
time, for example, or time spent in the garbage collector -- in the form of a 
non-standard system class.

In other cases, native code may be used by the non-portable measurement 
implementation: on Solaris, the program can manipulate the /proc file system to 
obtain information about virtual and real memory size, CPU and system time, and 
so on.  On Windows NT, there are system calls to obtain some of the same 
information.  The fact that different Java systems provide different and 
incompatible native-code interfaces is a practical problem only insofar as it 
increases the implementation effort: the measurement code for a single 
operating system may have to be implemented multiple time, once for each 
native-code interface in use.

A useful addition to standalone Java systems would be a non-standard system 
class that provides an implementation-specific interface to mechanisms for 
performance measurement, analysis, and evaluation, both memory-management 
related and not.  

I expect that such interfaces will become more common as people start using 
Java for developing large applications and tune them to run on specific Java 
systems; intelligent programmers should demand them.  It's a travesty that so 
few manual memory management packages have tuning hooks.

At the present time, the SuperCede Java system provides a primitive example of 
such functionality: a class "Gc" that has methods for turning on and off memory 
management tracing to a file.  The trace information is obscure, largely 
undocumented, and machine-unfriendly, and is probably not helpful in tuning 
beyond telling you that "all your time is spent in the GC", but at least they 
are providing something.


4.4 Interpreting and Postprocessing Results

I have implemented a framework for postprocessing results from benchmark runs.  
This framework includes a benchmark output parser and a standard object 
representation for benchmark output.  Relatively simple client programs can 
then be written that read benchmark output from an input stream (typically a 
file), analyze the results, and print result summaries.

I have written four sample programs using the postprocessing framework.

The first, PromptFinalizeProcessor, reads the results from the PromptFinalize 
benchmark and produces a plot of age versus time-to-finalization.

The second, TextSearchProcessor, reads the results from the TextSearch 
benchmark and produces a plot of heap size and time per unit of work (where 
time beyond the minimum is generally indicative of garbage collection overhead 
and pause times) versus units of work.  

The third, AllBenchmarks, reads a file that represents the output of all (or 
some) benchmarks and prints summaries of the results for each.

The fourth, CompareRuns, reads two files that represent the outputs of all (or 
some) benchmarks and prints side-by-side comparisons for those benchmarks 
present in both files.

To analyze the results of a run, one would typically run either CompareRuns or 
AllBenchmarks or both to view the condensed results, and if any of the results 
look interesting, the specialized plotting programs can be run, or the results 
can be inspected directly.


5  FUTURE WORK

Before the benchmark suite is ready for an external audience, several issues 
need to be addressed.  In this section I also suggest areas for further 
investigation.

5.1 Java 1.2

No benchmarks for Java 1.2 memory management functionality have been designed 
or implemented.  Although most Java systems are currently at 1.1, I expect most 
will upgrade to 1.2 within a year of release of 1.2 (currently in Beta 4).

5.2 Testing and Refinement

The benchmark suite must be tested on more Java implementations, and the 
results from these tests should result in further refinement of the benchmark 
programs.

5.3 Real Programs

Ideally, the benchmark suite should include several more larger programs 
(kernel benchmarks or real programs).

5.4 Reference Platform / Calibration

A reference platform should be chosen for the benchmark suite.  This would 
serve two purposes.  First, it would allow us, should we want to, to present 
benchmark results as a single number relative to the reference platform.  
Second, the benchmarks' difficulty can be calibrated to the reference 
platform.  Calibration would ensure that a benchmark is "difficult enough" on 
the reference platform.  Future machine generations can be accomodated by 
recalibrating to newer reference platforms.

5.5 Snazzy Interface

Displaying results graphically might make them easier to interpret.

5.6 Further Investigations

Future work on the benchmark suite could go in the following directions:

* The non-portable measurement interface and its use should be explored, and 
the benchmark programs should be adapted to use non-portable code when 
available.  To the extent possible, use of the non-portable code should be 
hidden inside the benchmark framework.

* Some benchmarks, notably the Boyer benchmarks, should produce more and better 
output.  More output would allow better plots of pause times, GC times, heap 
sizes, and so on.  TextSearch has good output already and could serve as a 
model (although its output is voluminous).  Perhaps it would be reasonable to 
worry less about human-readable output and output volume and just expect 
postprocessors to be used.

* It would be good to investigate if Java's debugger interface (new in 1.2, 
although non-portable interfaces are said to exist in 1.1) could be used in the 
measurement process.


6  INDIVIDUAL CRITIQUES AND COMMENTARY

[ @@@ Needs work ]

6.1  HeapSize Benchmark

It doesn't handle a GC architecture where memory is returned to the OS "after a 
while", ie if a program stabilizes at low memory consumption, and the GC then 
decides to get rid of its overstock.

6.2  PromptFinalize Benchmark

If the load of finalizable objects in this program (100K objects) is 
unnaturally high, then the finalization times computed may not be 
representative of real programs.  [Load=absolute number _or_ ratio; certainly I 
would expect the ratio to be unusually high.]

Random lifetimes of objects may perturb normal collection/finalization 
mechanism; for example a generation-based finalization mechanism would be under 
unusual stress.

6.3  TextSearch Benchmark

The TextSearch benchmark is not runnable as an applet, which voilates 
.req.portable.  This weakness can be corrected by modifying the program to 
allow it to read its input from an HTTP connection, for example.


7  RELATED WORK

7.1 SPEC jvm98 Benchmark Suite

SPEC (http://www.spec.org/osg/jvm98) has recently (1998-08-14) released a Java 
benchmark suite.  A license is $100; source code for most benchmarks are 
included (some of the programs are proprietary).  All benchmarks are run as 
applets, although a framework not unlike mine is present, and should allow the 
programs to be run standalone.

From a brief perusal, it appears that the SPEC benchmark suite includes some 
allocation and GC intensive programs, but also that the programs are not 
scaleable and that all run in 24MB or less, some in as little as 2MB.  In my 
opinion Harlequin's benchmark suite fills a different niche, and the existence 
of the SPEC suite is in itself not an argument for not continuing development 
on our own product.

7.2 Caffeinemark 3.0

The benchmark suite (http://www.webfayre.com/cm) is a popular benchmark suite 
for Java but does not include any GC benchmarks.  The benchmark is free; source 
is not available.  All benchmarks are run as applets.

