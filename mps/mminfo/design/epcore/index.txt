                    DESIGN OF THE MM/EP-CORE PROJECT
                             design.epcore
                           incomplete design
                            nickb 1995-12-05


INTRODUCTION:

This is the design of the EP-core memory management project (see 
project.epcore). It has been constructed by analysis of the requirements for 
that project (req.epcore).


OVERVIEW:

The EP-core memory manager will use the MPS to provide memory management 
through a simple and flexible interface, using mark/compact GC to collect 
PostScript "virtual memory".

.def: Some definitions:

.def.old: "The old memory manager" is those parts of the core RIP
which did memory management before the MM/EP-core project was begun.
This is what the MM/EP-core project replaces.

.def.new: A "new memory manager" is a replacement for the old memory
manager supplied by the MM group. There may be several of these.

.def.equiv: An "equivalent memory manager" is a new memory manager
which meets those requirements met by the old memory manager (e.g.
req.epcore.attr.run-time.seybold, but not req.epcore.fun.ps.gc).

.def.glue: "Glue" is code between the core RIP and the MM group's
memory pool system.


REQUIREMENTS:

.req: The requirements document req.epcore describes the requirements of the 
EP-core memory manager.

.req.evolve: The requirements have evolved, and continue to evolve, as a result 
of further analysis and the design process.


ARCHITECTURE:

.arch.overview: Glue code uses the MPS to support an MMI which starts out very 
simply. Initially, MV pools are used for everything. They are extended to be 
smarter about segment placement to reduce fragmentation. Later, we have special 
PS pools which support save/restore as well as mark/compact GC. Then we start 
dissolving the glue code to bring the MMI and the MPS interface together.


.arch.diagram: A diagram of the planned releases (see plan.epcore).
 - EPcore-releases 


ANALYSIS:

.sol.mmi.an: .sol.mmi is so valuable to the project it is hard to envisage a 
working solution without it. Without a common interface, the RIP is a moving 
target, with changes in the memory management protocol taking place at the whim 
of EP developers/debuggers.

.sol.mps.an:  Not using the MPS would be more costly 
(req.epcore.plan.release.*, req.epcore.plan.port.*, req.epcore.plan.supp.*), 
and would not provide as much utility to the memory management project in 
improving other and future memory management products (goal.epcore.improve).

.sol.pools.an: To start with, one MV pool for each display list and one pool 
for temporary memory (see .sol.pools.old) is sufficient to meet the 
requirements. Adding more pool flexibility later may be a very good idea.

.sol.pools.an.post: PostScript VM can start off using big blocks of "temporary" 
memory (as in the old memory manager). Later, we'll need PS pools in the MPS 
(so PS VM can be GCed).

.sol.contig.an: The top-down/bottom-up design of the old memory manager would 
be adequate to solve this. We could start with that, plus using allocation 
points to segregate small objects, and then add improvements as necesssary.

.sol.contig.an.place: This will require changes to the MPS to allow segment 
placement preferences to be expressed (at the moment, segment placement uses no 
preference information).

.sol.gc.an: Someone needs to read up on mark/compact GCs. It seems that a 
two-phase tracing system will work (one phase to identify garbage, then 
determine new locations, then a second phase to relocate everything and fix 
pointers)


IDEAS:

.sol.mmi: Provide a common memory management interface (MMI) within the RIP 
which can be supported either by the old memory manager or by a new memory 
manager. req.epcore.plan.deliv.cutover has, as its most natural solution, the 
provision of a common MMI.

.sol.mmi.release: Make a release using the MMI and the old memory manager.
Later, make a release using the MMI and a new memory manager which is 
equivalent (see .def.equiv). These two releases form the basis for cutover.

.sol.mmi.evolve: After the equivalent release, the MMI can evolve to support 
more functionality than the old memory manager provides.

.sol.mps: Use the MPS framework.

.sol.mps.glue: If using an MMI and the MPS, there will need to be some glue 
code which uses the MPS interface and implements the MMI (as the old memory 
manager cannot support the MPS interface, so we cannot use that as an MMI).

.sol.mps.glue.reduce: After an equivalent release, the MMI can evolve towards 
the MPS interface, reducing the amount of glue.

.sol.pools: There are several natural uses for pools suggested by the 
requirements. A pool class for each class of managed memory (display lists, 
postscript objects, caches, general, static, and temporary memory) is the 
obvious way to go.

.sol.pools.old: The old memory manager can be seen as using two or three pools, 
depending on configuration (although they are termed "banks"): one for each 
display list and one for general memory. The MMI could start with just these 
pools.

.sol.contig: The current system attempts to retain a single large free area, so 
that large contiguous objects (images) can be allocated. (It does this by 
allocating display-list objects from the bottom up and all other objects from 
the top down.) Any solution needs to satisfy similar allocation requests to 
have a chance of meeting req.epcore.attr.run-time.*.

.sol.contig.reloc: One way to satisfy large allocation requests is to relocate 
mobile objects &c in order to defragment free areas.

.sol.contig.large: Allocating large objects separately would avoid problems 
with sequences such as: alloc large1, alloc small1, alloc large 2, alloc 
small2, free large1, free large2, alloc large3.

.sol.contig.future: Possible future systems which compress and fragment these 
large objects in store rather than writing them to disk should be discussed 
with EP developers (e.g. Andy Cave).

.sol.gc.slow: There are no requirements on the speed of the GC, as it will only 
be invoked when running jobs which the old memory manager can't handle. So it 
can be slow (as long as it can tickle with the required frequency, see 
req.epcore.attr.tickle).

.sol.gc.compact: Use a mark/compact algorithm for GC of PS
VM. This makes save/restore/check efficient (req.epcore.fun.ps.save).

.sol.gc.compact.trace: The MPS in general, and the MPS tracing mechanism in 
particular, will need some tweaking to support mark/compact. This is a good 
thing, as it increases the generality of the MPS (goal.epcore.improve). 

.sol.deliv: Delivery as binary, plus source code for glue.

.sol.deliv.evol: Evolutionary delivery is a good idea (see book.gilb88).



IMPLEMENTATION:

.imp.plan: See plan.epcore.release for details of the planned releases.

.imp.blah: All other implementation design goes here


ATTACHMENT:

   "EPcore-releases"


