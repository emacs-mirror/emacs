              THE DESIGN OF THE TEXTSEARCH JAVA BENCHMARK
                    design.java-benchmark.textsearch
                           incomplete design
                             lth 1998-07-21


INTRODUCTION:

This is the design for the TextSearch Java benchmark, which is a server-like 
macro benchmark.

.readership: MM group members.

.source: analysis.java-benchmark(1) , design.java-benchmark.arch(3) .


OVERVIEW:

The TextSearch Java benchmark is a server-like application with a large live 
data set, and a moderate allocation rate.  The program runs in two phases.  
.overview.phase1: In the first phase, it builds a large text data base in 
memory.  The representation of the data base is optimized for 
conjunctive/disjunctive word searches.   .overview.phase2: In the second phase, 
the program performs searches in the data base and occasionally adds new data.  
.overview.measurements: The benchmark measures space usage and running time.


REQUIREMENTS:

Critical Requirements

.req.realism: The program should be similar to a realistic non-benchmark server 
application.  A server application type is chosen because Java can be expected 
to be used in a server setting.

.req.large: The program should have a large live data set that changes only 
slowly, because real programs often have large data sets, and because data are 
read more often than written.

.req.allocation: The program should have a moderate allocation rate, because 
real applications are tuned and do not allocate gratuitously.

.req.optimized: The program should have data structures optimized for speed, 
but the data structures should also be tuned for space, as real applications 
are tuned for space and do not use space gratuitously.

.req.challenge: The program must make memory management nontrivial by 
performing significant allocation and by allocating objects with nonobvious 
lifetimes.

.req.data-available: The data that was presented to the server as input should 
be available in the database, with the original formatting (ie, it's not 
sufficient to store the search keys or "lossily" compressed data).

.req.scaleable: The program should be scaleable, cf 
analysis.java-benchmark.req.scaleable.  In practice, facilities must exist 
whereby the data set and computational activity can both be scaled up.


Essential Requirements

.req.input.real: The input should be data that was constructed to mean 
something, not random gibberish.

.req.simple: The benchmark should be simple, so that its workings can be 
understood by interested parties.


ANALYSIS:

.anal.data:  I choose to use lines of text as the record type, with the words 
of the line as the keys.  Large amounts of text are easy to come by, and very 
little work is needed in constructing the keys.  HTML would have to be parsed, 
which is more work than a benchmark should really do.  .anal.data.source: The 
full text of the Bible is an attractive source: 31000 lines of text for a total 
of 4.5MB.  .anal.data.query: Each query expression is either the single word 
ADDRECORD or a sequence of words, all on one line, denoting a conjunctive query 
on those words.  There should be five ADDRECORDs for every 100 query 
expressions.  .anal.query.source: A moderate query set (100 queries) can be 
constructed by hand from common words in the input data set.  
.anal.data.analysis: The benchmark program can be extended to print a table of 
word frequencies and the number of hits for search terms, thereby providing 
guidance for construction of the query set.

.anal.data-record-adt: Each text line is represented as an ADT where the text 
is exposed as a String in a data field. The words in the line are cached in the 
record.  Those choices simplify the program, and increase the memory 
consumption somewhat; the increased memory consumption may make the program 
more difficult for the memory manager.

.anal.build: A preliminary investigation indicates that line-by-line I/O from a 
file is tolerably fast and does not significantly pollute the measurements.  
Consequently, it does not seem necessary to investigate eg a serialized 
representation.

.anal.lookup: A hash table keyed on word is used, and each word contains a data 
structure that records all the records that contain the word.  Using a hash 
table is convenient because Java already has them.  The data structure that 
holds all the records could be a linked list or a java.lang.Vector (a stretchy 
data structure); I use a Vector because it uses less space.  A Vector is 
reasonable because once the database has been built, it will only need to grow 
rarely.

.anal.scale:  Adding each line twice would violate the spirit, if not the 
letter, of .req.input.real, but has the advantage that it's easy.  A more 
subtle problem is that locality may be somewhat artificial: if the line is 
added twice when it's read, then occurences are paired.  If the input is read 
twice, then two subsets of occurences are equally spaced.  However, for the 
time being I choose to add each line as many times as needed for scaling 
purposes.  A scaling factor of k will add each line to the data base k-1 times, 
for k >= 2.  For k=1, each other line will be added, and for k=0, each fourth 
line.

.anal.addition: When a query expression is ADDRECORD, then a line from the data 
base will be extracted and added anew.  This is not entirely satisfactory (for 
the same reasons outlined in .anal.scale), but it is simple.  The line must be 
picked somewhat randomly to minimize the risk of repeatedly inserting the same 
lines.  Since Vectors are used to store the list of records for each word, we 
can pick a pseudo-random element from the list of records associated with the 
word "and", which occurs on nearly every line.  Note that the Java 
random-number generator is deterministic, and starting it with the same seed on 
every run avoids nondeterminism in the benchmark.

.anal.experiment: Some experiments that can be run are: 
.anal.experiment.noupdate: Process the query set, but ignore ADDRECORD; observe 
whether the memory consumption is stable.
.anal.experiment.simple: Process each query in order, and construct a reply (a 
string containing all the strings in the result set; for ADDRECORD this set is 
empty), then let the result set and the reply die.
.anal.experiment.disjunctive: Process each query in order, and keep the result 
set, then perform an OR on the result set.  Finally, build up a reply of the 
texts of the first 10 records in the set.  Then let the result sets and the 
reply die.
.anal.measurements: Measure the time it takes to process each query set and the 
heap size at the end of each query set.

.anal.working-set: Since real working sets tend to have good locality, a set of 
query terms with reasonable locality should be chosen.  For example, a query 
set that touches 10% of the database might be reasonable.

.anal.workload: The following calculations are based on the full Bible and a 
hand-built query set of 100 terms with no ADDRECORDs.  Preliminary analysis 
shows that processing 100 queries once (as for .anal.experiment.simple) causes 
about 140KB of allocation.  Then running 1000 query sets causes 140MB of 
allocation, which should be sufficient to stress the garbage collector.  
Furthermore, some of this allocation will be long-lived data, as the following 
calculation shows. 5% of the queries are ADDRECORDs, so 1000 iterations of 100 
queries results in 5,000 ADDRECORDs; that's 1/6 of the initial database of 
31000 records.  The initial data base occupies 20MB, and 1/6 of that is over 
3MB.  That is, on the face of it over 3MB will be added to the data base.  But 
that's probably an overestimate, because all words will be in the database 
already, and also because the occurence vectors have 25% slack on the average 
(hence, a 1/6 increase (17%) will not overflow all vectors).  In actuality, we 
can expect something like 1MB to be added (145 characters per line on the 
average, plus the data in the TextItem data structure, which includes an array 
of distinct words, plus object overhead, times 5000).




IDEAS:

.sol: Since this is a benchmark program, not an actual end-user program, the 
solution chosen does not need to be the best nor the most realistic, merely 
adequate on both counts.

.sol.data: Various types of data can be stored in the data base and can be 
searched in various ways.  .sol.data.text:  Lines of text searchable on full 
words is an easy solution.  Project Gutenberg (http://www.promo.net/pg/) 
maintains a large database of ASCII-format public-domain texts.  Large texts 
include the Bible (canonical and apocryphal books), the Don Quixote, and the 
works of Charles Dickens.  .sol.data.html: Another possibility is to store web 
pages and search on content words.  Web pages are readily available on the web 
and can be collected into an archive for the benchmark to use.

.sol.data-repr: Each data record can be represented as an ADT.  
.sol.data-repr.exposed: The data record ADT can expose the data (as a string, 
say).  .sol.data-repr.hidden:  The data record ADT can hide the data, allowing 
it to be stored on disk or compressed, thereby reducing memory requirements.

.sol.keys: Keys must be extracted from each data record.  .sol.keys.as-needed: 
Parsing a record can be done when the words are needed.  .sol.keys.cache: The 
keys can be cached in the data record ADT.  

.sol.lookup: The records that contain a key must be found quickly.  
.sol.lookup.tree: A B-tree can be used as the search structure.  
.sol.lookup.hash: A hash table can be used as the search structure, because we 
can restrict searches to exact matches.

.sol.structure: The program can be split into two phases, _build_ and _query_.  
.sol.structure.build: In the build phase, data is read from an external medium 
and the data base is built.  .sol.structure.query: In the query phase, queries 
and updates are run against the data base in an on-line fashion.

.sol.build: The database must be built by reading the data from some external 
medium, because the program code cannot be expected to contain the input.  
.sol.build.io: It is straightforward enough to allow the benchmark to read the 
input from a file of from a URL.  .sol.build.overhead: It is important that the 
I/O overhead does not swamp other system activity, because MM-activity in the 
building phase should also be measured.  .sol.build.serialization: A serialized 
database can be used, but only for Java versions 1.1 and later.

.sol.query: The query phase must run on-line, because it's unreasonable for a 
server to batch more than a few requests.  .sol.query.fixed: The query 
expressions can be built into the program.  .sol.query.external: The query 
expressions can be read from an external source, in textual or non-textual 
form.  .sol.query.patterns: The query expressions can be read from an external 
source but the pattern in which they are used (repetitions and so on) is built 
into the benchmark program.

.sol.additions: Some data should be added to the database during the query 
phase to "stir the pot".  .sol.additions.external: These additions can be read 
from an additional input source.  .sol.additions.internal: The additions can 
also be taken from the database itself: a (pseudo-random) line can be 
duplicated and added to the database anew.

.sol.scale: The benchmark can be scaled by adding more data and increasing the 
query set.  .sol.scale.synthetic: It would be possible to double the size of 
the database by adding each data record to the database twice; this would also 
double the result set of each search.  .sol.scale.real: The program could just 
read more data into the database, and add more terms to the query set.

.sol.volume: Even if the allocation volume per request is low, running the 
queries for many iterations result in a high volume overall and forces garbage 
collections.  




IMPLEMENTATION:

TextSearchBenchmark

The class TextSearchBenchmark represents the entire benchmark; there is only 
one instance of the class, created by the driver program.

TextDatabase

.text-database: The class TextDatabase that represents one entire data base.  
.text-database.attrib: The text data base has two attributes: a hash table of 
words known to the data base, and a record serial number.  The serial number is 
incremented by one for each new TextItem added.

.text-database.create-word: TextDatabase.createWord takes a word (a string) and 
returns a TextWord structure.  A new TextWord is created if the word is 
unknown; otherwise the TextWord to which the string maps in the hash table is 
returned.

.text-database.add-text: TextDatabase.addText takes a string representing a 
line of text, and adds a new record that represents that line to the database.

.text-database.lookup: TextDatabase.lookup takes a string representing a 
nonempty set of words, and returns a Vector containing all records, in serial 
number order, that contain all the words.

.text-database.and-lists: TextDatabase.and_lists takes an vector of Vectors of 
TextItems, each Vector sorted in serial-number order, and returns a Vector of 
TextItems (sorted on serial number) that occur in all Vectors in the vector.

.text-database.or-lists: TextDatabase.or_lists takes an vector of Vectors of 
TextItems, each Vector sorted in serial-number order, and returns a Vector of 
TextItems (sorted on serial number) that occur in any Vector in the vector; ie, 
it merges the Vectors.  It uses a priority queue to implement the n-way merge 
efficiently.

.text-database.print-frequency-table: TextDatabase.printFrequencyTable takes a 
filename and prints a table of words and their frequencies in the database to 
the named file, with each word/count pair per line, separated by blanks.

TextItem

.text-item: The TextItem class is the ADT for one line of text.  
.text-item.attrib: It has three attributes: the text line, the set of unique 
words in the line, and a serial number.

TextWord

.text-word: The TextWord class is the ADT for a word.  Words are unique in a 
database.  .text-word.attrib: The TextWord has three attributes: the string 
that is the word's printable representation, a Vector of the TextItems in the 
same database as the TextWord that contain the word, and a count of the number 
of TextItems (redundant because the Vector's size field always has the same 
value).  .text-word.java-bogosity: The Vector should really be a 
Vector<TextItem>, but Java doesn't yet have generic types.

.text-word.add-text-item: The method TextWord.addTextItem takes a TextItem and 
adds it to the word's set of records.  The record must have a higher serial 
number than all other records previously added to the word.

TextItemListContainer

.text-item-list-container: The TextItemListContainer class is a wrapper for 
Vector<TextItem>s sorted on serial number, and used during list merging 
(.text-database.or-lists).  .text-item-list-container.attrib: The class has one 
attribute, the TextItem record stored in the current element of the Vector (or 
null if the end has been reached).

.text-item-list-container.advance: Advance the current element to the next 
element in the Vector.

TextItemListContainerComparator

.text-item-list-container-comparator: The TextItemListContainerComparator class 
is an artifact of Java lacking first-class methods: it encapsulates a single 
method _compare_ that takes two TextItemListContainers that have not reached 
the end of their vectors and returns a code that reflects the relationships 
between the serial numbers of the current elements of the containers.

TextUtil

.text-util: The class TextUtil encapsulates some common string-manipulating 
functions.

.text-util.split-string: TextUtil.splitString takes a String and returns a 
vector of Strings: the words in the input. A word is any sequence of alphabetic 
characters and apostrophes that begin with an alphabetic character.  The words 
in the output have all been folded to lower case.

.text-util.uniq: TextUtil.uniq takes an array of Strings (words) and returns a 
sorted array of Strings without any duplicates.

TextStream

.text-stream: The class TextStream implements efficient line-by-line I/O.  It 
was implemented when I couldn't get Java's built-in I/O to perform well enough; 
no doubt I was being inexperienced in use of the Java class libraries.  The 
TextStream class should probably be discarded and standard facilities should be 
used instead.

.text-stream.readline: TextStream.readline reads a line of text and returns it; 
it returns null at end-of-file.

.text-stream.close: TextStream.close closes the stream.




TESTING:

The correctness of the search can be tested on a small data set, say some 100 
lines and a few keys.

