                THE DESIGN OF THE JAVA-BENCHMARK PRODUCT
                       design.java-benchmark.arch
                              draft design
                             lth 1998-06-25

INTRODUCTION

.intro: This document constitutes the high-level design for a suite of Java 
memory management benchmarks.  The design consists of descriptions of the 
planned benchmarks, which memory management aspects they will measure, and how 
they will measure those aspects. 
  
.structure: The requirements from analysis.java-benchmark are not directly 
addressed in the descriptions of the benchmarks themselves but in sections 
following those descriptions.

.readership: MM group members.

.source: analysis.java-benchmark(1) , plan.java-benchmark(1) , 
mail.ptw.1998-06-17.11-09(0).  .source.not:  An extensive search for Java MM 
benchmarks on the Internet has so far revealed that there are very few, and the 
ones I have found are either trivial or exercise too many parts of the JVM to 
be considered MM benchmarks proper.


DEFINITIONS

.def.age:  The age of an object is the number of allocations that have taken 
place since the object was allocated.

.def.ballast: Ballast is a data structure whose only purpose is to consume 
space and make things difficult for the garbage collector.  Ballast always has 
a certain volume (qv).

.def.byte: A byte is a measurement of space in the sense of the definition of 
java.lang.Runtime.totalMemory() (book.gjs96 section 20.16.7).

.def.jit-compiler: A JIT compiler (Just-In-Time compiler, sometimes abbreviated 
simply as JIT) is a compiler that is part of a Java interpreter and that 
compiles Java bytecodes to native code or other optimized format and then 
executes the optimized format, typically on demand or as the bytecodes are 
loaded.

.def.jls: The JLS is the book "The Java Language Specification" by Gosling, 
Joy, and Steele, 1st ed., 1996.  (book.gjs96)

.def.jni: Java Native Interface -- the standard Java foreign-language interface.

.def.jpl: The JPL is the book  "The Java Programming Language" by Arnold and 
Gosling.  1st ed., 1996 (book.ag96)

.def.jvm: A JVM is a Java Virtual Machine.  A Java Virtual Machine is a program 
that reads a Java program compiled to JVM bytecodes and packaged as set of 
class files, and runs it.  However, the JVM does not have to be an interpreter: 
the JVM program can contain a JIT compiler that compiles the bytecodes before 
the program they represent is executed. 

.def.volume: Volume is a measure of the size of a data structure.  Volume is 
measured in the number of user-visible data items; for example, a structure 
with four data fields has volume 4.  Measuring the volume in data items means 
that two data structures with the same volume may occupy a different number of 
bytes, because the exact number of bytes depends on the representation of the 
object in which the data words are allocated.


JAVA

.java.versions: At the time of writing, there exist three versions of the Java 
language, these are known as 1.0, 1.1, and 1.2.  Changes to the Java language 
over time include new language features as well as new and evolved 
APIs/libraries.  The JVM bytecode specification is, however, fixed.

.java.compatibility:  Generally, a Java program compiled for a particular 
version of Java will run on a JVM and set of libraries designed for a later 
version. .java.deprecation:  However, Sun occasionally deprecates certain Java 
library APIs or parts of APIs as they release new versions of Java.  In 
practice, this means that a Java program compiled for a particular Java version 
may fail to run on a later Java system because some library calls used by the 
program were deprecated after the program was compiled and eventually removed 
altogether.  .java.disunity:  Additionally, some vendors have developed subset 
or otherwise incompatible Java implementations.  It may not always be possible 
to ignore these incompatibilities.  [For example, Microsoft and Netscape's Java 
implementations are said not to implement the JNI, but I've yet to confirm 
that.]   

.java-version.1.0:  Java version 1.0 is described by the JLS.  
.java-version.1.0.mm-features: The MM-related features in Java 1.0 are 
allocation, garbage collection, finalization, and object identity hashing (EQ 
hashing) (java.lang.Object.hashCode).  Java 1.0 does not have a standard 
foreign language interface, though several Java 1.0 implementations have such 
interfaces.  .java-version.1.0.finalization: Any object may register itself for 
finalization by implementing a finalize() method.  The JLS does not guarantee 
that a finalizer will ever be run, although it guarantees that the finalizer 
will be run before the object is reclaimed.  The method 
java.lang.System.runFinalization() suggests to the run-time system that it 
should try to run finalizers for finalizable objects.  By default finalizers 
are not run when the process terminates; however, the program can ask to have 
all finalizers run on exit (java.lang.System.runFinalizersOnExit()).   
.java-version.1.0.eq-hashing: The JLS specification of Object.hashCode() is 
weak enough to allow an implementation to always return the same value 
independent of its argument, although the specification encourages 
implementations not to do this.  Furthermore, the JLS specification of 
java.util.Hashtable does not appear to require the hash tables to use the 
hashCode method of the key.  However, all JVMs I've looked at so far do 
implement object identity hashing, and the hash tables do use the hashCode 
method. 

.java-version.1.1:  Java version 1.1 is described by the JLS plus addenda.  The 
addenda are available from Sun and as Appendix D of the fourth printing of the 
JPL.  .java-version.1.1.mm-features: The MM-related features added to Java 1.1 
are: more explicit support for object identity hashing 
(java.lang.System.identityHashcode), support for object serialization 
(java.lang.io.Serializable, java.lang.io.ObjectInputStream, 
java.lang.io.ObjectOutputStream), and an entirely new foreign language 
interface (JNI). .java-version.1.1.eq-hashing: Object hashing was implicitly 
available in Java 1.0 (.java-version.1.0.eq-hashing) and the introduction of 
System.identityHashcode() should not require any changes.   
.java-version.1.1.jni: The design of JNI allows the JNI implementation to 
isolate the foreign code from Java storage by means of user-level reference 
counting and object copying, so an MM  that does not implement pinning or 
ambiguous roots collection can be used.

.java-version.1.2:  Java version 1.2, which is still in Beta, is described by 
the JLS plus addenda; the addenda are available from Sun.  
.java-version.1.2.mm-features:  This release adds reference types, reference 
queues (java.lang.ref.ReferenceQueue), a memory advice facility 
(java.lang.Runtime.MemoryAdvice), weak references in the JNI, and a debugger 
interface.   .java-version.1.2.references: There are three types of references 
(java.lang.ref.*): weak references, soft references (will be cleared if the 
system is low on memory), and phantom references (placeholders that allow 
cleanup actions to be performed on the referent, but prevent the referent from 
being made reachable again).  .java-version.1.2.reference-queues: Reference 
queues are available to the program to manage references: a reference can be 
registered with a queue, and when the reachability of the referent changes, 
then the reference is put on the reference queue with which it is registered.  
Reference queues can be used with the references to implement guardians.   
.java-version.1.2.memory-advice:  The MemoryAdvice facility provides simple 
hints to the Java program about how its use of storage affects performance.  
.java-version.1.2.jni: The _weak global references_ provides a simple 
weak-pointer facility to C code that references Java objects across foreign 
function invocations.  .java-version.1.2.debugger:  The debugger interface in 
Java 1.2 provides C code with a way of controlling the execution of the JVM.  
The C code can register memory allocation and deallocation functions with 
appropriate hooks in the debugger interface, so the MM may have to be able to 
interoperate with a foreign allocator, or at least be prepared to provide a 
separate pool for use by the debugger interface.  [This may be more a quality 
issue than a functionality issue, I'm not sure.]


PRACTICAL ISSUES

.versioning: There will be only one benchmark suite, but it will be adaptable 
to different Java versions.   .versioning.releases:  A benchmark suite release 
consists of several sets of programs, one set for each supported Java version.  
.versioning.version-number:  Each set of programs will be assigned a version 
number consisting of the release number and a Java version number.  The release 
number is the same for all sets in the release.  The Java version number 
denotes the Java version for which the set of programs was written and 
compiled.  .versioning.maintenance:  The benchmark suite must be maintained to 
ensure that the benchmarks remain runnable on new Java versions; maintenance 
should also introduce new benchmarks to test new features of Java, as 
appropriate.

.measures:  The benchmarks are used to characterize an implementation, and also 
to compare implementations.  Three measures are used: time, space, and 
functionality.  .measure.time: The elapsed time, measured in milliseconds, that 
a benchmark program uses to execute from one program point to another.  
.measure.space:  The amount of memory, measured in bytes, that a particular 
Java implementation uses to accomplish a particular task.  .measure.feature: A 
boolean value that indicates whether a particular type of functionality is 
present or absent.  A boolean value is reported as "yes" or "no" in the output.

.units:  The benchmark results must be reported to the user in a consistent set 
of units.  .units.natural: The results should be reported in the quantities 
mesured: time (ms), space (bytes), and features (booleans).  .units.absent:  It 
is undesirable to reduce the benchmark results to unitless 'marks'.  The 
benchmarks allow one to compare the results from one implementation to those of 
another on a given test platform under controlled conditions, and for this 
purpose, undisguised results seem most useful.  .units.ratios:  For some 
purposes it is useful to define a reference platform and report individual 
results relative to the results for the reference platform.  I consider this a 
packaging issue.

.output: The benchmarks report their results in a machine-readable form.  
.output.method:  The section .environment.output, below, describes the exact 
method of reporting for each environment.  .output.format:  The output format 
consists of lines of text.  Each line consists of a tag and data.  Each tag is 
preceded by a period. Lines that do not start with a period are program output 
and are to be ignored.  .output.tags: The tags, their values, and their meaning 
are:

.tag.benchmark.version: "benchmark.version" has a string value; it is the 
version number of the benchmark suite.
.tag.benchmark.revision: "benchmark.revision" has a string value; it is the 
revision number of the program being run.
.tag.benchmark.tag: "benchmark.tag" has a string value; it is the tag the 
benchmark uses for reporting results.
.tag.system.vendor: "system.vendor" has a string value; it is the value of the 
java.vendor property.
.tag.system.version: "system.version" has a string  value; it is the value of 
the java.version property (Java version implemented).
.tag.system.class-version: "system.class-version" has a string value; it is the 
value of the java.class.version property.
.tag.system.os: "system.os" has a string value; it is the value of the os.name 
property.
.tag.system.architecture: "system.architecture" has a string value; it is the 
value of the os.arch property.
.tag.system.os-version: "system.os-version" has a string value; it is the value 
of the os.version property.
.tag.inf: "inf" has a string value; it is an informational (intended for human 
consumption) message.
.tag.fail: "fail" has a string value; it is an indication of an internal 
failure. 
.tag.heap: "heap" has two numeric values, the current values returned by 
Runtime.totalMemory() and Runtime.freeMemory().
.tag.gc.before: "gc.before" has two numeric values, the values returned by 
Runtime.totalMemory() and Runtime.freeMemory() before a garbage collection.
.tag.gc.after: "gc.after" has two numeric values, the values returned by 
Runtime.totalMemory() and Runtime.freeMemory() following a garbage collection.
.tag.gc.done: "done" marks the end of the output from a benchmark.  It has one 
or two values.  The first value is the total running time of the program.  The 
second value, if present, is the peak heap size observed by the program during 
its execution.

output.tags.benchmark-specific: There are many other tags: the value of 
benchmark.tag is a prefix of each, and the suffixes depend on the particular 
benchmark.  These tags are described later, with the benchmarks.  
.output.tags.experiment: When a benchmark is split into experiments (explained 
later), the tag includes an experiment tag.  .output.tags.example: For example, 
if the benchmark has tag "bench.foo" and the benchmark wishes to report the 
running time 31415 for experiment "bar", it would report it as 
".bench.foo.bar.time 31415".  .output.tags.order: The benchmark.* tags are 
always printed before any other.    .output.tags.parameters: The benchmark 
parameters, if any, are printed with tags "bench.foo.parameter.<name>" where 
<name> is the name of the parameter.

.environment:  The environment of the benchmark is defined narrowly to be the 
configuration before benchmark start of the executable in which the benchmark 
runs.  .environment.external-doc: The environment types must be documented in 
an externally available document.  .environment.types: There are five 
environment types.  .environment.single-cl: If a benchmark is run from the 
command line in a fresh process, then it is said to have a "single-cl" 
environment.  .environment.multiple-cl: If several benchmarks are run from the 
command line but one after the other in a common process, then the benchmarks 
are said to have a "multiple-cl" environment.   .environment.single-applet: If 
a benchmark is run as an applet in the context of a browser or applet viewer 
that has just been started, then it is said to have a "single-applet" 
environment.  .environment.multiple-applet: If several benchmarks are run in 
the context of a browser or applet viewer but one after the other in a common 
browser process, then the benchmarks are said to have a "multiple-applet" 
environment.  .environment.other: Environments that don't fit any of the other 
categories are said to have an "other" environment.  .environment.quality:  
Since repeatability and controlled conditions are desirable, single-cl and 
single-applet environments are considered more reliable than multiple-cl and 
multiple-applet environments.  .environment.output: Benchmarks run from the 
command line print their output to the console; benchmarks run as applets print 
their output to a scrollable text window in the browser or applet viewer.

.regime: Some rules for how the benchmarks can be compiled and run are needed 
to ensure that the results can be interpreted in a consistent manner.  
.regime.external-doc:  These rules must also be presented in an externally 
available document, presumably distributed with the benchmark suite.  
.regime.byte-code: Benchmarks run under the "byte-code" regime may measure only 
an object program compiled with a particular compiler, compiler version, and 
set of compilation switches.  The values of these parameters must be published 
with the benchmark suite.  If the benchmark suite distribution includes 
compiled code, then that code must have been compiled in accordance with the 
stated compiler parameters.   .regime.source-code: Benchmarks run under the 
"source-code" regime may measure recompiled but unmodified source code.  This 
allows the user to compile the benchmarks using a nonportable compiler, to use 
additional optimization switches, and so on.  .regime.reporting:  When results 
from a benchmark run is reported (whomever it is reported to), the following 
information should be reported: the environment type, the regime (byte-code or 
source-code), and the output, optionally stripped of program output and 
informational messages.

.iterations: Many benchmarks are run multiple iterations.  Unless otherwise 
stated, all runs of a particular experiment are run consecutively, before any 
runs of the next experiment.

CALIBRATION

...

MEASURING

.measuring.intro: Java versions through version 1.2 provide very limited 
facilities for measuring system performance.  The available facilities allow 
the program to read the current clock to a resolution of one millisecond, and 
read the total heap size and the amount of free heap.  .measuring.inadequacy: 
The quantities are all approximations; in particular, the clock resolution 
depends on the resolution of the actual system clock.  
.measuring.inadequacy.consequences: Because of the inadequate performance 
measuring tools, some creativity is required to perform the measurements 
required by the benchmarks, and some measurements are not practical or 
possible.  This section outlines some possibilities.

.measuring.time: Elapsed time can be measured using System.currentTimeMillis(), 
although as noted above, the actual resolution depends on the system clock 
resolution.  Because good resolution is not guaranteed, the benchmarks should 
not attempt to measure single instances of brief operations.  Additionally, 
preemptive threads, OS activity, paging, and similar activities external to the 
benchmark program will affect time measurements.

.measuring.space: Space measurements can be made based on System.totalMemory(), 
which returns the total heap size in bytes at that moment, and 
System.freeMemory(), which returns the number of bytes available for allocation 
at that moment.  .measuring.space.live:  The space in use by live data can be 
approximated by forcing a garbage collection and subtracting free memory from 
total memory; however, this depends on being able to force a (full) garbage 
collection.  (In the following, I will assume that it is possible to force a 
garbage collection.)  .measuring.space.complications: Inaccurate memory 
reporting complicates space measurements.

.measuring.gc-time: Time spent in the garbage collector cannot be measured 
accurately in general with the facilities provided by portable Java.

.measuring.pause-time: Pause time can be estimated by observing the time it 
takes to do a constant amount of work over a period of time; if the work can be 
done in a short time and the time it takes to do the work is measured enough 
times, then the difference between the longest and the shortest times is 
overhead that is beyond the program itself, i.e., what the user sees as a pause.

.measuring.non-portable:  It is possible to use the Java Native Interface or 
other foreign language interface to obtain better measurements by calling 
low-level timing routines (performance monitoring routines in the OS). 
.measuring.non-portable.benefits: It may be particularly useful to measure 
memory consumption this way because it may give a truer view of memory 
consumption than that obtained by measurements of heap size.  
.measuring.non-portable.problems: This method will not work in an applet 
environment.  Also, it is possible that use of JNI may affect the garbage 
collector and skew the measurements.

.measuring.noise: Bytecode interpreters inject significant noise into the 
measurements because interpretation is relatively slow; consequently, effects 
may be more muted than if the user program ran at full speed.


BENCHMARKS

.benchmarks.intro: The benchmarks fall into several disctinct groups.  These 
groups are: micro-benchmarks that determine features, micro-benchmarks for 
basic performance and space measurements, micro-benchmarks for reliability, and 
macro-benchmarks.  Only programs in the latter group are at all similar to real 
programs; all other benchmarks test specific features of the JVM or MM.  
.benchmarks.design: Several benchmarks have their own design documents; these 
documents are cross-referenced when appropriate.  .benchmarks.experiments: Many 
benchmarks are broken down into experiments, where one experiment measures a 
very specific aspect of the implementation, the benchmark being a convenient 
way of grouping related experiments.  .benchmarks.parameters: Most benchmarks 
accept a common scaling parameter, the default value of which is 2; the value 
adjusts the workload of the benchmark. Most benchmarks that accept the scaling 
parameter also accept one or more benchmark-specific parameters that adjust 
parts of the benchmarks' workload.   By adjusting the scaling parameter, the 
benchmark-specific parameters (if any) are adjusted automatically.


MICRO-BENCHMARKS THAT DETERMINE FEATURES

.benchmarks.micro-features.intro: The micro-benchmarks in this section explore 
the capabilities of the JVM and its MM.  Some of the capabilities are not 
directly MM-related but are potentially useful in interpreting the results of 
other benchmarks.


HeapSize Benchmark

.bench.heap-size: The HeapSize benchmark determines whether the heap size grows 
and shrinks with the needs of the application.  .bench.heap-size.desc: The 
benchmark allocates increasingly larger amounts of live data, then lets the 
data die, and attempts to observe two phenomena:  (1) The value returned by 
System.totalMemory() becomes smaller, in which case we conclude that the heap 
shrinks.  (2) An OutOfMemoryError is signalled, in which case we conclude that 
the heap does not grow past a certain point.  .bench.heap-size.caveats:  The 
heap size is limited by the size of (virtual) memory, so running out of memory 
does not necessarily imply that the JVM limits the heap size artificially.  
Also, System.totalMemory() is under no obligation to return an accurate result 
or to reflect the Java process's memory footprint.  
.bench.heap-size.java-version: The benchmark runs on all Java versions.  
.bench.heap-size.params: The scaling parameter limits the probing for an 
out-of-memory error.   .bench.heap-size.report:  The benchmark reports whether 
the heap shrinks (tag: shrinks, value: boolean) and whether the heap grows as 
needed (tag: grows, value: boolean).


ExplicitGC Benchmark

.bench.explicit-gc: The ExplicitGC benchmark measures the effect of explicitly 
invoking the garbage collector (java.lang.System.gc()).  
.bench.explicit-gc.desc: Each experiment allocates some data, then asks for a 
garbage collection and attempts to measure whether the garbage collection did 
in fact take place.  .bench.explicit-gc.java-version: The benchmark runs on all 
Java versions.  .bench.explicit-gc.params: The scaling parameter controls the 
amount of allocation necessary to force data to look "old".  
.bench.explicit-gc.report: Each experiment  reports whether the request was 
honored (tag: gc-happened, value: boolean).  .bench.explicit-gc.experiments:  
There are two experiments:

.bench.explicit-gc.simple: The "simple" experiment (tag: simple) determines 
whether a garbage collection is performed when System.gc() is invoked.

.bench.explicit-gc.full:  The "full" experiment (tag: full) determines whether 
the collector can be made to collect "old" garbage by invoking System.gc() 
and/or sleeping (to let the GC thread, if any, run) for some iterations.  The 
experiment allocates a data structure, then allocates other data to make the 
data structure look old, finally letting the data structure die and invoking 
System.gc() until an equilibrium is reached or an iteration limit is reached.  
.bench.explicit-gc.full.caveats: Implementations that are conservative or not 
safe-for-space may retain the data structure anyway.


HasJIT Benchmark

.bench.has-jit: Measures whether the system has a JIT compiler.   
.bench.has-jit.desc:  The presence of a JIT compiler can be detected by 
examining differences in time between something that's known (or expected) to 
be implemented as a primitive (System.arraycopy(), say) and the equivalent 
operation implemented in Java.  By knowing the approximate ratios of primitive 
and non-primitive operation on both JIT and non-JIT implementations and by 
computing the ratio on a new implementation, it is possible to determine the 
likelihood of the new implementation having a JIT.  .bench.has-jit.java-version
: The benchmark runs on all Java versions.  .bench.has-jit.params:  The 
benchmark accepts no parameters.  .bench.has-jit.report: The benchmark reports 
whether the system is believed to have a JIT compiler (tag: has-jit, value: 
boolean).


IsPreemptive Benchmark

.bench.is-preemptive: The IsPreemptive benchmark measures whether a system has 
preemptive threads.  .bench.is-preemptive.desc: A long-running non-yielding 
computation is executed concurrently with a thread that updates a timestamp and 
sleeps; preemption of the computation is detected.  
.bench.is-preemptive.java-version: The benchmark runs on all Java versions.  
.bench.is-preemptive.params: The benchmark accepts no parameters.  
.bench.is-preemptive.report: Report whether the implementation is believed to 
have preemptive threads (tag: is-preemptive, value: boolean).  
.bench.is-preemptive.report: On a multi-CPU system with concurrent threads, 
this program may detect concurrency rather than preemption.


MICRO-BENCHMARKS FOR BASIC PERFORMANCE AND SPACE USAGE

.benchmarks.micro-performance.intro:  These benchmarks are synthetic programs 
that measure the cost of allocation and garbage collection under 
well-controlled conditions.  The programs perform no useful work but provide an 
indication of how expensive the basic memory management functionality in a 
system is.


FastAlloc Benchmark

.bench.fast-alloc: The FastAlloc benchmark measures the performance of 
collection and allocation in the presence of massive infant mortality, with and 
without ballast.  .bench.fast-alloc.desc: Each experiment allocates many 
short-lived objects, with and without ballast.  The ballast has the same volume 
in each experiment, to facilitate comparison.  Each experiment runs for some 
number of iterations.  .bench.fast-alloc.java-version: The benchmark runs on 
all Java versions.  .bench.fast-alloc.params: The benchmark has two parameters: 
a scaling factor for the ballast (default 0), and a scaling factor for the 
allocation volume (default 0).   .bench.fast-alloc.report: For each experiment, 
the benchmark reports the time for all iterations (tag: times) as a 
blank-separated list of times and the observed peak heap usage (tag: 
peak-heap).  .bench.fast-alloc.experiments:  There are six experiments:

- No ballast (tag: no-ballast)
- A ballast of small non-leaf objects (tag: small-nonleaf-obj)
- A ballast of medium-size non-leaf objects (tag: med-nonleaf-obj)
- A ballast of medium-size leaf and non-leaf objects (tag: med-mixed-obj)
- A ballast of large non-leaf objects (tag: large-nonleaf-obj)
- A ballast of large leaf objects (tag: large-leaf-obj)


HashCode Benchmark

.bench.hash-code: The HashCode benchmark measures the performance of 
Object.hashCode().  .bench.hash-code.caveats: Object.hashCode() is not required 
to do anything useful; see .java-version.1.0.eq-hashing.  .bench.hash-code.desc
:  Each experiment measures the time it takes to perform some number of calls 
to Object.hashCode().  The times from the experiments are not intended to be 
compared with each other.  Each experiment is run several iterations.  
.bench.hash-code.java-version: This benchmark runs on all Java versions.  
.bench.hash-code.coverage:  If we assume that the Java 1.1 
System.identityHash() method is the same as Object.hashCode(), which is what 
the spec implies, then this test also covers the former function.   
.bench.hash-code.params: The benchmark accepts three parameters: for the volume 
of live data, for the volume of allocation, and for the volume of hash code 
requests.  .bench.hash-code.report:  The benchmark reports a blank-separated 
list of iteration running times (tag: times) for each experiment.  
.bench.hash-code.experiments:  The benchmark consists of five experiments:

.bench.hash-code.reasonable: The "reasonable" experiment (tag: reasonable, 
value: boolean) determines whether Object.hashCode() has a reasonable 
implementation, that is, whether it returns values of reasonable distribution.

.bench.hash-code.noalloc: The "noalloc" experient (tag: noalloc) allocates many 
small objects and then repeatedly requests their hash codes.

.bench.hash-code.alloc:  The "alloc" experiment (tag: alloc) allocates many 
small objects and then repeatedly requests their hash codes, meanwhile 
allocating many short-lived objects.  An implementation that performs object 
hashing by having an internal hash table may be forced to perform rehashing of 
the internal table.

.bench.hash-code.alloc-and-queue: The "alloc-and-queue" (tag: alloc-and-queue) 
experiment allocates many small objects and repeatedly requests their hash 
codes, meanwhile removing some hashed objects from the set of live objects and 
adding new objects to the set.  An implementation that has an internal hash 
table but avoids rehashing promoted objects may still be forced to rehash.

.bench.hash-code.new-always: The "new-always" experiment (tag: new-always) 
allocates many small objects and requests their hash codes once.  Then it 
repeats the following cycle: allocate many short-lived objects, then request 
the hash code of one new object.  An implementation that has an internal hash 
table risks having to do a full rehash on every lookup.  


Serialize Benchmark

.bench.serialize: The Serialize benchmark measures the space/time cost of 
object serialization and unserialization.  .bench.serialize.desc:  The 
benchmark allocates a graph data structure, and then repeats: serialize the 
data structure into a byte array, then unserialize the byte array into a new 
data structure.  Each subsequent iteration uses as input the output of the 
previous iteration.  The benchmark does this for three sizes of data structure. 
 .bench.serialize.java-version: The benchmark runs on Java version 1.1 and 
later. .bench.serialize.parameters: The scaling factor controls the size of the 
data structures and the number of iterations to run.  .bench.serialize.report: 
For each phase _n_, where a phase tests one particular data structure size, the 
benchmark reports: the number of vertices in the graph being tested (tag: 
data-size-n); the list of iteration execution times (tag: iter-time-n); the 
list of iteration-end heap sizes (tag: iter-heap-n), the overall running time 
(tag: time-n); and the peak observed heap size (tag: peak-heap-n).


ReferenceAlloc Benchmark

.bench.reference-alloc: The ReferenceAlloc benchmark measures the cost 
associated with using java.lang.Reference objects. .bench.reference-alloc.desc: 
The exact functionality for this benchmark has yet to be determined.  
.bench.reference-alloc.java-version:  This benchmark runs on Java 1.2 and later.


MICRO-BENCHMARKS FOR RELIABILITY

.benchmarks.micro-reliability.intro:  These micro-benchmarks attempt to measure 
the reliability with which the system collects garbage, runs finalizers, and 
enqueue references to objects that are no longer strongly reachable. 
.benchmarks.micro-reliability.doubts:  It is with some apprehension I include 
these programs in the set, for two reasons: first, reliability may be very hard 
to measure reliably, and second, someone is bound to interpret the results from 
these programs as a measure of quality, which they are not.  However, the 
information, if reliable, appears to be valuable.


Reliability Benchmark

.bench.reliability: The Reliability benchmark measures reliability of 
collection of old objects, or more precisely, the stability of the heap size in 
the face of a constant live set where at least some data structures are "old".  
.bench.reliability.desc: The benchmark repeatedly allocates a data structure 
that is made to look old (by letting other allocation happen); the old data is 
then let go and a new data structure is built.  The program tracks the heap 
size over time.  If old garbage is collected, the heap will not grow over time; 
if it is not collected, then the heap will grow over time.  If the heap does 
not exceed its observed maximum size for 20 iterations, the program concludes 
that the heap size is stable.  .bench.reliability.java-version:  The benchmark 
runs on all Java versions.   .bench.reliability.params: The benchmark accepts 
three parameters: the volume of the data structure, the volume of allocation 
necessary to make the data structure look "old", and the limit on the number of 
iterations to run.  .bench.reliability.gc.report: The experiment reports 
whether the heap stabilized before the iteration limit was reached (tag: 
stable, value: boolean), the number of iterations that had to be run before the 
heap size was stable (tag: iterations), and the peak observed heap size during 
the run (tag: peak-heap).


PromptFinalize Benchmark

.bench.prompt-finalize: The PromptFinalize benchmark measures the promptness of 
finalization.  .bench.prompt-finalize.desc: The benchmark measures how long it 
takes from a finalizable object becomes unreachable until its finalizer is 
run.  Allocated objects are trees of uniform size.  The experiment allocates 
many trees but lets them die at various ages. Immediately before a data 
structure dies, the current time is remembered in the data structure.  When the 
data structure's finalizer is run, the difference between the present time and 
the remembered time is recorded.  The outstanding number of finalizations for 
an age is also remembered.  The size of the live data set  is kept stable and 
nontrivial. .bench.prompt-finalize.java-version: The benchmark runs on all Java 
versions.  .bench.prompt-finalize.params: The benchmark takes two parameters: a 
scaling factor for the live data, and a scaling factor for the number of 
iterations.  .bench.prompt-finalize.report: The benchmark reports six values 
for each object age as a blank-separated six-tuple of numbers (tag: per-age).  
The values are: age, number of objects (NB! not trees) finalized, longest time 
from death to finalization for the age, shortest time ditto, average time 
ditto, and the number of outstanding finalizations for the age when the program 
ended.


PromptReference Benchmark

.bench.prompt-reference: The PromptReference benchmark measures the promptness 
with which references are placed onto their reference queues after they become 
eligible for enqueueing.  .bench.prompt-reference.desc: The exact functionality 
for this benchmark has yet to be determined.  
.bench.prompt-reference.java-version: This benchmark runs on Java 1.2 and later.



MACRO-BENCHMARKS

.benchmarks.macro.intro: The macro benchmarks are different from the 
micro-benchmarks in several respects.  First, they perform useful or somewhat 
useful tasks; in particular, memory is allocated as working storage for 
computing results, not to stress the memory manager.  Second, these benchmarks 
tend to spend less of their time interacting with the storage manager than do 
the micro-benchmarks, because they are actually computing something.  Third, 
the programs have more complicated program logic than the micro-benchmarks; 
again, this is because they do something useful.  Taken together, these 
differences imply that overhead from the storage manager is diluted, and the 
programs may give a fairer view of memory management overhead in an application 
than the micro-benchmarks do.


The Boyer Benchmarks

.bench.boyer: The Boyer benchmark, written by R Boyer and J Moore, was 
published as part of the Gabriel benchmark suite (book.gabriel85).  The program 
is a simple theorem prover; Boyer describes it as "a rewrite-rule-based 
simplifier combined with a very dumb tautology-checker".  The program is 
allocation-intensive.  The original benchmark isn't much of a match for today's 
garbage collectors, but by adding a scaling parameter suggested by Boyer, the 
benchmark can be made more difficult.  .bench.boyer.desc:  Two variations on 
the Boyer benchmark are used in the Java benchmark suite, translated from 
Scheme versions used in GC research (paper.ch97).  .bench.nboyer: The nboyer 
benchmark is Boyer's benchmark with some bug fixes and the scaling parameter 
added.  .bench.sboyer: The sboyer benchmark is a variant on nboyer that uses a 
sharing cons; the sharing cons changes the storage use profile radically at a 
small cost in mutator time.  .bench.boyer.design: A more detailed description 
of the benchmarks may be found in design.java-benchmark.boyer .  
.bench.boyer.java-version: The benchmark runs on all Java versions.  
.bench.boyer.params: The Boyer benchmarks take two parameters: a maximum 
scaling parameter for a single run, and the number of iterations.    
.bench.boyer.report:  Each program reports the peak heap use (tag: 
peak-heap-m-n) and the running time (tag: time-m-n) for each iteration.  In 
addition, the time for every 1000 rewrites is recorded for each iteration, and 
the 20 longest and shortest times per iteration are reported as a 
blank-separated list of numbers (tags: max-gap-m-n and min-gap-m-n).  In all 
cases, the m-n in the tag denote the scaling factor and the iteration number.


TextSearch benchmark

.bench.text-search:  The TextSearch benchmark is the core of a server-like 
text-searching application.  .bench.text-search.desc: The program has two 
phases.  .bench.text-search.phase1:  In the first phase, the program reads a 
text file (from disk or from a URL connection) and builds a large in-memory 
database.  The data base is arranged for fast searches on words.  The build 
process results in allocation of both short-lived and very long-lived objects.  
.bench.text-search.phase2: In the second phase, the program processes a number 
of queries and update requests against the data base.  This results in a 
moderate amount of allocation of objects with short and intermediate lifetimes, 
and a smaller amount of very long-lived objects.  .bench.text-search.input:  
For the test input the benchmark uses the full text of the Bible, with one 
verse per line.  The input consists of 4.5 MB of text in 31102 lines, with 
12899 distinct words.   .bench.text-search.params: The parameters to the 
benchmark are a scaling factor for the amount of text, and a scaling factor for 
the number of iterations to run.  .bench.text-search.design: A full design of the benchmark may be found in 
design.java-benchmark.textsearch(0) .  .bench.text-search.pitfalls: If the 
benchmark is run as an applet, the download time for the input file will most 
likely swamp other computation except over a very fast connection; the 
benchmark is therefore not ideally suited to be run as an applet.  
.bench.text-search.report: The benchmark reports two values for each iteration 
of each experiment: the time for the iteration (tag: time-k) and the size of 
the heap at the end of the iteration (tag: heap-k) (for iteration k).  It also 
reports the total time for each experiment (tag: time).  
.bench.text-search.experiments: There are two experiments:

.bench.text-search.simple: The "simple" experiment processes each query or 
update request in isolation, computes the result, and throws the result away.

.bench.text-search.disjunctive: The "disjunctive" experiment processes each 
query or update request, then computes the union of the results into one big 
result set, and throws the result away.


FEATURE COVERAGE

.not-covered: The following MM-related features of Java are not covered by the 
benchmark suite.  .not-covered.jni: The interaction of JNI and MM is not 
explored.  Largely, the reason is that JNI does not require much MM 
cooperation, and it is possible to use JNI in the presence of a precise garbage 
collector.  .not-covered.reference-queue:  Reference queues are not mentioned 
explicitly in any benchmark but will be covered as part of the PromptReference 
benchmark.  .not-covered.memory-advice:  The memory-advice facility in Java 1.2 
is not covered.  A benchmark should be added that explores this facility.  
.not-covered.debug-interface: The Java 1.2 debugger interface is not covered, 
because I consider it peripheral to the benchmark suite.


USER INTERFACE

.ui: The user interface is the means by which the user interacts with the 
benchmark suite.  In the initial phase, the user interface will be minimal: a 
web page with a link for each applet benchmark, a Windows batch file for the 
command line benchmarks under Windows 95 and Windows NT, and a Unix shell 
script for the command line benchmarks under Unix.  Each applet benchmark will 
have an input field for the scaling parameter if appropriate, Start and Stop 
buttons to run the benchmark, and a text-output window.  The user can 
select-and-copy the output text from the output window if desired.  On-line 
help will be provided on the web pages themselves.


REQUIREMENTS

.req: This section discusses the design's conformance to the requirements in 
analysis.java-benchmark(1).req.*.

.req.public: The requirement .req.public requires a description of each 
benchmark's purpose to exist, detailing what the benchmark is expected to test 
and which measurements it makes.  The description of the benchmarks in this 
document are adequate descriptions, but this document is not public.  A 
document that can be made public must be authored. 

.req.impartial: The benchmarks have been designed without any particular memory 
management system in mind.  The requirement states that the benchmarks should 
be designed so that it's hard to cheat on them.  However, part of the idea of a 
standard benchmark suite is that its source and object forms are well-known.  I 
propose that the best we can do is to write programs that do not contain 
obviously useless computations.

.req.portable: Portability is preserved in that, although measurement 
technology may be re-implemented for each JVM, a portable version of the 
measurement technology will be implemented, and each benchmark will be written 
to conform to the Java specification.  There only implementation-dependent 
aspects of the Java specification deal with nondeterminism due to thread 
scheduling; any threads code in the benchmark suite will be written so that it 
does not depend on thread scheduling details.

.req.comparison: The benchmarks allow JVMs to be compared: time and space are 
reported for each benchmark as appropriate, and these quantities are comparable 
(on comparable systems).

.req.reproducible: Except for issues that lie outside the Java language (JVM 
"pollution" across multiple runs; JVM/OS interaction; other processes present 
on the computer; amount of RAM, etc), the benchmarks will be independent of 
system state such as the time of day and random number generator seeds.

.req.scaleable:  The benchmarks for which scaleability is an issue accept one 
or more scaling parameters.  .scale.maintain: The default settings of scaling 
parameters can be changed as part of the benchmark maintenance process.  New 
benchmark suite versions may incorporate changes in the default settings.  
.scale.self: Some benchmark scale up the computation automatically to avoid 
problems of measurement precision.

.req.early-bird: I'm giving the project my undivided attention!

.req.maintained: The splitting of the tests into many specific benchmarks 
(rather than a  single "MM Benchmark") allows benchmarks to be added and 
removed without invalidating old results for benchmarks that are not affected.  
That structure also allows individual benchmarks to be upgraded independently 
of the others, and simplifies maintenance work generally.  The structuring of 
each benchmark into a framework part and a benchmark part makes it easier to 
maintain the suite as a whole and each individual program.

.req.coverage: The line between MM functionality and other JVM functionality 
can be blurry.  However, as defined in the sections JAVA and FEATURE COVERAGE, 
the current design covers all of Java except JNI and the Java 1.2 memory advice 
facility.  The current design takes a narrow view and examines only those 
aspects of the Java language (as opposed to the JVM implementation, say) where 
the storage manager will be involved on any reasonable implementation.

.req.reality-check:  The benchmarks .bench.boyer and .bench.text-search have 
realistic allocation, use, and deallocation patterns and can fairly be said to 
represent real, if difficult, applications.

.req.easy: The proposed user interface makes it reasonably convenient to run 
the benchmarks.

.req.mps-compatible: This requirement is suspect.

.req.measure.speed: Speed (time) is measured in various ways by FastAlloc, 
HashCode, Serialize,  Boyer, and TextSearch.

.req.measure.space: Space is measured in various ways by FastAlloc, Serialize, 
Reliability, Boyer, and TextSearch.

.req.measure.pauses: Pause times are measured by Boyer and TextSearch.

.req.measure.leakage: Leakage is measured by Reliability, Boyer, and 
PromptFinalize; these all have constant live sets and heap growth over time 
should be interpreted as symptoms of leakage.

.req.measure.correctness: This requirement is suspect.

.req.measure.limits: The existence of a heap size limit is probed by HeapSize.

.req.measure.reliability: This requirement, as stated in 
analysis.java-benchmark(1), is suspect; however, reliability in terms of 
guarantee of collection is a useful concept.  See .req.measure.leakage.

