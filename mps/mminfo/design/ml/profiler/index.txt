                       THE MLWORKS SPACE PROFILER
                           design.ml.profiler
                            obsolete design
                            nickb 1996-08-22


INTRODUCTION:

The MLWorks profiler can measure and report space-related behaviour of programs 
(allocation, copying by the GC). This facility is called "the space profiler". 
This document describes it.


OVERVIEW:

None; everything's still in the NOTES section.

.overview.hist.0: Nick Barnes, who wrote the space profiler, wrote a document 
describing it (mail.nickb.1996-08-16.17-17). Revision 0 of this document simply 
has that mail, slightly edited, in the "NOTES" section.


TEXT:

.define.function: In this document, a "function" is a single code item
generated by the MLWorks compiler. It will reside (maybe with other
code items) in a code vector. It corresponds approximately to a single
piece of source code (as munged by the compiler), but does not
necessarily correspond to a function in the ML sense.

.define.function.example: In the code generated for the following ML:

  fun f x y = x+y:int;
  val g = f 3;
  val h = f 4;

g and h are ML functions, but are not functions for purposes of
profiling. There are two functions here for the purposes of profiling:
(1) f and (2) the partially applied function f n for any n. g and h
are both instances of the latter function.

.collect: This section describes the kinds of data obtained by the
space profiler.

.collect.other: MLWorks also has time profiling and call-count
profiling. The different kinds of profile information are held
together in a profile record for each function.

.collect.other.time: "Time" profiling scans the stack at each tick of
a profiling clock, gathering information about functions found at the
top (i.e. currently executing) or elsewhere. It is also capable of
recording information about call patterns.

.collect.other.call: "Call-count" profiling counts calls to each
function.

.collect.alloc: The space profiler counts the size of objects
allocated by each function. E.g. this function allocated 9424 bytes.

.collect.copy: The space profiler counts the size of objects allocated
by each function which are copied by the garbage collector (and
therefore which survive until GC time). E.g. the GC has copied 352
bytes which were allocated by this function.

.collect.copy.more: Although the "copied" number is usually much
smaller than the "allocated" number (typically 10%), it may be larger
for a function which mainly allocates very long-lived data (which is
copied more than once by the GC as it advances up through the
generations).

.collect.copy.gen: The space profiler can break down the copying
information for each function according to the generation being
collected at the time of copying. E.g. this function allocated 304
bytes which were copied out of generation 0 and 48 bytes which were
copied out of generation 1.

.collect.type: The space profiler can break down the allocation and
copying statistics into runtime object types.

.collect.type.types: The runtime object types are: records, strings,
arrays, bytearrays, code, weakarrays, closures, pairs, and total.

.collect.type.typical: Most functions allocate records and pairs.
Hardly any functions allocate strings or bytearrays. As yet, no
functions allocate code or weakarrays.

.collect.type.sizes: The type breakdowns include number of objects,
total size of objects, and "overhead" (i.e. headers, padding, entry
list slots).

.collect.type.example: E.g. this function allocated 300 pairs, total
size 2400 bytes, overhead 0 bytes, and 71 records, total size 7024
bytes, overhead 320 bytes. Of these, 19 pairs (total size 152 bytes,
overhead 0 bytes) and 2 records (total size 152 bytes, overhead 12
bytes) were copied out of generation 0. Of these, 1 pair (total size 8
bytes, overhead 0 bytes) and 1 record (total size 40 bytes, overhead 8
bytes) were copied out of generation 1.

.report: The MLWorks runtime can be made to profile all code which
runs while it is invoked, and output a report to a file.

.report.options: There is some degree of control over the functions
profiled and the kind of profiling done. See the runtime help for more
details.

.report.format.simple: The simplest level of space profiling produces
a report in the following format:

> ----- report begins here -----
> MLWorks space profile
> 28436256 bytes used for profiling
> 6480 collections
> functions profiled : 7011
> ---
>        alloc      copied  name
>   6651782992   896813760  total profiled
>            0           0  decode_sigma[main._encapsulate:1681]<Closure>
>        18296        4256  decode_sigenv[main._encapsulate:1689]<Closure>
>       210320       35944  augment_cb[main._toplevel:903]
>      1176936      235368  <case><Match0>[main._toplevel:1021]
>     99362096           0  <anon>[harp._registercolourer:457]

.report.format.copies: If the space profiler is collecting
per-generation information on copying (.collect.copy.gen), each
function's space profile report is formatted like this:

>      1176936      235368  <case><Match0>[main._toplevel:1021]
>                   211160       21440        2528         240

.report.format.full: If the space profiler is doing full type
breakdowns (.collect.type), each function's space profile report is
formatted like this:

>      1176936      235368  <case><Match0>[main._toplevel:1021]
>                   211160       21440        2528         240
>           gen            records       pairs    closures     strings      
arrays  bytearrays        code  weakarrays       total
>            0     no        40085       52892         416           
0           0           0           0           0       93393
>                size       747144      423136        6656           
0           0           0           0           0     1176936
>            overhead       213232           0        1664           
0           0           0           0           0      214896
>            1     no          194       25990           8           
0           0           0           0           0       26192
>                size         3112      207920         128           
0           0           0           0           0      211160
>            overhead          780           0          32           
0           0           0           0           0         812
>            2     no            0        2680           0           
0           0           0           0           0        2680
>                size            0       21440           0           
0           0           0           0           0       21440
>            overhead            0           0           0           
0           0           0           0           0           0
>            3     no            0         316           0           
0           0           0           0           0         316
>                size            0        2528           0           
0           0           0           0           0        2528
>            overhead            0           0           0           
0           0           0           0           0           0
>            4     no            0          30           0           
0           0           0           0           0          30
>                size            0         240           0           
0           0           0           0           0         240
>            overhead            0           0           0           
0           0           0           0           0           0


.result: MLWorks allows ML code to obtain profile results as ML
values. The interface (in MLWorks.Profile) allows a very fine degree
of control over the profiling of each function. The profiling results
contain every piece of information generated by the profiler.

.tool: MLWorks has a fairly basic profiling tool, which uses space
profiling information.

.tool.discard: The tool does not use any of the per-generation
information (.collect.copy.gen) or the type breakdown information
(.collect.type).

.tool.chart: The tool uses one space and one time number for each
function to create a bar chart, in which the space and time numbers
are combined (in a weighted average) to make the bar height for that
function.

.tool.chart.weight: The weights of the weighted average are controlled
from a slider.

.tool.chart.space: The space number used is the total size of
allocated objects.

.tool.chart.time: The time number used is the number of times this
function was found at the top of the stack.

.tool.total: The tool displays some total information [####].

.tool.detail: The tool displays some space profile information for the
currently selected function: total allocation, total copying.

.how: This section describes how MLWorks gathers space profiling
information. Most of the sources are in rts/src/profiler.c, under the
"Space profiling" banner.

.how.compile: space and time profiling place no requirements on the
compilation of profiled functions. Optimized code can be profiled.

.how.compile.call: Call-count profiling requires code to be compiled
differently (with the switch "compile for stepping and call-counting"
on).

.how.alloc: MLWorks uses distinctive code sequences to allocate
objects. These sequences may be different in leaf and non-leaf
functions.

.how.modify.start: When starting a profile, the MLWorks runtime scans
the code of every function in the heap, modifying allocation sequences
into calls to assembly-language routines in the runtime (see
.how.routines).

.how.modify.stop: When stopping a profile, the MLWorks runtime rescans
code in the heap, undoing all the modifications.

.how.modify.source: The code modification stuff is in
rts/src/arch/$ARCH/arch_code.c.

.how.modify.flush: The instruction cache is flushed after the code
modification.

.how.routines: The assembly language routines perform the allocation
and then record the closure of the allocating function and a pointer
to the allocated memory in a space profile area of memory (see
.how.area).

.how.routines.source: The assembly language routines are in
rts/src/arch/$ARCH/interface.S, under ml_profile_alloc*.

.how.gc: After the GC of any generation, the space profiler updates
its per-function records and copies information on surviving objects
from the space profile areas of the "from" generation to those of the
"to" generation (see .how.area).

.how.gc.creation: At the GC of creation space, the space profiler
records every allocated object and changes each function closure (see
.how.routines) to point to that function's space profile record.

.how.gc.other: At the GC of other generations, the space profiler need
only record information about surviving objects.

.how.area: The profiler maintains, for each generation, a list of
space profile areas.

.how.area.creation: For the creation space, the area indicates, for
each object allocated by a space-profiled function, the function
closure and the allocated memory (see .how.routines).

.how.area.other: For other generations, the area indicates, for each
object allocated by a space-profiled function, the space profile
record for that function and the allocated memory (see
.how.gc.creation).

.how.time: Profiling has some time overhead.

.how.time.time: Time profiling makes ML code take 5-20% more time.

.how.time.space: Space profiling makes ML code take 250-500% more
time. This breaks down as follows:

.how.time.space.alloc: Code modified to use the allocation routines
(.how.modify.start, .how.routines) takes 50-100% more time than
unprofiled code, depending on how much allocation it does.

.how.time.space.gc: The post-GC space profile work (.how.gc) takes
200-400% more time than unprofiled code, depending on how much has
been allocated by profiled code and on how much information is being
recorded.

.how.time.record: The time profiler records and reports the time spent
in post-GC space profile work separately. It does not do this for the
allocation routines. Thus when doing both time and space profiling the
time reports do not truly reflect the performance of unprofiled code.

.how.alloc: The profiler can allocate large amounts of memory for
profiling records. This memory is then freed all at once when
profiling finishes. The profiler has a tweaked allocator for this
purpose. See the macro profile_allocator() in profiler.c.

.future: This section describes possible future directions for (space)
profiling in MLWorks.

.future.tool.name: The bars in the bar chart presented by the profile tool are 
not labelled in any way. They should be labelled with function names.

.future.tool.name.how: If the bars were horizontal, they could contain function 
names.

.future.time.routines: The time profiler should record and report time
spent in the assembly-language allocation routines (see
.how.time.record).

.future.time.routines.how: This can be done simply by checking the PC
against some known range in interface.S.

.future.centre: The profiler should do some sort of cost-centering,
attributing clock ticks (for time profiling) and objects (for space
profiling) to "the current cost centre" as well as "the current
function". 

.future.centre.stub: Note that there are already stubs for
cost-centred profiling code (producing an empty report and an empty
result value).

.future.centre.how: "cost centres" will be created explicitly by ML
code, and "the current cost centre" will be changed explicitly by ML
code.

.future.centre.how.space: Changing the cost centre would add a special
record to the creation space profile area. In the post-GC scan, keep
track of the "current cost centre". Propogate these special records to
the areas of higher generations.

.future.centre.how.approx: The above description of cost centres
requires user code to set cost centres. This could be done after the
fact, in an approximate manner, by identifying functions to be
cost-centres. The time profiler could then set the cost-centre as it
finds a cost-centre function when scanning the stack. This could be
used to do cost-centred profiling on delivered code.

.future.centre.how.approx.race: Care would need to be taken when
adding a cost-centre record in a time profile scan, as the scan may
occur in the middle of an allocation. See .future.time.routines.how.

.future.tool.detail: The profiling tool should present more of the
detail of profiling information (e.g. caller information from the time
profile, generation and object analyses from the space profile).

.future.tool.detail.how: This information could be presented for the
currently selected function in a new window (or pane).

.future.live: There should be some way of obtaining a profile analysis
of the data currently in the heap.

.future.live.how: There are already comments, and some ifdeffed-out
code, to support this. Search for "live data" in profiler.c.

.future.live.how.max: This analysis could be obtained whenever the
heap size exceeds the historical "maximum" for this profile run? Then
at the end of the profile there would be a live data analysis for the
largest heap peak. Alternatively for several local heap peaks.

.future.runciman: It would be good to generate space-time graphs
similar to those in Colin Runciman's work (paper.runciman92,
paper.rr96).

.future.runciman.how.live: One way would be to generate live data
analyses at many points in time (e.g. after every GC, once per second,
&c), and doing post-profile analysis on all the data.

.future.runciman.how.live.data: Doing this at every GC might take too
much data, as "live data analyses are much larger than their deltas".
A cost-centred version may be preferable. Doing it "once per second"
might be cheap enough (imagine 20k significant allocating functions, 8
bytes/function => 160k/second).

.future.runciman.how.live.data.discard: We could say "use up to X Mb
of space for generating Runciman graphs", then discard alternate data
points when this space fills up.

.future.runciman.how.death: An alternative way of generating Runciman
graphs, at the "every GC" granularity, is to record at each GC a
breakdown of the surviving objects by allocator. Also each creation
space GC would need to record a breakdown of allocated objects by
allocator. This would allow deltas to be computed, but might still be
too costly.

.future.tool.heap: The profile tool should in any case be able to draw
a graph of heap size against time. This could be shaded according to
current cost centre.

.future.tool.heap.max Heap maximum points (see .future.live.how.max)
could be marked, with a gesture to bring up the heap analysis.

.future.tool.heap.runciman: If we have Runciman graphs, they can be
presented here.


