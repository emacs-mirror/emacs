               INITIAL TASK ANALYSIS -- INTERVIEW 5: JEM
                       analysis.task.interview.5
                             temporary doc
                           richard 1996-07-09

 - Report 5 

Interview Number 5. Jem. 

Normal Development Tasks: 

Watson: developing, and/or fixing problems. 

Experience: 

YeArs

Platforms: 

UNIX, Win95/NT.  Develop on UNIX, cross-compile to PC.  Needs min. 16MB, better 
32MB. 

Software tools and environments: 

LispWorks

Past use of MM tools: 

None. 

Memory Problems in past: 

Performance problems can be due to inefficiencies with the algorithm, or with 
allocation, due to the way it is implemented.  The size of the application is 
also sometimes a problem.  Some bits of code allocate huge amounts of transient 
memory which is GC’d at the end of the process. This slows things down.  
Another problem was that when the database was saved, it doubled the size of 
the process temporarily.  This gave an artificial limit to the size of the 
memory that was the minimum.  You could use the database, but never save it.  

Solutions: 

The doubled database problem was solved by changing the way things were saved.  
We were originally using some standard Lisp technology which did things a 
particular way.  The code was keeping an unnecessary record of everything that 
was being dumped.  We looked with the standard LW tools, so we could see which 
function was being called, and look at the implementation.  This did not need 
any special tools.  We changed the code so that we did not keep a record of 
exactly what we had dumped.  When you are dumping Lisp objects it is very 
important to record exactly what was dumped so that you can load it back in in 
the same state.  

We found another good saving with the Generic Function collapser, when the size 
of Watson was an issue at 5MB.  It turns out that most of the GFs in Watson 
only have one method, so that you can re-write that method as an ordinary 
function and throw away the GF.  We wrote some code that went through the image 
and did this just before we saved everything. This saved about 300K on the size 
of the image.  

As we use Lisp, it always manages its own memory, so that it always works.  
What we have to find are places where it allocates memory that is GC'd almost 
immediately afterwards.  This is laziness in coding - you allow it to allocate 
extra things because you know that it is going to be cleaned up for you.  We 
have to find these and make sure that the allocations do not happen.   This is 
the level we normally work at: the code level.  We don't look at the GC 
itself.  We were doing some computationally intensive stuff for a layout, where 
it used single floats, which in Lisp were boxed.  For every calculation it 
allocated a new floating point number object and then threw this away after 
use.  We asked for some means of re-using the same bit of memory, like an 
accumulator.  This massively reduced the allocation during that part of 
execution .  We seek to make the best use of the MM resources that are already 
there.  We have not been down to the allocator level at all. 

Other Tools that would be useful; Other ways of looking at the data: 

One of the things we would like would be weak pointers.  These would say to the 
GC “if this is the only thing pointing to this object, throw the object away".  
In Watson we have a list of all objects currently in the image - we need to 
check that somethign loaded from the database is the equivalent of something we 
already have.  If we get rid of all the charts that reference that object, then 
we can throw it away, but we don't know that at present, as it is still in the 
list.  We want the list to be "weak".  We have asked for this for 2 years now, 
but it cannot be very easy as it has not appeared in our Lisps.  

What we really want are tools for finding out and managing the redundant stuff, 
at the level of code.  The GC handles allocation and de-allocation ok.  You 
want to be able to put some monitoring gear in the code, and run it for a 
while, and say look, this bit of code is allocating huge amounts of memory, so 
lets look at them.  At the moment we have a profiler which tells us how long a 
particular function was on the stack.  It ranks the functions so that you can 
look at the ones used the most, and get some real improvements.  However, 
top-level functions are permanently on the stack, so what you want to be able 
to do is to group the functions, and say "while these functions are on the 
stack, keep counting".  You want to know when they are actively executing as 
this identifies the relevant block of code that is being executed the most, so 
that improvements can be made.  You want a similar thing in terms of 
allocation, as if you can reduce allocation you can improve performance.  (Q: 
Size or frequency of allocation?) Not sure.  There is a fixed overhead for 
allocation, but large allocations can trigger GC.  We're interested in both. 

Are you interested in being able to influence how the GC works at the 
allocation / de-allocation level?

Yes.  If we could say that this bit here is ephemeral, you can say that "I'm 
going to create garbage now, but when the function exits it can all be thrown 
away".  There is this notion of dynamic allocation in Lisp, where you can make 
allocations on the stack, so that  it automatically gets GCd when the function 
exits.  This kind of thing would be useful.  

Software Development

Any particular approach to software or development that works for you? 

In much Watson work we find much redundant translation of data from one form or 
representation to another and then back to a different structure.   Strings to 
something else, back to lists etc.  Once the same basic data is in memory, you 
should be able to use it without all these changes.   We have taken about 100K 
off of Watson by removing this kind of redundancy.  This makes it go much 
faster: streamlining. 

Is there a point in development when you specifically consider MM issues?

Yes, we always think about how much memory we are taking up. A database could 
be up to 20MB, so if you can change one byte per row, you can save a great deal 
of space.  There are different entry paths to a database.  For instance, a 
record could be represented as a list.  In using a list, there are an extra 12 
bytes of data on top of the content for every element in that list.  Older 
versions worked like this.  Using more basic representations these days, we can 
get this down to 4 bytes extra per element.  This can save a lot of memory.  
Also, looking for the 5th. element in a vector is just an address calculation, 
while going for part of a list involves constantly consing down it.  Converting 
a record into an object as a vector in the old system created a completely new 
vector, so that we had two identical ones.  Now we just use the one for both 
database and object rather than do conversions.  There is a trade-off between 
ease of access and performance.  When you have 100 objects, anything is ok and 
will work.  When you have 30,000 objects, things are different.  I have a chart 
of 100,000 object, each with 20 slots.  As well as this there is the object for 
the graphical representation, which points at all the individual objects, and 
includes x and y co-ordinates, centre of gravity, etc.  Many of our 
improvements come from looking at all these data structures, and removing some 
of them.  We would like  to be able to look at some of these data structures, 
and ask "how big is it?", although everything points to everything in Lisp.  
You cannot get that information at the moment.  

How do you handle these issues? 

How do you test the results of your decisions? 

Do you have tools to help with this?

Do you avoid particular techniques because of your requirements? 

Thinking about possible MM products. 

For a gain in performance or size, do you want a product "out of the box" or 
tuneable?  

We work more at the level of the code.  We want to say that this stuff here is 
going to be garbage when the function exits, so get rid of it.  We don't 
usually look at the GC itself.  We could be interested in looking at this 
though.  We could supply the GC with info. as we know what the application is 
doing.  For instance, when we load a database into memory, we know that it is 
going to be there for a long time, with nothing deleted, until a new database 
is loaded.  Therefor there is not point in scanning it looking for garbage.  
When a new database is loaded, the whole of the old one is thrown away.  We can 
tell the GC important things about the objects that are being allocated, i.e. 
whether they are worth looking at , and these can help gain efficiency.  We 
would like mechanisms for this.  That is the level we want to work at.  

Locality would also be a very interesting thing to look at.  If Watson grows 
larger than real memory, it starts using swap.  This should be ok, as for 
instance, if you are looking at the Link chars, then all the other stuff for 
the report etc. should be swapped out so that you end up with a working set 
that is wholely memory-resident.  However, this does not happen.  Instead the 
system thrashes like crazy and dies, because the locality in the code is so 
bad.  It is just the way it is allocated.  We would like something to help with 
this, and to show the effects that such help has had.  The basic unit of work 
in Watson is the dataset.  If this was allocated contiguously, then perhaps it 
could all be swapped out as a chunk, rather than as distributed bits.  
Would you value a range of MM policies to suit the jobs at hand? 

Would it help to have precise feedback on how a given policy affected your 
code?  

Would you consider any re-engineering of your software to work with the MM 
products?

Do you have any reservations about using MM products? 

Would this apply for the rest of your team? 

Would you want your users (the OEMs) to have access to the MM tools?  Or even 
the end users?

Anything else to add that we have not covered? 

We have other problems to do with delivery, because in Lisp everything is 
included, and we have to say "throw out anything that is not referenced by 
anything else".  I expect that this is a different area to the one you are 
interested in, though. 

ATTACHMENT
   "Report 5"

