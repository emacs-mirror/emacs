              INITIAL TASK ANALYSIS -- INTERVIEW 4: GAVIN
                       analysis.task.interview.4
                             temporary doc
                           richard 1996-07-09

 - Report 4 

Interview Number 4. Gavin. 

Normal Development Tasks: 

Worked on a 3D modelling engine for a CAD/CAM system 

Experience: 

Several years.  

Platforms: 

Multi-platform system, with lots of RAM.  

Software tools and environments: 

All home-grown systems.  Very little use of debuggers, but much use of internal 
tracing tools.  One of these shows memory allocations as a particular 
structure.  When these change it checks them in the context of the client 
rather than the memory manager.  This is very useful for finding leaks etc.  
This tool knows about different objects and can check actions as they are 
executed.  

Past use of MM tools: 

None. 

Memory Problems in past: 

Worked on a 3D modelling engine for a CAD/CAM system.  This was part object 
manager, part memory manager, with many levels of service.  There were many 
problems in trying to debug code which uses this allocate a great deal of 
memory.  The system is very powerful, and has much checking and support 
provided over the years.  The problems I found were the usual things like 
corrupted pointers, although these were not so common.  More common was having 
temporary data that should have been kept (but was not) as it was still 
referred to by some more long-lived data.  Another problem was that of trying 
to find when a particular piece of memory has a specific value assigned to it.  
We has a number of particular problems due to the sophistication of our MM.  We 
had various allocation domains, along with prediction of lifetimes, and a 
traversal system that was vulnerable to corruption.  We had blocking of changes 
and rollback built into the MM system and the programming language.  Excessive 
size was not typically a problem, but performance was.
  
Solutions: 

Allocation was seen as a very costly operation, so we tried to second-guess 
this performance by keeping data statically.  A particular trick was to use 
conditional stack allocation, so that if a function used one block of memory 
under a certain size we would use memory statically allocated on the stack, or 
call a heap allocator.  The whole operation was automated with a macro.  

We also used temporary watermarks, which was a cheap way of doing multiple 
‘free’s.  This solves two problems in one: a) keeping track of what workspace 
you have allocated, and b) the system may be able to optimise performance by 
freeing multiple blocks at the same time.  A watermark operates by setting 
temporary marks and getting identifiers for them.  You can then tell the system 
to clear the whole lot; this was done with every API call.  

These were fairly effective solutions.  There was very little difference in 
performance between allocating temporary and permanent data.  One problem was 
that of trading off the idea of temporary data that you can forget about and 
free off later against just how much total memory you can allocate before you 
come to a ‘clear’.  This caused a problem with certain large parts and certain 
algorithms.  Because we had this distinction between permanent nodes which 
could be traversed in a tree, and temporary nodes, we did not suffer much from 
memory leaks as we could pick them up very quickly.  However, we had to work 
very hard to achieve the nodes and the traversal system.  We used our own 
in-house C-like language with a pre-processor.  It worked but it was clumsy.  
It would be much easier in Dylan. 

Other Tools that would be useful; Other ways of looking at the data: 

Software Development

Any particular approach to software or development that works for you? 

Yes, we were only dealing with the one product.  Either fixing problems or 
adding new functionality.  We had quite a sophisticated life-cycle, with 
proposals, detailed requirements specifications, design proposals, review, and 
testing.  Not all projects got the full treatment.  

Is there a point in development when you specifically consider MM issues?

This was often a consideration, as new functionality often added new fields to 
structures.  We would often try to calculate roughly how many of such 
structures would be allocated in a typical application, and what the increase 
in memory requirement would be.  If it was less than 10% extra then typically 
it was acceptable.   

Work on performance rarely targeted MM, but rather targeted algorithms, to 
avoid doing the same job twice for instance.  Often the perceived cost of 
allocating data led to attempts to keep things static through increased 
complexity, but that can sometimes be the right thing to do.  If this perceived 
cost was changed, then the balance of work might be changed.  

We had one function that repeatedly allocated and freed two blocks of memory, 
one large, and one small.  This gave dreadful fragmentation, which we solved by 
keeping and re-using the same blocks.  

How do you handle these issues? 

How do you test the results of your decisions? 

Do you have tools to help with this?

None that showed memory usage.  We had statistics from the MM module that 
showed how many nodes of each type were used.  There was no tool for finding 
out when a thing was allocated, or what the sequence of allocation was, or 
where they had been allocated from.  You could use sophisticated tracing by 
switching it on in the allocator and watching it go by.  You could also see the 
call tree.  This was one way to detect fragmentation.  After this I would look 
at the sections of code that were actually making the calls.  We had no real 
tools for showing memory use graphically or dynamically.  

Would these have been useful?  Something which was able to show memory usage 
graphically in a way that could usefully be watched, and which gave you the 
information you wanted, would be very useful, but I would find it very 
difficult to specify such a tool.  

Do you avoid particular techniques because of your requirements? 

Thinking about possible MM products. 

For a gain in performance or size, do you want a product ‘out of the box’ or 
tuneable?  

Would you value a range of MM policies to suit the jobs at hand? 

Would it help to have precise feedback on how a given policy affected your 
code?  

Yes perhaps.  Our performance tools told us how much was going in allocation, 
but typically we did not look at this.  The MM module was very rarely modified 
in fact.  It was seen as being stable, and full of crocodiles.  Change to that 
was seen as very dangerous.  

Would you consider any re-engineering of your software to work with the MM 
products?

Do you have any reservations about using MM products? 

Would your colleagues there have been cautious about using them?

The company had a very strong NIH culture, with very little used in the way of 
outside tools.  Their own debuggers, source control, editors, language 
compiler, file formats.  One reason was that you cannot really rely on outside 
tools having the flexibility you need.  We had all these built-in features that 
you cannot throw away lightly.  We looked at the C analysis tools, but did not 
find them very helpful.  If they had used an external MM and it turned out to 
have a bug, it could be a year before a bug got fixed.  This would severely 
damage our competitiveness.  The issues are perceived reliability and 
flexibility.  

Would this apply for the rest of your team? 

Would you want your users (the OEMs) to have access to the MM tools?  Or even 
the end users?

Anything else to add that we have not covered? 

I think you have a very difficult problem in what you are trying to do, in 
making something that is usable, and yet powerful.  Hard to track down memory 
use problems were quite rare for us, due to the nature of our product, but I am 
aware that other environments may be different.  Once you are dealing with 
memory at an object level verification becomes much easier.

ATTACHMENT
   "Report 4"

