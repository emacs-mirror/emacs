          INITIAL TASK ANALYSIS FOR MEMORY MANAGEMENT PRODUCTS
                             analysis.task
                             temporary doc
                           richard 1996-07-09

ORIGINAL DOCUMENT

 - TA Deliverable 1 


CONTENTS

0.0 Executive Summary
1.0 Introduction
2.0 Problems and Solutions in the Past
2.1 Background
2.2 Past use of Memory Management tools
2.3 Past Experience of Memory Problems
2.4 Solutions  Adopted in the Past
3.0 Software Development Methods
4.0 Tool Requirements and Developers’ Views
4.1 Tool Requirements
4.2 Developers’ Views on Memory Management Products
4.3 Summary Table of Tools and Facilities Requested
5.0 Task Outlines
5.1 ScriptWorks Tasks
5.2 Dylan Tasks
5.3 CAD/CAM Tasks
5.4 Watson Tasks
6.0 Conclusions
6.1 General Conclusions
6.2 Further Work


0.0 EXECUTIVE SUMMARY

The purpose of this work is to provide an initial task analysis which can be 
developed to support the design of a Graphic User Interface (GUI) for Memory 
Management tools.  This interface will be required where the tools can be tuned 
or configured, and where they give feedback to the user on the performance of 
the system that is using them.  

The first step in this process was to contact a range of Harlequin developers 
in order to obtain a base level of contextual and task information.  A set of 
structured interviews were carried out and written up to produce the report 
below.  It is possible that the more detailed analysis of specific tasks will 
require further work with these or other developers.  

The interviews and subsequent analysis reveal three distinct approaches to the 
problems of application size and performance.  Each approach corresponds to a 
company product area, and has its own set of priorities with associated methods 
and tasks.  Each will thus have its own set of usability criteria.  The 
description of each approach indicates the information and facilities that a 
developer from the relevant group would wish to find in the user interface to 
the tools that they are offered.  There is some common ground between the 
groups, as in the requirement for allocation profilers, but the more detailed 
concerns of each group differ considerably as their tasks and methods differ.  
The ScriptWorks group are most interested in diagnosing problems of 
fragmentation and interactions with the operating system.  The Dylan group are 
most concerned with reducing the quantities of data held by a compiler, and 
with monitoring garbage collection.  The Watson group would very much like to 
know the size of specific data structures.  All three groups have a strong 
interest in passing useful information back to the memory allocator and in 
seeing how this affects performance. 

The tasks as they are undertaken now are summarised in section 5.  The 
interviews were also successful in going beyond this to provide additional 
information about how the developers would like to work if they were given the 
tools they asked for.  This is important in designing for usability, as the new 
tools may offer users facilities that they do not currently enjoy.  Their tasks 
and methods may thus show corresponding changes when the new tools are 
introduced.  A summary table of the tools requested is given in section 4.3. 
This table identifies areas where tool requests overlap, and may help to 
identify the areas of most general usability, where usability is interpreted as 
fitness for purpose.

In section 6.0 a set of steps for subsequent work is defined. 


1.0 INTRODUCTION

The purpose of this work is to provide an initial task analysis which can be 
developed to support the design of a Graphic User Interface (GUI) for Memory 
Management tools.  This interface will be required where the tools can be tuned 
or configured, and where they give feedback to the user on the performance of 
the system that is using them.

The work has an initial complication since no similar tools focused on code 
size and performance improvements currently exist, and thus there is no 
existing body of users who can provide a detailed description of how they may 
be used.  The problems of dealing with code size and performance issues are 
however, well recognised.  We may thus approach developers who are experienced 
in these areas in order to determine:

  - The methods they use in dealing with the problems;

  - The information they require in order to achieve success in different 
contexts;

  - The specific tools that they would wish to have available.

The last bullet point above is particularly relevant.  It takes the application 
of usability engineering beyond the "surface" features of GUI design and 
addresses the problem of determining which product features users regard as 
essential.  This is particularly important as the memory management products 
planned could, in principle, support many of the features found in existing 
products such as "Purify".  Rather than compete directly in this way, the 
products are intended to be positioned as tools for dealing with code size and 
performance issues.  If they are to be highly usable in this context, then we 
have to know which particular elements the users wish to have available in 
different contexts.  There is no existing model to copy, and we must determine 
the answer through task analysis.

The first step in this process was to contact a range of Harlequin developers 
in order to obtain a base level of contextual and task information.  A set of 
structured interviews were carried out and written up to produce the report 
below.  It is possible that the more detailed analysis of specific tasks will 
require further work with these or other developers.  

In total nine interviews were conducted.  In four of these the developers 
turned out to have no experience of dealing with memory management problems 
which related to either the size and speed of the application.  The remaining 
five interviews are reported here in summary form.  

A mixture of open and more specific questions were used to structure each 
interview.  These were used as required: where an interviewee wished to 
continue talking and volunteer a great deal of information, they were not 
interrupted.  This means that some interviewees may not have answered every 
question.  Summary transcripts of each interview are available separately. 
[Where?]

The interviews and subsequent analysis had three goals, (short-term), and two 
objectives, (long-term).

The goals are as follows:

  1. To determine how the developers interviewed could get an immediate and 
convincing benefit from using a product aimed at resolving problems of code 
size and performance.

  2. To specify and prioritise specific usability goals for the product in 
different contexts.

  3. To determine how users might want to work with tools that give feedback on 
the product"s effects on their code. The 'feedback loop' is unique to the 
intended product, and the tools provided must fit the user's work patterns.

The objectives are to provide the task-description information we require in 
order to:

  1. Set usability goals for each stage of the product's design and 
implementation.

  2. Specify the user-machine dialogues that the GUI must support. 


2.0 PROBLEMS AND SOLUTIONS IN THE PAST


2.1 Background

Interviewee 1, ScriptWorks: This developer carries out a wide range of tasks on 
the ScriptWorks core, and has over two years experience of this work.  Target 
platforms are PC, Mac, and UNIX, with Win95 and Mac OS being most important.  
These can be large multi-processor installations.  He works mostly on NT, using 
Visual C environments with compilers, debuggers, profilers etc.  The normal 
"make" tools are used for project management.

Interviewee 2, ScriptWorks: He has done everything for ScriptWorks over a 
period of nine years.  Target platforms are Mac, PC, UNIX from 16 MB up to 600 
MB with 20 GB of disc plus a stripe drive.  His favoured development tools are 
the normal Mac ones, Visual C on the PC, and DBX on UNIX.
 
Interviewee 3, Dylan Compiler: He is a Dylan compiler-writer with over 8 years 
experience of writing compilers and language demonstrators for Lisp.  Target 
platforms are PC, Mac, and UNIX.  He usually works on a PC using Visual C++ 4.0.

Interviewee 4, CAD/CAM Application: He has worked on a 3D modelling engine for 
a CAD/CAM system for several years.  This was a multi-platform system which 
used large quantities of RAM.  All their tools were home-grown systems, with 
little use made of debuggers, but much use made of internal tracing tools.

Interviewee 5, Watson: This interviewee carries out general development tasks 
on Watson, and has many years of experience.  The target platforms are UNIX and 
Win95/NT.  They develop on UNIX using LispWorks, and cross-compile to the PC.  
Watson needs a minimum of 16MB, and works much better with 32MB.


2.2 Past use of Memory Management tools  

None of those interviewed had experience of using "Purify"-type tools.

The ScriptWorks developers had added quantities of "debug-type" information to 
their own memory management code in order to overcome some significant 
problems.  Using Purify was often not appropriate due to the way that they 
organise their own memory. They ask for large amounts of memory at once, and do 
not give it back until the end of processing.  On the Macintosh they used 
virtual pointers with their own paging system.
  

2.3 Past Experience of Memory Problems

ScriptWorks 

These developers had experienced a range of problems.  They were particularly 
bothered by fragmentation due to either the software allocating static buffers 
lazily, or from losing references to memory and ending up with leaks.  
Fragmentation has severe implications, as  PostScript(R) is an "all or nothing" 
system.  If you try to load a 100 MB image into 98MB of memory, the whole image 
is put down to disc.  Some applications will split this into tiles which can be 
rotated individually.  This might give some 2000 images each composed of 
exactly 200K plus some ancillary data.  This could produce fragmentation 
problems where even small objects were taking up large amounts of memory and 
images had to be put down to disc.  500MB would be needed to run a 100MB job.  
This remains their biggest problem.

It is made worse by the way they use 64K segments with different memory pools.  
Objects overlaid on these can cause interactions which consume a great deal of 
space.  In one example an OEM puts 2 million rectangles on a page.  This may be 
extreme, but the OEM really cares about it.  Where the rectangles are put 
vertically down the page, or the page is rotated, ScriptWorks uses much more 
memory.  Competitor"s RIPs do it differently and appear to use much less 
memory.  They are thus easier to sell.  In this instance EP have a work-around 
but if they could, for example, group rectangles with similar attributes, then 
they could get much better reductions in size.

A sensitive point for EP is that they do not have complete garbage collection, 
and one of the reasons for moving to MPS is to get this for the core RIP.  This 
has only been a significant problem once, where Level-2 PostScript(R) 
introduced compression filters.  These are used to retrieve parts of a 
compressed job in sequence, and for colours four may be used in parallel.  Once 
used, they were not recognised as dead objects, and neither were the buffers 
used by each filter, so that they were not collected.  They put in code to free 
the data structures, but then had to go a stage further to make sure that each 
new filtering job inherited the old filters.

Other ScriptWorks problems are related to the paging systems.  Where they used 
their own virtual memory system the display lists could become huge.  They also 
used their own paging system and this could make things crawl.  At the same 
time, NT could get very confused.  Where there should have been plenty of room 
to read several images into blocks, NT would try to cache the files and run out 
of space.  It would then try to page out the areas they were trying to read 
into.  They cured this by using asynchronous read-ahead.

NT also has other peculiarities: running the same job repeatedly gave 
successively shorter execution times until the paging had settled down to suit 
the job in hand.  This was obviously a real problem for bench-marking.  It was 
cured by locking down as much memory as possible.  This produced other 
problems, as where an OEM wants to run co-operative programs in the 
background.  If the RIP has grabbed most of the memory NT starts paging as it 
runs the other programs in low memory.  If they could say that they only really 
care about part of what they have taken, then things would be better, but it is 
very hard to do this on NT.  

Dylan

The Dylan developer noted that  compilers tend to be very memory-hungry, and 
like to hang onto things forever.  As the developers struggle to achieve 
adequate performance, the compiler is embellishing its representations so that 
the roots of the program reference more and more information.  In this case 
garbage collection cannot help.  They need to find out why the compiler is 
hanging onto more and more data.

Where generational garbage collectors are used they face the "Pig in the 
Python" problem.  There is a mass of information generated during a compilation 
which is used during the process and then thrown away at the end of it.  What 
should the GC do?  If it keeps it in the "first" generation, i.e. the one 
collected most frequently, then this generation is very expensive to collect 
because there is so much of it.  If the information is "promoted", it is very 
expensive to get the world back to normality after the compilation is over.  
What useful heuristics can be applied to the memory management model while the 
data is being generated?

A different problem is that of knowing why an application allocates as much as 
it does.  Allocation profilers are used to work on this.  The problem concerns 
both frequency and size of allocation, although it is most commonly related to 
frequency.  The point is that when writing the code the developers may know the 
size of the allocation, but have not have understood the control flow well 
enough to grasp how may times it was going to happen.  Alternatively the 
allocation may be hidden, as it is carried out by system code which is not as 
efficient as they believed.

On another occasion severe memory restrictions meant that the routines for 
"free" and "new" were larger than was comfortable.  Those parts of the program 
that allocated objects with "dynamic extent" were changed to allocate the 
objects as an array on the stack.  This had a huge impact on the efficiency of 
the application.  In Dylan, this sort of thing should really be done by the 
compiler. 

CAD/CAM

The main problems described in relation to the CAD/CAM system were the loss of 
temporary data which should have been kept as it was still referred to by some 
more long-lived data, and the problem of trying to find when a particular piece 
of memory had a specific value assigned to it.  The system has been given much 
checking and support over the years, so that when things like corrupted 
pointers did occur, they were not so difficult to find or fix.  Some specific 
problems were due to the sophistication of their memory management.  They used 
different allocation domains, along with the prediction of lifetimes, and a 
traversal system that was vulnerable to corruption.  The blocking of changes 
and rollback were built into the memory management system and the programming 
language.

Excessive size was not typically a problem, but performance was.

Watson

The performance and size problems encountered by the Watson developer reflected 
inefficiencies in the use of their algorithms, or in allocation, due to the way 
that the application was implemented.  Some code segments allocate huge amounts 
of transient memory which is garbage collected at the end of the process.  This 
slows things down.  Another problem was that when the database was saved it 
doubled the size of the process temporarily.  This gave an artificially 
inflated minimum memory size.  You could use the database, but never save it.


2.4 Solutions Adopted in the Past

ScriptWorks

The ScriptWorks developers described a number of solutions.  The simplest one 
involved counting the number of allocations and de-allocations that had been 
done, and using this to report leaks.  Other approaches were to do 
fence-posting at the end of all allocations or to provide tagging of 
allocations using a separate memory pool.  The latter used memory provided by 
the system rather than by their own internal mechanisms, and allowed them to 
trace where each allocation came from and what it was used for.  The tracing is 
tedious as the information comes back in a textual form and has to be 
manipulated by hand.  It is only compiled in for special "debug" versions of 
the RIP which allow them to investigate unusual problems.

Other ScriptWorks solutions, such as locking down large blocks of memory, have 
been described above.

Dylan

Where some low-level function was suspected of allocating more memory than was 
efficient, the culprit would be tracked down through some hard graft with a 
profiler and some way found of "coding around" the problem.  Data structures 
were also carefully considered, as when using vectors instead of lists, or when 
copying is avoided by the use of destructive operations.  The solution to the 
"pig in the python" is typically to set up some heuristics such as "I have just 
finished a compilation, now might be a good time to do a serious garbage 
collection".

Where the compiler is hanging on to more memory than it should, you can either 
stare hard at the code, or start looking for memory usage that you did not 
expect.  The goal is to analyse memory usage and to work out why some of it is 
confusing.  A typical problem is that once you have worked out what memory is 
being held, you have to determine the cause, and fix it.  This may involve 
destructively modifying data structures to get them back to some sane value.  
By doing this you make them forget about data at the end of a compilation run.

CAD/CAM

For the CAD/CAM developer, allocation was seen as a very costly operation.  
They tried avoid it by keeping data statically.  One trick was to use 
conditional stack allocation, so that if a function used a block of memory 
under a certain size they would use memory statically allocated on the stack, 
or else call a heap allocator.  The whole operation was automated with a macro.

They also used temporary watermarks, which was a cheap way of doing multiple 
"frees".  This solved two problems in one: a) keeping track of what workspace 
had been allocated, and b) helping the system to optimise performance by 
freeing multiple blocks at the same time.  A watermark operates by setting 
temporary marks and getting identifiers for them.  You can then tell the system 
to clear the whole lot.  This was done with every API call.

These solutions were effective: there was very little difference in performance 
between allocating temporary and permanent data.  There was a possible problem 
in the trade-off between the amount of "temporary" data that could be assigned 
to be freed off later, and the total amount of memory that could be allocated 
before coming to a "clear".  The problem arose with certain large machine parts 
and certain algorithms.  The distinction between permanent nodes which could be 
traversed in a tree, and temporary nodes, helped them to avoid memory leaks as 
these could be picked up very quickly.  However, they had to work very hard to 
achieve the nodes and the traversal system.

Watson

The Watson developers usually coded around their problems.  Their "doubled 
database" problem was solved by changing the way things were saved.  They were 
originally using some standard Lisp technology which kept an unnecessary record 
of everything that was being dumped.  They used the standard LispWorks tools to 
see which functions were involved, and changed the code so that it did not keep 
a record of exactly what had been dumped.  (Keeping an "exact" record is 
important for Lisp itself).
 
They found another good saving with the "Generic Function Collapser".  
Initially, a Watson size of 5MB was seen as a problem.  They realised that most 
of the Generic Functions in Watson only had one method, and so could be 
re-written as an ordinary function.  This process was automated, saving some 
300K on the size of each image.

As Lisp manages its own memory, this aspect of their code always basically 
works: they do not bother looking at the garbage collector.  Their priority it 
rather to find the places where memory is allocated only to be garbage 
collected very soon afterwards.  This is seen as laziness in coding, and the 
goal is to prevent the allocation from occurring.  An example is the 
computationally intensive work to produce a layout.  This may use single 
floats, which in Lisp were boxed.  For every calculation it allocated a new 
floating point number object which was then thrown away after use.  They asked 
instead for some means of re-using the same bit of memory, and this massively 
reduced allocation during these calculations. 


3.0 Software Development Methods

None of the interviewees uses a formal development methodology. 

ScriptWorks

For larger projects ScriptWorks developers prepare specifications which are 
peer-reviewed, but frequently these are not up-dated as work proceeds.  The 
developers generally think hard and write some notes.  From such a 
specification they start to build the system, but normally avoid using 
third-party libraries due to uncertainties about legal constraints.

ScriptWorks has an approval process where work is classified as bug, task, or 
project.  After coding an approver then looks at the code to check for 
structure, functioning, side effects, and such quality items as the provision 
of asserts to check that assumptions are valid. 

In ScriptWorks they try to avoid allocating memory for large structures which 
might stay in the system for a long time.  They try to re-use parts of memory 
that they already have, and to avoid fragmentation.  If many small allocations 
are necessary, they will take a large block of memory and carve it up 
themselves.   Memory management is a constant concern, mainly in relation to 
the RIP which is the heaviest user.  Their main strategies were set long ago, 
but are being re-considered in relation to EP 2000.  They know that specific 
techniques suit specific tasks, but they are very performance constrained.  
They try to make sure that everything fits into the working sector so that they 
do not to rely on virtual memory.  For any new algorithms they must consider 
what happens if it runs out of memory.   Can they provide enough memory 
temporarily to finish the operation and then throw it away? How can they make 
space? Can the job be put out to disc?  The main concern is to see that the job 
will process and not stop.  Beyond that, they look for performance.  For any 
new code these issues should be checked by the approver, although that part of 
the process is probably not as strong as it should be.  The approver should 
check for fragmentation issues, and that the code does not run off the end of 
allocated arrays.

ScriptWorks generally only faces problems such as fragmentation in specific 
circumstances.  This means that they can be spotted without much difficulty.  A 
30MB job into a 40MB RIP may be very fast the first time, but  much slower the 
second time.  They have compile warnings turned right up, use many consistency 
checks, and write conservative C with lots of asserts.  The normal kinds of 
errors are thus much less frequent. 
Where they suspect memory management they faults will first try to identify the 
type of problem that is occurring.  If fragmentation is suspected, they will 
look at what the low-memory handler is doing.  They  look at the ratios of the 
sizes of allocations.  In the case of images, for example, they try to keep all 
the image data in memory if possible.  Before starting a job they calculate how 
many images there are, and whether they should all be able to fit into memory.  
If they should but don't, then there is  something preventing it, so they start 
looking at the other allocations which are occurring  These are things like 
half-tone caches, font caches etc.  If everything for a page that should fit in 
does not fit in, we have to look at how things are being compressed or moved 
around, and how the different allocations are interleaving to prevent it 
fitting.  

ScriptWorks developers generally rely on the knowledge of those who know the 
RIP very well.  Newcomers may miss some issues, and, for example, allocate more 
memory than is needed.  This will generally be caught in the "approval" process 
which should be carried out by an experienced RIP developer.  

Dylan

The Dylan compiler-writer sometimes uses formal techniques for requirements 
capture, and will almost certainly do so for design.  Where work is distributed 
geographically, a written document describing interfaces is produced.  For 
complex problems a prototype will be produced, but in simpler cases they go 
straight to implementation and debugging.  

Memory management issues are at the core of the Dylan compiler development, as 
their primary concern is to make the language work with the memory managers.  
This issue extends beyond the compiler-writer's own code, as they must also 
consider how the memory manager is going to work with other people's code in 
the run-time system that they are compiling for.  They need to think about the 
issues in both contexts. 

CAD/CAM 

The CAD/CAM developer described a sophisticated life-cycle focused on their 
single product.  This involved proposals, detailed requirements specifications, 
design proposals, review, and testing, although not all projects got the full 
treatment.  

In the case of the CAD/CAM system they considered memory management most 
frequently when they were adding new functionality, as this often added new 
fields to structures and thus increased the memory requirement.  Attempts were 
made to avoid allocations by keeping things static, even if this resulted in 
increased complexity.  Where one function caused serious fragmentation by 
repeatedly allocating and freeing two blocks of memory, they solved the problem 
by keeping and re-using the same blocks.  

Watson 

In Watson their main concern is to find any redundant translation of data from 
one form or representation to another and then back to a different structure.  
Once the same basic data is in memory, they aim to be able to use it without 
such translations.   They have taken about 100K off of Watson by this kind of 
streamlining, and the product goes much faster.   
The Watson team also have to consider memory management in all new 
developments.  A database could be as large as 20MB, so if they could remove 
one byte per row, they could save a great deal of space.  There are, for 
example, different entry paths to a database.  A record can be represented as a 
list, or much more efficiently, as a vector.  In a vector, access in a specific 
element is also more efficient.  They seek other savings by such strategies as 
having only a single vector to represent both an object and its corresponding 
record.  There is a continual trade-off between ease of access and performance, 
and they must also make sure that their solutions can scale up to deal with 
large numbers of objects each of which may have many slots.  They would very 
much like to be able to look at some of their data structures and say "How big 
is it?", although everything points to everything else in Lisp.  They cannot 
get that information at the moment.  


4.0 Tool Requirements and Developers" Views


4.1 Tool Requirements

ScriptWorks

An early piece of work by one of the ScriptWorks developers for the Macintosh 
OS showed graphically which areas memory was allocated to.  Coloured bars 
indicated how much was allocated to font cache, to half-tone cache, to display 
list etc.  There was a line showing red for display list memory, and green for 
general memory.  Another line was for images, which sit in display list memory, 
with colours showing whether they were in memory or disc.  The colours could be 
displayed as proportions of a dial, to indicate proportions of memory used.  It 
could help them detect thrashing between caches which we might not be detected 
by other tools, such as one giving a time-slice view, or one giving a structure 
display.  Their original tool was updated as the program executed.  It was 
supported by code inserts which produced diagnostic information.  They remarked 
that visual information is much easier to use.  Visual displays of structures 
etc. would make life much easier, as would the ability to trace where things go 
graphically rather than step through one level of pointers at a time in a 
debugger.  It is essential that these tools should be implemented across 
platforms, although they never had the time to do this.  

All memory allocations in the RIP are now tagged, so that each has a type.  The 
developers asked for a GUI where they could invoke an interface element to 
display allocations of a given type. They have actually used this method to 
diagnose problems in the past.  If you run such a system for a time before you 
a problem occurs, you can see what happens normally.  It is thus fairly easy to 
say what is going wrong in the case of the problem.  The system would show when 
a large image got put out to disc, or when a font cache was compressed and 
re-allocated.  It would show just where it was allocated.

They also wished to see the distribution of objects in memory: i.e. the number 
and the average size of allocation for that class of object, or, if possible, 
the distribution of every single allocation plotted in a neat graph.  This 
would be valuable in setting an initial size for jobs.  If they could do this 
more accurately, the jobs would run faster.

They would also appreciate help in dealing with fragmentation and the ability 
to re-locate objects.  Where, for example, a transfer function is used once for 
a single colour early in a job, the allocation is still locked in 
subsequently.  They hope that this might improve with the MPS system, and that 
they should eventually be able to sub-class memory pools to keep objects of 
similar type located close together.  Where they use large numbers of 
rectangles, the ability to group those with similar attributes could bring  
great reductions in size.

There are various levels of debug information that can be compiled in for 
ScriptWorks.  The simplest level is the allocation and de-allocation count.  
This gives the total amount of memory that has been allocated, or is currently 
allocated to each of the pools, along with the total numbers of allocations and 
de-allocations;  Next comes the fence-posting code which will add fence-posting 
to the allocations and check them on allocation or de-allocation operations.  
This checks for array overruns etc.  The final level is the tagging were every 
allocation is tagged with the line and file-number of where is happened.  The 
actual tag represents a separate memory pool controlled by the system rather 
than by the ScriptWorks code.  It should thus be in an entirely different area 
of memory and should not be affected by any problems which are influencing 
ScriptWorks' own allocations.

With these mechanisms they can get a "current state of memory" dump at a given 
point.  They can use this to see which function was making the allocation.  It 
would be much better to see that information structured by category, showing, 
for example, "The fonts allocations".  This would effectively give the 
developers the full structure of the font cache.  They do not have that level 
of sophistication yet.

They also use "asserts" which run over the tag pool and check something being 
"freed" has not been freed already, and that it is a valid piece of memory.   
If they hit a problem when the program is running, they can either ignore it, 
drop into the debugger, or abort at that point in order to examine the state.

If they could say that they only really care about part of what they have 
taken, then things would be better, but it is very hard to do this on NT.  

Dylan

The Dylan developer asked for more polished versions of tools he had used in 
the past, and in particular for allocation profilers.  These should indicate 
quickly which parts of a program were making allocations.  Their questions are 
"What memory am I referencing, and what is referencing it?"  The output could 
be a summative account, or a snapshot that changed as the program executed.  
The basic tools used in the past collect data from a run of the program and 
then display it in a sorted form.  The quality of the visualisation could be 
much improved in newer tools.

CAD/CAM

The CAD/CAM developer described a tool which showed memory allocations as a 
particular structure.  When these changed it checked them in the context of the 
client rather than that of the memory manager.  This was very useful for 
finding leaks etc.  The tool knew about the different objects that were used, 
and could check actions as they were executed.

The CAD/CAM developers did not have tools which showed memory usage.  There 
were no tools for finding out when a thing was allocated, or what the sequence 
of allocation was, or where it had been allocated from.  They did have 
statistics from the memory management module that showed how many nodes of each 
type were used, along with sophisticated tracing and a representation of the 
call tree.  A tool which was able to show memory usage graphically in a way 
which gave you the information you wanted would be very useful, but the 
interviewee believed it would be very difficult to specify such a tool.  

Watson 

The Watson developer asked for tools to help them find and manage redundant 
material at the code level.  In particular, they asked for "weak pointers", 
which would say to the garbage collector,  "if this is the only thing pointing 
to this object, throw the object away".  Watson gives them a list of all 
objects currently in the image; (they need this for checking purposes when 
loading objects).  If they get rid of all the charts that reference an object, 
then they can throw the object away.  However, the garbage collector cannot 
make that decision at present, as the object will still be referenced in the 
overall list. They want this list to be "weak".  They have asked for this for 
two years now, but assume that it cannot be easy to do, as it has not been 
provided.  They would also like to be able to pass information back to the 
garbage collector, to indicate when a set of allocations were ephemeral.  This 
is effectively a means of saying that "I"m going to create garbage now, but 
when the function exits it can
 all be thrown away". 

Another request was for an allocation profiler which would allow them to see 
which parts of the code were allocating most memory, and which should thus 
receive most attention.  They currently have a profiler which gives them a 
ranking of how long given functions were on the stack, but this has some 
drawbacks.  One of these is that top-level functions are permanently on the 
stack.  What they would prefer is the ability to group functions, and to obtain 
feedback on the allocations performed by a given group.  Both size and 
frequency are relevant here, as there is a fixed overhead for allocation, but 
large allocations can trigger garbage collection.  

Watson faces a continual trade-off between ease of access and performance, and 
the developers must also make sure that their solutions can scale up to deal 
with large numbers of objects each of which may have many slots.  When trying 
to eliminate redundant representations they would very much like to be able to 
look at a given data structure and say "How big is it?", although everything 
points to everything else in Lisp.  They cannot get that information at the 
moment. 


4.2 Developers" Views on Memory Management Products. 

ScriptWorks

Different answers were given with regard to tuneability and initial settings.  
One developer maintained that a tuneable product was essential, as customers 
use the RIP in many different ways.  One customer may have a typical job where 
image handling issue are most important, while another may be most concerned 
with Quark or Illustrator jobs, or text handling.  They need to be able to tune 
such things as cache sizes when they configure ScriptWorks for specific types 
of job.  Tuneability would also be useful for "problem" jobs, if it allowed you 
to use an unusual configuration to just squeeze the job through, and then 
re-set to normal.  

A different view on initial settings was described by the other developer.  He 
wished to be able to control the impact of allocations on the machine"s 
resources.  An OEM might say "I want to give the RIP 40MB, but only work with 
20MB so that I can run a co-operative application.  If necessary, I will allow 
the RIP to go up to 40MB to avoid a partial paint".  This would allow them to 
determine not just how ScriptWorks behaves, but how other software such as NT 
or other applications work with it.  This happens through the actions taken by 
the ScriptWorks low-memory handler.  If necessary, it will do a partial paint, 
which can have a high overhead.  If it means compressing 1.5 GB of memory with 
reads and writes it is a huge overhead.  It might be better to extend the 
memory chunk a little to avoid doing a partial paint, even if the machine 
thrashes a bit.  The problem is that there is no way of knowing when the 
process will reach the end of a page -- it might only need a little more memory.

They definitely need tools which give them statistical instrumentation from the 
memory manager, as they often have to guess about such things as cache size and 
expected usage patterns.  They would dearly like to get the information without 
having to do all the instrumentation themselves.  This applies to sizes and 
frequency of allocation, patterns of usage, -- everything.  Anything which 
helps to get a handle on these things would be excellent, as long as it did not 
hit performance in the eventual product.  They could work with a "debug" 
version that was slower, but API identical. 

They would appreciate a range of memory management policies, but would need to 
know how they interacted with the different parts of the product.  Like the 
Watson developers, they have some allocations that they know will be 
long-lasting or static, and others that are definitely temporary.  For display 
lists they will do multiple allocations at once: the data is used "as one" and 
thrown away "as one".  They could easily pass this data to an allocator or 
garbage collector.  

A range of specific services was requested:

  - de-fragmentation, through knowledge of different types of allocation; this 
could involve a request for "help" from the allocator, as when fixing up a 
pointer;

  - provision of help in trade-offs between speed and size;

  - debugging facilities such as fence-posting;

  - provision of information on allocation behaviours;

  - control of memory management in relation to the underlying operating 
system; 

  - fast in-line allocation for special needs as when rendering a page.

An example of the last bullet point above may be the need for large numbers of 
8K objects.  These would normally be cached, but if the cache is exhausted, 
then there can be a dramatic impact on the RIP's performance.  The ability to 
re-use the cache memory for rendering could make a huge difference to 
performance and could help them to run in a smaller memory system.  This is 
starting to matter as they are now working for users with much smaller 
machines, and they need to dynamically use memory in an efficient way.

The provision of feedback on how memory management is affecting performance is 
being developed for the EP 2000 project, possibly in conjunction with the use 
of "intelligent agents".  "Managed" or "interpreted" data would be very 
useful.  They would probably not use the tools on a daily basis, but rather 
would use them to tackle such problems as a customer who has cache thrashing.  
The ability to do a little instrumentation and get the data out would be most 
useful.  Applying the instrumentation should thus be easy, and take little 
effort. 

They have no reservations about using memory management products as long as 
they work, and are cross-platform.  Obviously, the tools should not impair 
performance in the eventual product.  They could work with a "debug" version 
that was slower, but API identical. 

An alternative view was that they might not use the GUI that came with the 
tools, but that they would be happy to let a different team take care of memory 
management issues.  It would be good to have this relationship on a formal 
footing, as they need to stay ahead as the "fastest RIP", especially in the 
problem cases.

There was a serious concern about the extent to which memory management 
controls might be made available to users.  The controls to tune initial 
settings described above should probably go into an "advanced user options" 
box.  They would not be hidden, but it should not be possible for all users to 
play with them.  It might be useful to give the users some means to provide 
input, but only under strong controls, or through a subsidiary interface.  They 
would not want users to have direct access to full debugging and contents of 
memory information.  

Another possibility is to install some monitoring of how the users generally 
work with the system.  One OEM does this already.  This could show whether 
adding some more memory was cost-effective, or whether the RIP could be 
re-configured to make better use of the resources.  These issues are really 
more concerned with the RIP than with memory management.  They currently have 
some controls which influence the interaction between the machine and 
ScriptWorks rather than governing internal functioning.  These controls allow 
you to say "get every bit of memory you can, but leave a reserve", or "use this 
amount of memory", or "I know best: use 60MB even on a 40MB machine".  It is 
necessary to make allowance for other applications that may be on the machine, 
and these settings have a strong influence on the performance of the RIP.

They also have to remember that users can be less than skilled or smart, and 
have caused much trouble in the past by changing things arbitrarily.  
ScriptWorks' name is built on performance, and they cannot risk losing it.  
They don't publish all tuneable parameters as it is. 

Dylan

The Dylan compiler group have not done much performance testing of their memory 
management work, as it is very hard to measure the performance of a memory 
manager.  They are hoping for some help from the tools that we intend to 
provide, and would expect to do a lot of metering at a later date.  At the 
moment their implementation is in its infancy.  They do, however, know what 
information they would like to see.

They would want to know:

  - how effective the memory management has been; i.e. what the heap looks 
like, in terms of a breakdown of different objects;

  - how long-lived the objects are;

  - how much is being allocated;

  - how often the objects reference data in the heap that is a from a 
conservatively scanned reference rather than from a hard reference;

  - how much time is spent in garbage collection;

  - how much time is spent in the different phases of garbage collection: 
conservative stack scan, tracing, processing normal objects, copying; 

  - a great deal of information about efficiency.

An implementation detail about the interface to the memory manager that the 
Dylan group are using is that they have read and write barriers which are 
triggered by page faults inside the hardware.  They don't now how often these 
will occur, or even just how expensive each one of them will be.  They will 
need to measure this very carefully.  They want to know how well the system 
will behave with very little memory, and also with lots of memory.  They will 
need to do lots of metering, and will want memory management tools which give 
them a good handle on this.

With regard to a tuneable product, it did what they wanted straight out of the 
box they would feel no need to tweak it.  It is inevitable that they will have 
specific needs for different memory managers.  Obvious examples are the need 
for "real-time" memory management for applications, or for Multimedia 
applications, or for applications with time constraints of various sorts.  This 
will certainly require some special tuning of the garbage collector, if not a 
dedicated garbage collector for each application.  At the other end of the 
spectrum there may be programs which are much less interactive and where the 
cost of supporting an incremental garbage collector cannot be justified.  The 
quest here would be for overall efficiency, rather than for good interactive 
pause times.  Where efficiency is an important consideration, whether overall 
efficiency, or garbage collector efficiency, then the ability to tune or select 
the memory management will be important.

They are quite open to the idea that parts of Dylan may have to be implemented 
in different ways in order to fit in with different memory managers.  Speaking 
as an application programmer, the interviewee stated that if he needed better 
performance, then he would certainly be prepared to modify his program in order 
to get it.  He was, in fact, sceptical about the possibility of avoiding this.

They would value precise feedback on how their program is working with the 
memory manager in order to identify sections where changes were required.   
Small sections of code can have a profound effect on overall performance, and 
they need to isolate these.  An example would be a hard real-time loop, where 
one or two ways through it break your time constraints.  It is only those one 
or two that they are interested in. 

As Dylan is a development environment they would certainly want application 
developers to have access to the memory management tools.  There are two 
different levels of interest in these tools.  One is the language implementor 
who is trying to make his language work with the memory manager; the other is 
the application developer who wants to use the combined system.  Both have an 
interest in tools which show them what is going on.  The language implementor 
probably wants more, and will probably respond to it in different ways, but the 
application developer, if he has a program that is at all challenging or 
complex, will want to get at that information. 

As well as the feedback from instrumentation, they might also want to make the 
memory management parameters available to users in some sanitised fashion.  
They would like to make some form of bolt-on memory management modules 
available in Dylan, with different ones tuned for different purposes.  The 
standard garbage collector might be replaced with the "real-time" garbage 
collector, or the "null" garbage collector where you have a simple program that 
does not need the overhead of automated management. 
CAD/CAM

The developer's previous employer had a strong "NIH" ("Not Invented Here") 
culture, and used very few outside tools.  Part of this was due to the need for 
flexibility and built-in features, and the need to rapidly fix any bugs in 
their code.  They feared that getting a third-party tool fixed could take a 
long time.  

Watson

They work more at the code level, and are more interested in indicating what is 
going to be garbage and when it should be cleared.  They could also be 
interested in passing other data to a garbage collector, e.g. when a database 
is loaded.  They know that the database is going to be in memory for a long 
time, with nothing deleted, until a new database is loaded.  At this point the 
whole of the old one is thrown away.  They could thus pass useful information 
to the garbage collector about what is being allocated, and so produce 
efficiency gains.  They could indicate whether a data structure should be 
scanned for garbage, or whether it is to be collected en mass.  

They also have a problem with locality.  If Watson grows larger than real 
memory it starts using swap.  This should be OK, as the user's focus is usually 
on one topic, and other material can be swapped out so that they have a working 
set that is wholly memory-resident.  Instead, the locality is so bad that the 
system thrashes and dies.  They need something to help with this, and to show 
the effects that such help has had.  The basic unit of work in Watson is the 
dataset.  If this was allocated contiguously, then perhaps it could all be 
swapped out as a chunk, rather than as distributed bits.

The Watson developers have other problems related to delivery, because in Lisp 
everything is included and they have to say "throw out anything that is not 
referenced by anything else".  They believe it is unlikely that the memory 
management tools can help with this.   


4.3 Summary Table of Tools and Facilities Requested


Tool Requested	ScriptWorks	Dylan 	CAD/CAM	Watson
Coloured Graphical representation of memory allocations by type, as a 
proportion of memory used.	X			
Visual display of structures	X			
Graphical tracer for pointers 
(cf. debugger)	X			
GUI option to display allocations of a given type or types, including location, 
and/or structure of caches	X	X	X	
Display of distribution of objects in memory: number and average size of 
allocations for a given class.	X	X	X	X
Graph of all allocations	X	X		
Allocation and de-allocation count	X			
Display of allocations by file and line number, and/or graph of call tree	X		X	
Statistical feedback on sizes and frequency o f allocation, cache sizes; also 
on patterns of usage	X	X		
Feedback on how long-lived objects are		X		X
Allocation from independent memory pools	X			
De-fragmentation tool: remove redundant allocations; re-locate objects	X			
Anti-fragmentation tool: group objects with similar attributes	X			X
Fast in-line allocation for special needs	X			
Allocation Profiler to show which parts of memory are being referenced 
(Summative and/or Continuous)		X		X
Allocation Profiler to show  what is referencing a given part of memory.  
(Summative and/or Continuous, possibly with grouping of functions). 		X		X
Tool to check memory allocations in the context of the client			X	



Summary Table of Tools and Facilities Requested (continued)
Tool Requested	ScriptWorks	Dylan 	CAD/CAM	Watson
Weak pointers to pass information to the garbage collector				X
Means to label ephemeral vs. long-lasting allocations to pass information to 
the garbage collector	X			X
Tuneable initial allocations, e.g. cache sizes, for configuration.	X			
Extensible initial allocations, to cater for difficulties during execution	X			
Range of memory management policies to suit different jobs	X	X		
Tuneable  memory management policies		X		
Provide help in speed/size tradeoffs	X			
Debugging facilities e.g. fence-posting	X			
Control of memory management in relation to underlying operating system; 
(dynamic and efficient memory use).	X			
Feedback on patterns of usage of RIP	X			
Feedback on how often applications reference data on the heap from a 
conservatively scanned rather than a hard reference		X		
Time spent in garbage collection, and different phases of garbage collection; 
(conservative stack scan; tracing; copying)		X		
Feedback - lots of it - on efficiency		X		
Tools to meter performance when lots of / very little memory available		X		
Feedback on read/write barriers and page faults; (frequency and cost).		X		
Tools to measure time-related performance of specific functions		X		
User access to memory management controls	X	X		
Tool to remove un-referenced material for application delivery				X



5.0 Task Outlines

The interviews yielded a significant amount of information on the basic tasks 
undertaken by developers grappling with size and performance issues in relation 
to memory management.  The relevant tasks are summarised in this section.  

In order to design and implement a GUI for the memory management products which 
has good usability characteristics, it will be necessary to study a 
representative selection of these tasks in detail.  

NB:

  - Alternative approaches to a given task are indicated by numbers.

  - The order in which sub-tasks are listed here does not imply a strong 
ordering in their execution. 

 - Sub-tasks are indicated by indentation.


5.1 ScriptWorks Tasks

Task: Locating Memory Leaks:

- Count allocations and De-allocations;

- Determine amount currently allocated to each pool. 

Task: Locate Memory Errors:

1) Identify the type of problem that is occurring. 

        - If Fragmentation is suspected, examine the behaviour of the 
        low-memory handler.  

        - Look at the ratios of the sizes of allocations;

        - Determining whether the working set should fit in available memory;

        - If the working set should fit in available memory but does not do so, 

        Look for the cause;

            - examine other allocations that are occurring, such as half-tone 
caches or font caches. 
            - examine how data is being compressed and/or moved around. 
            - examine how different allocations are interleaving to produce the 
problem. 

2) Do fence-posting at the end of allocations;

3) Tag all allocations and use a separate memory pool. 

        - Trace which function made a specific allocation;

        - Trace what the specific allocation is used for. 

Task: Diagnose performance problems: 

1) Compare the use of memory by different modules and caches during execution;

2) Examine debug information on structures used by the program;

3) Carry out tracing using a debugger. 

4) Use allocation tagging information to examine all allocations of a given 
type. 

5) Examine average size, distribution, and number of allocations for a given 
object type. 

6) Dump and examine current state of memory at a given point in the execution:

7) Run program until problem occurs, then either: 

        - Enter debugger and continue;

        - Abort program and examine state. 

Task: Resolve problems with host Operating System:

1) Use asynchronous read-ahead;

2) Lock down as much memory as possible. 

Task: Start and progress a new project:

- Classify work as bug, task, or project. 

- Discuss and analyse requirements. 

- (Produce a written specification)

- Complete coding and check into source control system. 

- Approver checks code:

        Check structure:

        Check functioning;

        Check side effects:

        Check memory management;

        Check for array overruns;

        Check use of asserts and overall quality 

Task: Produce new code while avoiding fragmentation:

1) Re-use available memory rather than allocating new segments. 

2)  For multiple small allocations:

        Allocate a single large block of memory; 

        Write code to use small parts of this as required. 

3) Use a conservative style of C with many asserts;

4) Compile with warnings turned up high.  

Task: Provide initial configuration for RIP:

- Study user’s pattern of use;

- Determine necessary cache sizes;

- Determine necessary overall RIP size;

- Determine requirements of OS and co-operating applications. 

- Consider relevant speed and size trade-offs.


5.2 Dylan Tasks

Task: Improve performance of compiler:

- Run the program and collect relevant data;

- Examine data to determine what allocations are being retained unnecessarily:

- Examine data to determine what is referencing the retained memory;

- Examine how effective the memory management has been; i.e. what the heap 
looks like, in terms of a breakdown of different objects;

- Examine how long-lived the allocated objects are;

- Examine how much is being allocated;

- Examine how often objects reference data in the heap that is a from a 
conservatively scanned reference rather than from a hard reference;

- Examine how much time is spent in garbage collection;

- Examine how much time is spent in the different phases of garbage collection: 

        - conservative stack scan;

        - tracing;

        - processing normal objects:

        - copying ...... 

- Examine all available data regarding efficiency.  

- Find a means to code around the problem.  

        - If necessary, destructively modify some data structures to return 
them to a sane value. 

Task: Determine why an application allocates large amounts of memory. 

1) Stare hard at the code;

2) Use a function profiler;

3) Use an allocation profiler

        - Run the program to collect relevant data;

        - Determine size of allocations (from code);

        - Determine frequency of allocations;

            - Revise code to produce a more efficient control flow; 

        - Determine whether allocations are made by inefficient system code;

            - Run (and re-run) the code with a profiler to collect data;

            - Examine the profiler output to identify the problem;

            - Code around the problem. 

4) Examine the structures being uses, e.g. vectors as opposed to lists. 

Task: Improve the performance of a generational garbage collector:

- Run the program and collect relevant data;

- Identify temporary allocations used during the life of the relevant process:

- Produce a strategy for distributing the allocated objects among the garbage 
collector’s generations

- Apply relevant heuristics to the memory management model.  

Task:  Start and progress a new project:

- Consider how Dylan will work with the memory manager;

        - Consider how the compiler-writer’s code will work with the memory 
manager;

        - Consider how other developers’ code will work with the memory manager 
in the run-time system they are compiling for;

- Think hard and produce a statement of requirements (possibly formal);

- Use formal techniques to produce a design;

- If the work is to be distributed, produce a document defining interfaces;

- If the project is complex, implement a prototype:

- If the project is not complex, begin implementation and debugging. 


5.3 CAD/CAM Tasks

Task: Gain efficiency by doing multiple "frees" at once (using a "watermark"):

- Set temporary mark and get identifier for it;

- Continue processing to next API call;

- Free all allocations back to temporary mark.  

Task: Add new functionality to system:

- For each object total number and size of new fields to be added;

- Use this total to estimate increased memory requirement; (10% or less is OK);

- Where possible use static rather than dynamic allocations, even at the cost 
of increased complexity;

- Avoid fragmentation by re-using existing allocations. 


5.4 Watson Tasks

Task: Gain efficiency by avoiding allocations:

- Inspect code to detect allocations that are garbage collected soon 
afterwards; 

- Provide a means to re-use existing allocations;

- Revise code to re-use the existing allocations.  

Task: Streamline data representations:

- Inspect code and running system to detect points where the same data is 
translated between different forms of representation (e.g. vector vs. list;

- Determine which single representation is appropriate for the data;

- Revise the code to utilise the single form of representation;

Task: Minimise size of data representations:

- For a given piece of data, determine whether a more efficient form of 
representation is available;

- If the same data is represented in two forms for two purposes, determine 
whether a single representation may be used;

- Where a specific solution is adopted:

        - determine whether it will scale up to cope with large datasets;

        - consider the trade-offs between ease of access and performance;

- Revise the code to utilise the new single representation.  

Task: Determine which parts of the code are allocating most memory:

- Run Watson with a profiler to gather data;

- Inspect data to determine how long given functions were on the stack;

- Allow for the fact that the top-level functions are always on the stack. 

- Estimate size of given data structures;

- Estimate size and frequency of allocations made by each function.


6.0 CONCLUSIONS


6.1 General Conclusions

The interviews and subsequent analysis have revealed three distinct approaches 
to the problems of application size and performance.  Each approach corresponds 
to a company product area, and has its own set of priorities with associated 
methods and tasks.  Each will thus have its own set of usability criteria.  The 
description of each approach indicates the information and facilities that a 
developer from the relevant group would wish to find in the user interface to 
the tools that they are offered.  There is some common ground between the 
groups, as in the requirement for allocation profilers, but the more detailed 
concerns of each group differ considerably as their tasks and methods differ.  
The ScriptWorks group are most interested in diagnosing problems of 
fragmentation and interactions with the operating system.  The Dylan group are 
most concerned with reducing the quantities of data held by a compiler, and 
with monitoring garbage collection.  The Watson group would very much like to 
know the size of specific data structures.  All three groups have a strong 
interest in passing useful information back to the memory allocator and in 
seeing how this affects performance.

The tasks as they are undertaken now are summarised in section 5.  The 
interviews were also successful in going beyond this to provide additional 
information about how the developers would like to work if they were given the 
tools they asked for.  This is important in designing for usability, as the new 
tools may offer users facilities that they do not currently enjoy.  Their tasks 
and methods may thus show corresponding changes when the new tools are 
introduced.  A summary table of the tools requested is given in section 4.3.  
This table identifies areas where tool requests overlap, and may help to 
identify the areas of most general usability, where usability is interpreted as 
fitness for purpose.  

Some caveats should be borne in mind:

  - The interviewees were all Harlequin employees.  Developers in other 
contexts carrying out other tasks may have different usability requirements. 

  - The tasks outlined in section 5 give a broad indication of what the 
developers do.  A representative selection of these tasks need to be expanded 
in further work in order to serve as the basis for user interface design.  

 - The number if interviewees is small.  This does not invalidate the 
information that they give us, but this information should not be regarded as 
complete.


6.2 Further Work.

The memory management group needs to consider the contents of this report in 
order to decide which groups and which tasks they wish to focus on.  These 
tasks can then be explored in more detail, and candidate designs for the user 
interfaces to the relevant tools can be developed.  This work should specify a 
structure of human-machine dialogs that will support each task, along with 
usability goals for each dialog.  Scenarios should then be produced for each 
dialog.  A scenario gives a graphical representation of a candidate design for 
a dialog, in terms of user interface widgets and menu structures.  These should 
be subjected to usability testing either in paper or prototype form with 
potential users as early as possible.

ATTACHMENT
   "TA Deliverable 1"

