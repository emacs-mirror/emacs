          GOALS AND REQUIREMENTS FOR A JAVA BENCHMARKS PRODUCT
                        analysis.java-benchmark
                               draft doc
                             rit 1998-06-16

INTRODUCTION

.intro: This document states goals and requirements for a product consisting of 
a set of benchmarks for memory-management in Java.

.structure: Goals and requirements themselves are listed first, followed by a 
section of ommissions and issues. Lastly there is a note on naming. [Perhaps 
this should really be in a separate document.]

.source: Except where otherwise indicated, the material in this document comes 
from a meeting with gavinm, ptw, lth and rit on 1998-06-16. Unfortunately there 
are no minutes available. "in review" means the material comes from 
review.analysis.java-benchmark.1(0).


GOALS

.stakeholders: Stakeholders include: JVM clients in HQN, JVM clients worldwide, 
MPS developers, JVM developers worldwide. .source.stakeholders: gavinm, in 
review.

.goal.inform: The benchmarks will provide the computing community with a way to 
obtain useful information on memory management in Java implementations. [But 
useful in what way and to whom?]

.goal.characterise: The benchmarks will be able to be used to characterise Java 
memory managers.

.goal.mps: The benchmarks will be useful for measuring MPS performance and 
hence for improving it.

.goal.image: Harlequin's image will be enhanced.

.goal.use: People will actually use the benchmarks.

.goal.endure: The benchmarks will be useful for some time -- say 10 years.
.source.goal.endure: verbally from gavinm on 1998-06-18.


REQUIREMENTS

.req.public: The benchmarks should be publically available. (.goal.inform, 
.goal.image, .goal.use).

.req.public.src: The benchmarks should be publically available in source form.
.source.req.public.src: lth, in review.

.req.impartial: The benchmarks should be impartial, and they should be 
observably so. (Specifically, they should not be designed so that the mps does 
particularly well on them.) (This implies that it should be made hard for Java 
systems to "cheat" by recognising benchmark programs.) (.goal.inform, 
.goal.characterise, .goal.image, .goal.use)

.req.portable: The benchmarks should be portable across platforms and Java 
implementations, and should run both as Java Applets and stand-alone programs. 
(.goal.use, .goal.endure)

.req.comparision: The benchmarks should allow meaningful comparisons to be made 
between the memory management abilities of different JVMs (Java Virtual 
Machines), independent of the compiler. This includes comparisons across 
platform (these may require more human interpretation). (.goal.inform, 
.goal.characterise, .goal.use)
.source.req.comparison: verbally from gavinm on 1998-06-18.

.req.reproducible: Results obtained from running the benchmarks should be 
reliably reproducible. (.goal.inform, .goal.characterise, .goal.image, 
.goal.use)

.req.scaleable: The benchmarks should scale up (and down) to systems of varying 
sizes. [In particular, to future systems which may have much more memory 
available to them.] The scaling need not necessarily be automatic. 
(.goal.inform, .goal.use, .goal.endure)
.source.req.scaleable: discussion between gavinm and lth on 1998-06-17.

.req.java-version: Initially, the benchmarks should be written according to a 
version of the Java specification commonly implemented at the time. Eventually 
they should be extended to cover newer versions of Java. (.goal.inform, 
.goal.image, .goal.use, .goal.endure)
.source.req.java-version: mail.rit.1998-07-16.13-37(0) and previous messages in 
the same thread.

.req.early-bird: Harlequin's Java benchmarks should be among the first Java 
memory management benchmarks to be released. (.goal.image, .goal.use)

.req.maintained: The benchmarks must be maintained, keeping track of changes in 
Java implementations and usage. [They need not necessarily be maintained by 
Harlequin, though they might be.] (.goal.inform, .goal.image, .goal.use, 
.goal.endure)

.req.coverage: The benchmarks should cover all JVM functionality that is 
relevant to memory management. (.goal.inform, .goal.characterise, .goal.mps)

.req.reality-check: The benchmarks should, at least to some extent, reflect 
typical or likely behaviour of real-world Java programs. (.goal.inform, 
.goal.characterise, .goal.mps, .goal.image, .goal.use, .goal.endure)

.req.easy: They should be easy to use. (.goal.image, .goal.use). [How easy? 
Should we specify a minimum target user? -- gavinm, in review.]

.req.mps-compatible: It should be possible to use the benchmarks to measure the 
performance of the MPS. [See .issue.req.mps-compatible below] (.goal.mps, 
.goal.image)

.req.measure: The benchmarks should measure or test each of the following 
(goal.inform, .goal.characterise)
.req.measure.time: time (or speed) of MM operations,
.req.measure.space: space (i.e. memory usage),
.req.measure.pauses: length and distribution of memory-management-related 
pauses,
.req.measure.leakage: size of memory leaks (if any),
.req.measure.correctness: that the MM doesn't throw away live objects or 
otherwise behave in a functionally incorrect way,
.req.measure.limits: what limits (e.g. space limits) the JVM imposes,
.req.measure.reliability: the reliability of the JVM's memory manager (e.g. as 
mean-time-before-failure).
.req.measure.functionality: What memory management functionality does the JVM 
support? E.g. incremental, generational, getting memory from and returning 
memory to the OS. .source.req.measure.functionality: ptw, in review.

.sugg.req.measure: Suggestions for additions to .req.measure are:
.sugg.req.measure.finalization: promptness of finalization, 
.source.req.measure.finalization: drj, in review.

.sugg.req.measure.weakness: how quickly do objects that are weakly-referred-to 
die? .source.sugg.req.measure.weakness: drj, in review.

.sugg.req.measure.multi-processor: Is the GC multithreaded? How does 
performance change as more processors are added? 
.source.sugg.req.measure.multi-processor: drj, in review.

.sugg.req.measure.native: E.g. how GC performance is affected by the presence 
of non-Java stack frames. .source.sugg.req.measure.native: drj, in review.


GOAL-JUSTIFICATION-COVERAGE, AKA REQUIREMENT IMPACT ANALYSIS

How many requirements does each goal justify?
.justification-score.goal.inform: .goal.inform scores 9
.justification-score.goal.characterise: .goal.characterise scores 6
.justification-score.goal.mps: .goal.mps scores 3
.justification-score.goal.image: .goal.image scores 8
.justification-score.goal.use: .goal.use scores 9
.justification-score.goal.endure: .goal.endure scores 4

.justification-score.comments: .goal.characterise would do much better if we 
had scored each of .req.measure.* separately. .goal.mps scores very low, 
perhaps reflecting the fact that to achieve it requires work on integrating the 
MPS into a JVM in addition to the set of benchmarks.


OMISSIONS AND ISSUES

.omission.goals: How do the goals serve goal.general? Is there a goal about 
increasing revenue for some saleable product?

.omission.product.java: Should refer to documents about the MM Java product. 
What such docs are relevant?

.omission.identify: Who are the "parties involved" in driving the goals?

.omission.measure: We need ways to measure whether or to what extent each 
requirement has been met. [See rule.req.scale, rule.req.test.] For some of the 
above requirements, this may be very difficult.

.issue.req.mps-compatible: .req.mps-compatible is not really a requirement on 
the benchmarks, but a requirement that we can plug the MPS into a Java virtual 
machine. As such it doesn't belong here, but I don't know where else it should 
go. [If it _were_ a requirement on the benchmarks, then it might also 
contradict .req.impartial.]


NAME

.name: We thought off a bit about what to call the benchmarks. Suggestions: 
java-benchmark, bean-counter, coffee-stain (by analogy with caffeine-mark). The 
ideal name would presumably involve a bad pun and an allusion to desirable 
properties of coffee.


