                           ANALYSIS OF MM/ASG
                              analysis.asg
                               draft doc
                           richard 1997-02-06

INTRODUCTION

.intro: This document analyses the goals, requirements, and solution areas for 
collaboration between the Memory Management Group (MM) and the Adaptive Systems 
Group (ASG).


DEFINITIONS

.def.mps: The "MPS" is the Harlequin Memory Pool System -- the flexible memory 
management framework developed by the Memory Management Group.

.def.adaptive: A system is "adaptive" if it is able to use information about a 
situation to change its behaviour in order to meet requirements that would 
otherwise require modification to that system.


GOALS

.goal.adapt: To make the MPS adaptive (.def.adaptive).  It will be a selling 
point of the MPS that it is adaptive.  It will be a selling point of products 
which include the MPS that they contain an adaptive memory manager.

.goal.req: To ensure the MPS can meet its performance requirements.


REQUIREMENTS

.req.adaptive: The MPS must include a component which modifies behaviour based 
on gathered data.  (See .goal.adapt.)

.req.dylan: The adaptive component must control internal parameters of the 
Dylan Memory Manager (based on the MPS) such that it remains within its 
performance requirements.  More details see req.dylan.attr.time.* (timing 
requirements) and req.dylan.attr.space.* (space requirements).

.req.epcore: Ditto the EP Core RIP Memory Manager.  For more details see 
req.epcore.attr.run-time.* (run-time requirements), req.epcore.attr.tp.* 
(throughput requirements), and req.epcore.attr.footprint (size requirement).

.req.product: Ditto the MM Product.  These requirements have yet to be defined.


PRIORITIES

.prio.1: .req.dylan
.prio.2: .req.adaptive
.prio.3: .req.epcore
.prio.4: .req.product


POSSIBLE SOLUTION AREAS


Predicting Mortality

.mort.background: Objects can be recycled once they become unreachable from the 
program state.  In some programs, there is a hyper-exponential relationship 
between the time since the object was allocated and the probability that it has 
"died" (become unreachable).  There are other such relationships in other 
programs.

.mort.problem: The problem is to predict death.  In particular, for a given set 
of objects, what proportion are dead?

.mort.features: We already know a set of potentially useful features (age, 
type, size, stack depth, stack contents, allocating function, etc.).  The 
problem is how to combine them to make useful predictions without much overhead.

.mort.sched: The mortality prediction is a vital input to GC Scheduling 
(below).  If no other predictor is developed, an empirical age-based function 
can be used.


Scheduling Garbage Collection

.sched.background: Objects are organized into sets.  At any one time there are 
a number of sets which can be garbage collected.

.sched.problem: The problem is to decide whether to start a collection of a 
set.  (The arrangement into sets might also be controlled.  See .sets below.)

.sched.costs: The cost of collecting a set is derived from:

  - a fixed overhead
  - the size of the "foundation" (objects which must be scanned in order to
    collect the set)
  - the size of the objects in the set which will survive (this is a prediction)
  - the amount of temporary space needed during collection

.sched.benefit: The benefit comes from the amount of space recycled (the size 
of the set minus the objects which survived).

.sched.overview: The scheduling algorithm should weigh these up and decide 
whether to take action.  


Optimizing Collectable Sets

.sets.background: Objects are arranged into collectable sets when they are 
allocated and when they survive a garbage collection.  They could potentially 
be rearranged at any point, but at quite a high cost.

.sets.problem: The problem is to arrange them into sets which will all die at 
the same time, and to predict that time, so that a cheap garbage collection can 
recycle them all at low cost.

.sets.cheap: A very cheap decision needs to be made at object allocation time 
or when the object is moved by a garbage collection.  Compiling the input to 
this decision could be made more slowly over a longer period.


Specifying Utility

.util.background: The client program (the one using the MPS) needs to specify 
its requirements in some way.  Some of the things it will want to specify are:

  - space overheads
  - desired maximum heap usage
  - time overheads
  - pause times

The client program _shouldn't_ have to specify values for internal controlling 
parameters of the MPS.  The scheduler should use predictions to continuously 
optimize the MPS parameters in order to meet the client's requirements.

.util.problem: The problem here is how to specify these requirements so that 
they can be used for optimization (or rather "satisfization", as Louise put it).


Incremental Controls

.incr.background: The GC can run "incrementally", doing small amounts of work 
at once in order to avoid interrupting the client program for long periods.  
This is how we meet the pause time requirements.  However, there is an overhead 
in starting and stopping work.  (There is another problem, see .scan below.)  
There is also the problem of making sure you finish all the work on time, where 
"on time" is decided by the scheduler (see above).

.incr.problem: How should the rate of work be controlled?  Is there dynamic 
decision making to be done?  On what basis?


Deciding what to Scan

.scan.background: When the GC is operating incrementally, memory protection is 
used so that the client program can't see unprocessed objects.  When the client 
tries to access the object a protection exception occurs and the GC gets to do 
some processing first.  The result is that the client only sees the finished 
result.  There are two problems:

  1. protection exceptions are expensive in CPU time,
  2. if the client access lots of protected objects one after the other then it 
won't make any progress, and will effectively be paused.

.scan.problem: The problem is to minimize the number of page faults by choosing 
which objects to process intelligently, anticipating what the client program's 
pattern of access is likely to be.


Deciding when to "Flip"

.flip.background: There are two main incremental GC techniques.  One uses a 
"read barrier" and the other a "write barrier".  The read barrier technique 
takes a step known as a "flip" at the beginning of the collection.  The write 
barrier takes this step at the end.  The step is quite expensive.  The MPS has 
a generalized algorithm which allows a flip in the middle, chaning from write 
barrier to read barrier mode.  These two modes have different characteristics, 
and impact the client program in different ways.

.flip.problem: The problem is deciding when the flip should occur.

