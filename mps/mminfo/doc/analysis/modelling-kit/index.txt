                  MODELLING KIT FOR MEMORY MANAGEMENT
                         analysis.modelling-kit
                             incomplete doc
                            pekka 1998-04-01

INTRODUCTION

.intro: This is a modelling kit for memory management performance 
optimisation.  It contains informal definitions of quantities of interest and 
equations modelling relationships between those quantities, under some stated 
assumptions.

.author: Everybody will add to this document, so while it has a creator and an 
editor, it won't have authors as such.

.no-model: The quantities and relationships do not form a single model, they 
are a constantly evolving collection of useful pieces for building models.  We 
expect developers to use these to design MM strategies, analyse performance 
problems, and discuss the theory of MM.

.strategy: _Strategies_ is the term for proposed designs for making some 
decisions within the MPS (but often in cooperation with the client).  They will 
state the requirements they seek to address, select some models out of this kit 
whose assumptions match the requirements, fill in the missing information from 
user parameters, estimates or other sources, and derive the decision algorithms
therefrom.

.readership: AMM developers, ASG developers.


Document History

.hist: First draft written by Pekka P. Pirinen 1998-03-05.  Paging 
considerations contributed by Tony.  This document will not have any 
versioning, since it's an open-ended project for extending a collection of 
useful bits and pieces of models.


Limitations

.ideal: Models are simplifications, and one could always build a more detailed 
model.  Therefore, we simply number the equations we present and state the 
assumptions it depends on, instead of trying to decide which is the "best" 
model.

.segments: In many cases, the actual values used in the MPS are calculated by 
summing over the relevant segments, but we have not tried to indicate that here.


NOTATION


Prefixes

t   time elapsed (either CPU or wall-clock)
tc  CPU time
tw  wall-clock time
b   start (beginning) time
e   end time
s   size (memory)
n   number (of objects, usually)
pg  number of pages
d   momentary change (cf. differential)
c   cost of work (time expended per memory processed)


Names

.abbrev: In documents, real words are used to name the different quantities.  
In conversation (whiteboards), it is allowable abbreviate to the shortest 
unambiguous prefix (of each word) and drop arguments that are clear from 
context, e.g.:
  sAlloc(bTrace, eTrace) -> sA(bTr, eTr) or sA

.foo: "Foo" is a meta-variable, as usual.

.estimates: When equations are written that estimate or approximate some 
quantity, the tag of the equation is postfixed with a number.  No meaning is 
attached to these numbers, they merely serve to distinguish the different 
estimates.


Arguments

.time: One-argument functions take a time and return the quantity at that time, 
unless otherwise indicated.  Two-argument functions take two times and sum of 
the quantity over that period of time, u/o/i.


Effect

.effect: The effect of some action on a quantity is notated
  D(action -> quantity)
If we need to subdivide the effects, we annotate the arrow:
  D(action -effect-> quantity)
This can be rendered as superscripting on media capable of representing that.


Segment Granularity

.segments: In practice, many of these quantities would be only be meaningful at 
segment granularity, or at least they'd be recorded and measured per segment.  
We do not usually bother to indicate that in the models, except where special 
care is required.


MODELS


Allocation

.alloc.quantities:
  sAlloc = "amount currently allocated (by the mutator)"
  sFree  = "amount freed (by the mutator)"
  sCons  = "amount consed (by the mutator and collector both)"
  sAvail = "free space available for allocation"
  sRAM   = "RAM available for this process (free and used)"
  sSwap  = "swap space available for this process (free and used)"
  sPage  = "size of a page"
  sMWPS  = "size of the mutator's working page set"
  sZone  = "zone size"
  sObjectAv = "average object size"
We use sSwap for the total available memory even if it isn't being
acquired from a VM system.

.avail.1:
  sAvail = sSwap - sCons


Trace Costs

.trace.quantities: All of these can be parameterized by trace, when
necessary.
  sFoundation = "size of the foundation (incl. roots)"
  sCondemned = "size of the condemned set"
  nCondemned = "number of objects in the condemned set"
  pgCondemned = "number of pages in the condemned set"
  sCopied = "size of objects copied during the trace (0, if non-moving)"
  nCopied = "number of objects copied during the trace (0, if non-moving)"
  sNailed = "size of objects nailed d.t.t. (all survivors, if non-moving)"
  sScan = "size of objects scanned during the trace"
  nScan = "number of objects scanned during the trace"
  sSurvivors = "size of the survivors of the collection"
  nSurvivors = "number of the survivors of the collection"
  sReclaimed = "amount of free space reclaimed by the collection"
  tScanFoundation = "time to scan the foundation (excluding copy)"
  tCopySurvivors = "time to copy the survivors of the white set"
  tScanSurvivors = "time to scan the survivors"
  tTrace = "time to do the complete trace"
  tPaging = "time spent paging for the trace"
  cScan = "cost of scanning (excluding possible copy)"
  cCopy = "cost of copying"
  MortalityS = "mortality rate (size)" = sReclaimed / sCondemned
  MortalityN = "mortality rate (number)" = nReclaimed / nCondemned
  sGCWPS = "amount of memory accessed during the collection"

.trace.survivors.1:
  sSurvivors = sCopied + sNailed

.trace.reclaimed.1:
  sReclaimed = sCondemned - sSurvivors
  nReclaimed = nCondemned - nSurvivors

.trace.time.1: Ignoring setup and reclaim time,
  tTrace = tScanFoundation + tCopyCopied + tScanSurvivors
         = eTrace - bTrace

.trace.scan.time.1: This estimate assumes that scanned sets are relatively 
homogeneous, so that the cost depends mainly on the size.  This is not true if 
we have managed to sort the objects.  Paging can also introduce a non-linearity 
as larger sets are less likely to be resident.  [@@@@ I suspect there might be 
an increased load at the beginning of a trace from paging in stuff that is 
being fixed, but hasn't been touched by the mutator in a while.  Pekka 
1998-03-11]
  tScanFoo = sFoo * cScan

.trace.copy.time.1: See .trace.scan.time.1 for assumptions.
  tCopyFoo = sFoo * cCopy

.trace.time.1: Using the time.1 estimates,
  tTrace = sFoundation * cScan + sCopied * cCopy + sSurvivors * cScan

.trace.space.1: During the trace,
  sCons = sCopied + sAlloc

.trace.gc-working-set: Considering only data in mutator pools,
  sGCWPS = sFoundation + sCondemned + sCopied

.trace.paging-time.1: We might expect that paging time for a trace is
a function of sGCWPS and sRAM.  The function is likely to be 
platform-dependent.  Something like this:
  tPaging
    = IF sGCWPS <= sRAM THEN
        sGCWPS * C1
      ELSE
        (sGCWPS - sRAM) * C2 + sRAM * C1
      END
The constants C1 and C2 are platform-specific [won't there be a cost from 
paging out the mutator's working set?  pekka 1998-04-02].  C2 is likely to be
much larger than C1.

.trace.paging.memory-cost.1: Taking .trace.paging-time.1 and differentiating 
tPaging with respect to sGCWPS gives a cost in (paging) time for an increase in 
the working set:
  dtPaging/dsGCWPS = IF sGCWPS <= sRAM THEN C1 ELSE C2 END
[@@@@ We need names better than C1 and C2.]

.trace.paging-time.2: If we start trashing (i.e., the access pattern is such 
that the next page to be accessed is likely not to have been accessed in a long 
while and therefore has to be paged in), there's a large increase when sRAM is 
exceeded, and then a slower climb:
  tPaging
    = IF sGCWPS <= sRAM THEN
        sGCWPS * C1
      ELSE
        (sGCWPS - sRAM) * C4 + C3
      END


Gangs

.gangs: As a generalization of the generational model, we partition the heap 
into _gangs_, which are intended to be the units of condemnation.  We might 
choose to condemn more than one gang at a time, but we'd rarely, if ever, 
consider condemning only a part of a gang.

.promotion: Collections will sometimes move objects from one gang to another; 
this is called _promotion_.  No particular policy of promotion is assumed, that 
is a matter for the individual strategies (but we might include models of 
common patterns, like generations, in this kit).

.zones: In practice, gangs will have a close relationship with zones, but we 
don't want to prejudice the question.

.gang.quantities:
  sFoundation(Gang) = "size of the foundation for the gang (incl. roots)"
  sPromoted(Gang1, Gang2) = "size of objects promoted from Gang1 to Gang2"

.foundation.gangs.range: Foundations are subadditive,
  sFoundation(Gang1+Gang2) <= sFoundation(Gang1) + sFoundation(Gang2)
Being able to calculate the combined foundation of two gangs would be quite 
useful for collection strategies.


Trace payoff

.trace.payoff: Some of these are benefits and some are costs incurred by 
subsequent traces or the mutator.
  D(Trace -> sFoundation(gang))
    = "size of change of sFoundation(gang) as a result of Trace"
  D(Trace -> Mortality(OtherTrace)) = "change in mortality"
  D(Trace -> sMWPS) = "change of the mutator's working page set"
  D(Trace -> sGCWPS(OtherTrace))
    = "change in the working page set of another trace"
  D(Trace -> tPaging(OtherTrace)) = "change in paging time"
  D(Trace -> Locality) = "change in locality"
  D(Trace -> cScan(OtherTrace))
    = "change in the cost of scanning (from better locality)"
  [@@@@ there must be others]

.trace.promotion.1: At the moment, we always promote, but there are other 
alternatives.
  sPromoted = sCopied

.trace.foundation-change.promotion:
  D(Trace -promotion-> sFoundation(Gang))
   = "change in sFoundation(Gang) due to promotion in Trace"

.trace.foundation-change.promotion.1:
  -sPromoted(*, Gang) <= D(Trace -promotion-> sFoundation(Gang))
                      <= sPromoted(Gang, *) + sFoundation(Promoted(*, Gang))
The promoted stuff might now be in the foundation of the set that it was 
promoted out of; it might have previously been in the foundation of the set 
that it was promoted into.  When the foundation is maintained at segment 
granularity, the promotion quantities above refer to segments consisting 
entirely of promoted objects (and it is unproductive to count segments as both 
reclaimed and promoted).

.trace.foundation-change.reclaim:
  D(Trace -reclaim-> sFoundation(Gang))
   = "change in sFoundation(Gang) due to reclaim in Trace"

.trace.foundation-change.reclaim.1:
  -sReclaimed <= D(Trace -reclaim-> sFoundation(Gang)) <= 0
The reclaimed stuff won't be in any foundation anymore; it's rather like 
promoting into a non-existent gang.  Knowing the intersection between 
Condemned(Trace) and Foundation(Gang) would sharpen these limits and those in 
.trace.foundation-change.promotion.1.  In particular, if Gang was condemned, 
then the stuff reclaimed in it won't affect its foundation.

.trace.foundation-change.update: If foundations are determined via segment 
summaries, simply scanning a segment might remove it from some foundation that 
it was previously a part of.  Similar (but smaller) effects happen in other 
methods using remembered sets.
  D(Trace -update-> sFoundation(Gang))
    = "change in sFoundation(Gang) due to scanning in Trace"

.trace.foundation-change.1:
  D(Trace -> sFoundation(Gang))
    = D(Trace -promotion-> sFoundation(Gang))
      + D(Trace -reclaim-> sFoundation(Gang))
      + D(Trace -update-> sFoundation(Gang))
Furthermore, copying objects around might have a slight effect, since
it concentrates the foundation into fewer segments (or, if we're very
unlucky, spreads it around).  However, most of this effect will be
accounted for when we remove the reclaimed objects from the
foundation, so we ignore the rest of it.

.trace.mwps.1:
  D(Trace -> sMWPS) = @@@@

.trace.gc-working-set-benefit.1: If the condemned sets don't intersect, the 
reduction in the working set of another trace would be mostly due to reclaim 
and update effects on the foundation, since copied stuff would mostly end up in 
the foundation (if it was there in the first place) or the condemned set, 
anyway.
  D(Trace -> sGCWPS(OtherTrace))
    = sum(Gang in Condemned(OtherTrace),
          D(Trace -reclaim-> sFoundation(Gang))
          + D(Trace -update-> sFoundation(Gang))

.trace.gc-working-set-benefit.2: If the condemned sets do intersect, we can 
count all reclaimed objects as benefit:
  D(Trace -> sGCWPS(OtherTrace))
    = -sReclaimed(Trace)
      + sum(Gang in Condemned(OtherTrace),
            D(Trace -update-> sFoundation(Gang))

.trace.gc-paging-benefit.1: In a high-generation collection, we can assume 
everything else is in the foundation.  [Anecdotal evidence suggests that the 
scan time can be the most significant cost of GC if it leads to heavy paging 
because the GC working set is grater than the RAM size.  This might be the most 
important benefit of the collection. tony 1998-03-10] Using 
.trace.paging.memory-cost.1 to estimate the effect:
  D(Trace -> tPaging(HighGenTrace))
    = -sReclaimed(Trace)
      * IF sGCWPS(HighGenTrace) <= sRAM THEN C1 ELSE C2 END


A Solitary Incremental Trace

.incr.solitary: Here, we're assuming only a single trace at a time.

.incr.quantities:
  tSlice = "GC time slice (current)"
  tSliceAv = "GC time slice (average length)"
  tSliceMax = "GC time slice (maximum length)"
  sSliceAlloc = "mutator allocation between slices"
  GCOverhead =
    "proportion of CPU time spent in the collector during a trace"
  @@@@

.incr.cons.1:
  sCons(bTrace, eTrace)
    = sAlloc(bTrace, eTrace) + sCopy

.incr.alloc.rate.1: Under a fixed-size mutator slice policy,
  sAlloc(bTrace, eTrace) = sSliceAlloc * nSlices

.incr.scan.rate.1: Scan rate regulation as in the current code (1998-03-05) 
[@@@@ This definitely doesn't belong here. pekka 1998-04-02]: Assume we scan in 
equal slices,
  sScan(t, t + tSlice) = sScanSlice
  nSlices = sScan / sScanSlice
try to finish by the time 1 Mb has been allocated
  sAlloc(bTrace, eTrace) = 1 Mb
Use .incr.alloc.rate1 to solve for sScanSlice, and divide by 4 Kb to get the 
number of segments to scan (and add one for luck).

.incr.rate.space.1: The basic rate equation under a space constraint at time t 
is:
  sCons(bTrace, t) + sCons(t, eTrace) = sCons(bTrace, eTrace) < sConsLimit
We know the first term (count nailed segs in here), and the third is the 
constraint given.  Break down the second term using .trace.space, and we get a 
constraint on further allocation relative to remaining copying, which we can 
estimate.
  sAlloc(t, eTrace) < sConsLimit - sCons(bTrace, t)
                      - sCopied(t, eTrace)

.incr.space.copied.max:
  sCopied(t, eTrace) < sCondemned - sSurvivors(bTrace, t)

.incr.space.copied.1: Estimating using a mortality estimate, assuming no more 
nailing,
  sCopied(t, eTrace) =
    sCondemned * (1 - MortalityS) - sSurvivors(bTrace, t)

.incr.space.copied.rate: Another estimate might be obtained by considering the 
progress of the trace.  Most copying work is done at the beginning, so if we 
have a model of that and fit it into the data from the current trace, we can 
predict the remaining work.

.incr.copy-rate.1: [Not sure this belongs here.  Pekka 1998-03-12] If we assume 
a homogeneous distribution of references (very unlikely), the probability of 
needing to copy while scanning is proportional to the number of currently 
unfixed live objects, divided by the total number of live objects.  So, 
modelling the variation of nCopied and nScan, for some constant C0,
  dnCopied = dnScan * C0 * (nSurvivors - nCopied) / nLive
Separating the variables and integrating, we find that, under these 
assumptions, the rate of copying decays exponentially:
  C1 - ln(nSurvivors - nCopied) = nScan / C2

.incr.space.copied.2: If we assume a homogeneous object size distribution, we 
can use the exponential model from .incr.copy-rate.1 to obtain
  sCopied(bTrace, t) = sSurvivors - exp(C1 - sScan / C2)
then use this in the manner suggested in .incr.space.copied.rate, getting C1 = 
ln(sSurvivors) from the boundary condition at sScan=0 and curve-fit C2 and 
sSurvivors.

