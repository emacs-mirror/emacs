                        ANALYSIS OF MPS TESTING
                             analysis.test
                             incomplete doc
                           gavinm 1996-07-30

.purpose: This is an analysis of the goals, requirements, and solution ideas 
for the software testing of the MPS (written by gavinn) followed by a 
discussion of what the MM group already does and ideas for testing and test 
procedure (written by dsm).

.origin: The second part of this document is taken from obj.test-proc (now 
obsolete).

.future: This document needs a complete overhaul; rit should do this.

GOALS

.goal.quality: Ensure the quality of our software.


REQUIREMENTS


Functional Requirements


.fun.req: Testing must check that the software complies with its requirements.

.fun.rel: Testing can be done thoroughly within a release cycle.

.fun.dev: Testing can be done reasonably well within development. 


Attribute Requirements

.attr.cpu: The execution time of the tests must be reasonably short. (Cost)
    .attr.cpu.metric: Metric: Maximum CPU-hours used over all platforms per 
test cycle
    .attr.cpu.plan: Planned level: 2 
    .attr.cpu.worst: Worst acceptable case: 8 (overnight)
    .attr.cpu.best: Best possible case: 1/6
    .attr.cpu.current: Current level: unknown

.attr.user: The test plan must take little man power to use.
    .attr.user.metric: Metric: Total man-hours used per test cycle for all 
platforms (Cost)
    .attr.user.plan: Planned level: 1
    .attr.user.worst: Worst acceptable case: 5 
    .attr.user.best: Best possible case: 1/12 (fully automatic; reports 
generated)
    .attr.user.current: Current level: unknown

.attr.impl: Designing and implementing an MPS test plan should not take too 
much effort.
    .attr.impl.metric: Metric: Man-days (Cost)
    .attr.impl.plan: Planned level: 20
    .attr.impl.worst: Worst acceptable case: 40
    .attr.impl.best: Best possible case: 5
    .attr.impl.current: Current level: 0

.attr.debug: Bugs found must be easily traced to the coding error.
    .attr.debug.metric: Metric: Expected time to fix bug indicated by testing 
in man-hours (Cost)
    .attr.debug.plan: Planned level: 4
    .attr.debug.worst: Worst acceptable case: 12
    .attr.debug.best: Best possible case: 1
    .attr.debug.current: Current level: unknown


Design Constraints

.dc.resource: The test plan should not require more resources (e.g. machines) 
than are required for development.


PLAN

.plan.attr: We have more intuitive confidence that our software meets its 
functional requirements than that it meets its attribute requirements.  We 
therefore plan to focus on testing attribute requirements.


PROTOCOLS

.prot: Unknown.


SOLUTION IDEAS

.idea.manual: The tests must be guided through (run) and interpreted by hand 
(compare .idea.auto).

.idea.interactive: A person sits and tries to break the software (see 
.idea.harness).

.idea.auto: The tests run automatically and regularly (e.g. every night), 
reporting results by, e.g, mail (compare .idea.manual).

.idea.harness: We implement an interpreted test harness that could be used for 
scripting, running traces, or interactive testing (see idea.interactive).  
GDB?  LispWorks/Dylan FFI?

.idea.unit: We test the interfaces of the internal modules to ensure they meet 
their sub-requirements.  [Reqs? -- GavinM]

.idea.fun: We design tests to cover all aspects of documented functionality, to 
ensure it meets its requirements.

.idea.flow: We design tests to follow every possible control-flow path in the 
software.

.idea.perf: We design tests to measure against attribute requirements.  These 
would test latency, throughput, etc.

.idea.stress: We design tests that hammer the software in statistically 
distrubuted ways, with the intention of finding holes by brute force.

.idea.regression: We design tests that correspond to reported bugs, to ensure 
they remain fixed.

.idea.prot: We design tests that exercise the full domains of the interfaces, 
thus testing the protocol.  These tests need not test that functionality is 
correct.

.idea.quick: We design short tests that give a low level sanity check for basic 
acceptance of an image.  [Create a cube.]

.idea.some-plat: We only run tests on that subset of platforms with that subset 
of environments on which it is easy to do.

.idea.all-plat: We run tests on all required platforms and in all required 
environments.


SOLUTION IMPACT ANALYSIS

.idea.* 
(out of 10)	.fun .req	.attr .cpu	.attr .user	.attr .impl	.attr .debug	Total Cost	Ratio
manual	0	0	8	2	0		
interactive	1	2	15	1	0		
auto	0	0	1	4	0		
harness	0	1	0	3	0		
unit	0	2	2	2	2		
fun	4	1	1	2	5		
flow	2	3	2	3	4		
perf	4	5	1	2	5		
stress	2	6	2	2	7		
regression	2	2	2	2	2		
prot	1	1	2	2	1		
quick	1	1	1	1	3		
some-plat	1	0	1	0	0		
all-plat	3	0	5	2	2		
Total							


=================================================
What follows is taken from obj.test-proc.

DESCRIPTION OF TASK

.desc: Our tests are currently generated in an ad-hoc manner.  We need to work 
out what tests we need, how to make them, how to run them and automate them, 
and what we consider acceptable for exit criteria for release.


REPORT

.report: dsm: This is as far as I got

INTRODUCTION

Testing is a way of detecting defects.  In the literature this includes
Reviews, Inspection, Unit testing, System testing, etc. i.e. any way
of finding errors.  I am using it here to mean runnable tests, which
I suppose includes unit testing and system testing.

THINGS WE ALREADY DO

.cover: Write coverage tests.  These tests are intented to ensure that
all lines of code are executed.

.stress: Write stress tests.  A stress test may be a simulation (of real
program).  It covers some functionality.  It may attempt to 'stress' a
particular part of the system.

.oneoff: Write oneoff tests in a hacky way while making a change.  This
is often done by modifying existing tests, as we do not have much of a
harness at the moment.

.automate: Automate running of our tests.  At the moment the sources
are checked out on every night and built on a SUN.  Some tests are run
and we are mailed if they fail.


SUGGESTIONS

For each piece of code we have some method or methods for testing that
piece of code.  That is there are continually maintained documents 
(eg impl.std.testdoc?) which describe the procedure for testing each
piece of code.  Writing code (proc.impl.create?) and updating code
(proc.impl.edit?) involves maintaining this document.


What a Testdoc should have in it

1. Map from code to test procedure 
this might be a general procedure eg
- .run.blind  (build & run. exit code  0?)
- .run.examine (build & run examine output)
- .run.step (step the code)
- test unfinished
- not tested
- .hope.blind
- or a specific one (eg build & run this program.  Time it.  Check it takes 
less that 7 seconds)

2. Other information
- deficiencies of tests
- ideas for improvements to tests
- justification for why a piece of code isn't tested, or testing difficult


Suggested Outline of Test Procedures

.cover.run:
- proc.build(the test)
- run the test
- check it ran ok (check exit code 0?)

.cover.update:
- check all functions in interface are tested, add if not.
- proc.build it
- profile test prog (there are hacks in some NT makefiles to do this)
- check by hand it covers
- either add coverage or document in (.testdoc) where it does not cover with 
alternative test or justification for absence

.cover.create: 
- like cover.update except build it first

.stress.run: The procedure at the moment seems to be
- proc.build it
- check it runs ok

Test Plans

What is a test plan?  Is what I call testdoc a test plan?  We should find out.  
They are probably a good idea.


Test Reviews

We could from time to time review test code


Regression Tests

We also need tests corresponding to defects found, that hopefully will
detect when they reoccur.  There should be a documented way of going from 
detected defect (issue) to test that implements it.  Perhaps this should
go in the testdoc.


Test Creation Guidelines

.test.impl.guide:  We need to have some procedures for creating tests.  There 
are some areas
where this is weak at the moment.  These can be updated as our experience 
improves.

Whether a test is worthwhile (results justify costs) depends on:
1. The cost of creating the test.
2. The cost of running the test.
3. The effectiveness of the test.  How many errors are detected?
4. The severity of the errors detected.
5. Orthogonality to other means of detecting errors.  How expensive is
it to detect the errors by other means?


Deficiencies in Tests

There are a couple of things to think about more when writing tests:
.check.results: We do not check results of functions enough.  We should check 
code gives correct results not just that it returns ErrSUCCESS.
.assert: Use asserts that will fire in both debug & release versions.  At the 
momemt we use AVER in test code, which is therefore turned off in the release 
version of the test -- this is unneccessary.


Testing During Development.

At the moment this is often done by messing around with an existing
test.  As proc.impl will probably include updating tests this will
encourage all tests to be added permanently to the existing test suite.
I think that would be very cost effective.


Testing for Release.

This will be a more or less full run of all test procedures, including
the tedious checking by hand ones.


Automation

.automate.more: The cheapest way to run a test is automatically.  This gives us 
a lot of
checking with a low running cost.  Ultimately, I would like to see blind
runs of all tests run on all platforms (most importantly NT) regularly and
automatically.


Acceptance Tests

.acceptance.attr: We would like a test that we could do to tell us how
far from meeting our various attribute requirements we are.  We need a
definitive way of knowing when we have met them.  This may involve
running a program and seeing how long it takes; or for example writing
special tests that measure pause times.

.acceptance.fun: I would include tests for functional requirements
too.  We should test our interfaces against what they are required to
do, if possible.  Running real programs is a weak form of this.


Perfomance Testing

.check.perform: Having measured our tests we should check for
any unacceptable regression or unexpected change in performance.


THINGS TO THINK ABOUT

.realprog: Use real programs as tests.  These are good for detecting
errors that are likely to arise in practice.  Errors that stop a
typical program running are the most severe.
richard: I think this is particularly valuable.  We should get frozen
Dylan systems and include them as tests.
Nosa has pointed us in the direction of the cmu test suite.


.method.emit: An effective lightweight way of generating tests is to
write code that prints data that needs to be checked.  The emitted data
is compared from one run to the next.  This is easy to automate, and
any change in functionality is immediately picked up.  Our current
tests are not very suitable for this as they print out machine
addresses and pictures of the heap.  Difference in output here would
not imply a change in functionality.

.bench: Write tests to measure performance of specific features.  We
need to have a better idea of what we wish to measure before we do
this.  Real programs are useful for giving overall performance
estimates.

.analyse: Analyse measurements to provide feedback about what can
be done to improve the code, and what effects various changes to the
code have.

.investigate.literature: There is some literature about testing, which
could be read.  From a brief glance it seems that perhaps the most
useful stuff is the hard figures about the effectiveness of different
types of testing.

.harness: We use C as our test harness.  This isn't very
sophisticated...



