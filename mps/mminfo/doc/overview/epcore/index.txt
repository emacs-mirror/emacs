        DOCUMENTATION FOR THE CORE RIP MEMORY MANAGEMENT SYSTEM
                            overview.epcore
                              obsolete doc
                            nickb 1997-04-24

HARLEQUIN COMPANY CONFIDENTIAL

0. Introduction

.readership: This document is intended for EP developers and management.

.intro: This document describes the background, scope, interface, and plans of
the Core RIP Memory Management System,

.version: This version of this document corresponds to the "trapping
beta 4" release of that system.

.breakdown: Section 1 of the document is an overview or executive
summary.  Section 2 discusses the structure of the new code.  Section 3
describes the underlying Memory Pool System.  Section 4 describes our
intentions for future work.  Section 5 briefly describes the Memory
Management Group.

.infosys: This document refers to several other documents in the Memory
Management Information System, or InfoSys.  These other documents can be found
in the Memory Management Information database in Spring
(\PROJECTS\Memman\MMINFO2.NS3).

.tag: This document is "tagged".  For instance the ".tag" at the start of
this paragraph is a tag.  Tags allow precise identification of individual
points in a document.  For more information on tagging, see guide.tag in
the InfoSys.


1. Overview of the memory manager

.overview.project: The Memory Management Group is providing a new memory
manager for the ScriptWorks core RIP.  The project to develop this memory
manager is known as "MM/EP-Core"; for more details of it, see the
document project.epcore in the InfoSys.

.overview.mps: The new memory manager is based on the MM Group's portable
memory management framework, called the Memory Pool System, or MPS.  The MPS
also forms the core of the Dylan memory manager, will be a part of the
planned memory management product, and may become part of MLWorks at some
point.

1.1. Motivation

.motive: The MM/EP-Core project was initiated by EP in 1995, because the
existing memory management code was inflexible and hard to maintain, and
did not provide garbage collection as required for Level 2 PostScript.
Various memory-related requirements have emerged since then, which
increase the motivation for a new memory manager (for instance, the
desire to extend the memory arena temporarily in low-memory situations).
See also goal.epcore in the InfoSys, for the goals of the MM/EP-Core
project.

1.2. Progress

1.2.1. History

.history.layer: Detailed requirements for the new memory manager were
established in 1995 (see req.epcore in the InfoSys).  Development started
in 1996 with the separation of memory management functionality from the
core RIP into a separate compound with a well-defined interface (the
memory management interface, or MMI).  This was the "layered" MM/EP-Core
release.

.history.proto: An experimental memory management system was tried on a
branch in the latter part of 1996, which established some further
requirements and performance baselines.  This was the "prototype"
MM/EP-Core release.

.history.trapping: A release was made into the SW trunk in February 1997
(1997-02-14).  This was the "trapping beta" release.  Testing by EP
developers in the trunk quickly identified some integration defects,
fixed in the "trapping beta 2" release (1997-02-20).  Performance
problems on some jobs were addressed in the "trapping beta 3" release
(1997-03-27).  Further tweaks, and a port to a new platform, are in the
"trapping beta 4" release (planned for 1997-04-22).

1.2.2. Cut-Over

.cutover: The changes to the memory manager have been done in a careful
and modular fashion, so that it is still possible for the RIP to use the
previous memory manager.  The new memory manager has a number of features
which are not supported by the old memory manager, but the RIP avoids
using these in order not to jeopardize this "back off" option.

Shortly, following side-by-side performance testing, EP management will
either decide that the new memory manager is satisfactory, and "cut
over" to it, or will find it unsatisfactory and "back off" the trunk to
the previous memory manager.

If EP management decide to "cut over", EP developers may then take
advantage of the greater flexibility and functionality provided by the
new memory management system.

1.2.3. Future Plans

.plans: Following cut-over, the MM/EP-Core project will continue to
improve the new memory manager, to provide garbage collection for
PostScript VM, to take advantage of operating-system memory management
capabilities where available, to provide better performance, debugging,
and analysis tools, to provide specialized memory management sub-systems
where required (for instance, for PDF), and so on.  For more details of
future plans, see .future in section 4.

1.3. Where to find the new MM

.where: The new MM is in the SWmm_mac, SWmm_pc, and SWmm_unix compounds.
It is a combination of interface files, "glue code" and MPS binaries.
More details on this structure, including our reasons for delivering the
MPS as binaries, are in section 2 of this document.

.where.old: The old MM is in the SWmm compound.  "Backing off" would be a
simple matter of removing the SWmm_* compounds from their parents and
replacing them with SWmm.


2. Code Structure

.structure.hope: The Hope structure of the new MM is as follows.  There
are three parent compounds, SWmm_mac, SWmm_pc, and SWmm_unix.  These
contain platform-specific items (make files and MPS binaries) and also
the common MM code (in sub-compounds SWmm_common_export and
SWmm_common_src).

.structure.mmi: In order to allow for cut-over, MM/EP-Core moved the
memory manager out of SWv20 into SWmm*, and introduced the memory
management interface between the core RIP and the memory manager.  This
has allowed us to replace the MM without further change to the core RIP.
The MMI is described in section 2.1.

.structure.glue: The MMI is not supported directly by the MPS.  Instead,
there is code (in SWmm_common_src) which implements the MMI on top of
the generic structures provided by the MPS.  This code is called "glue
code", and is described in section 2.2.

.structure.rip: Some MM-related code was left outside the MMI.  This
resides in SWv20!src:mmcompat.c, and is described in section 2.3.

.structure.binaries: The MPS is provided in binary form, with two
binaries for each platform (debugging and release varieties).  These
binaries are discussed in section 2.4.

2.1. The Memory Management Interface

.mmi: The MMI is defined in SWmm_common_export!mm.h and
SWmm_common_export!mm_core.h.  mm.h declares functions and values
exported from the MM to the RIP.  mm_core.h declares functions and values
exported from the RIP to the MM.  Both of these files are quite heavily
commented.

2.1.1. MMI Types

.mmi.types: Various concrete and abstract types are declared:

mm_addr_t is the type of addresses managed by the memory manager: void *.

mm_size_t is the type of object sizes: uint32.

mm_result_t is the type of memory management function result codes:
uint32.  Values of this type are always one of the enum constants
MM_SUCCESS and MM_FAILURE.

mm_alloc_class_t is the type of "allocation classes": uint32.  Every
allocation specifies an allocation class, which may be used for
debugging.  Values of this type are taken from the enum mm_alloc_class_e.
See .mmi.debug in section 2.1.8 for more information.

mm_pool_t is the abstract type of a "memory pool": a "place" in which
memory can be allocated and/or freed independently of other pools.  See
.mmi.pools in section 2.1.3.

2.1.2. MMI Control

.mmi.control:

mm_init(block, size) initializes the memory manager, providing it with a
block of memory to manage (starting at 'block', of size 'size' bytes).
This must be called before any other MMI function.  Before this is
called, no memory management function or variable is meaningful.  If it
returns MM_FAILURE, an insufficient amount of memory was passed.

mm_finish() notifies the memory manager that the RIP has exited normally.

mm_total_size() returns the number of bytes of memory managed by the MM.

2.1.3. Pools in the MMI

.mmi.pools:

There are two or three pools, of type mm_pool_t: mm_pool_temp is a
general-purpose pool, mm_pool_dl_1 is a display list pool (and so is
mm_pool_dl_2, if the MM is compiled for MULTI_PROCESS).  There are
several generic pool functions:

mm_pool_size(pool) returns the number of bytes in 'pool'.

mm_pool_free_size(pool) returns the number of bytes in 'pool' which are
currently free.

mm_no_pool_size() returns the number of bytes not currently in any pool.

2.1.4. Free and Allocate in the MMI

.mmi.free: mm_free(pool, what, size) frees a single object of size
'size' at address 'what'.  The object must have been allocated in the
pool 'pool'.

.mmi.alloc: mm_alloc(pool, size, class) allocates a single object of
size 'size' and class 'class' in the given pool.  It returns NULL if the
allocation fails.  'size' must be non-zero.  Classes are only used if
MM_DEBUG_ALLOC_CLASS is on (see .mmi.debug in section 2.1.8).

mm_alloc_multi_homo(pool, count, size, class, returns[count]) allocates
'count' objects of size 'size' bytes each, with class 'class' in pool
'pool'.  If all the allocations succeed it puts the resulting pointers in
returns[] and returns MM_SUCCESS.  If any allocation fails then they all
do, and the call returns MM_FAILURE.

mm_alloc_multi_hetero(pool, count, sizes[count], classes[count],
                      returns[count]) allocates 'count' objects in pool
'pool'.  Object i is of size 'sizes[i]' and class 'classes[i]'.  If all the
allocations succeed it puts the resulting pointers in returns[] and returns
MM_SUCCESS.  If any allocation fails then they all do, and the call returns
MM_FAILURE.

2.1.5. Display List Pools in the MMI

.mmi.clear: mm_pool_clear(pool) frees everything in the DL pool 'pool'.

.mmi.promise: Each display list pool supports a single "promise", which
is a specialized contiguous sub-allocator.  The promise is started by
calling mm_dl_promise(pool, size): if this returns MM_SUCCESS then it
promises that 'size' bytes may be suballocated contiguously.  The next
piece of promised memory is obtained by mm_dl_promise_next(pool, size).
This fails with an assert if the promise is exhausted.  When the RIP has
finished making sub-allocations, mm_dl_promise_end(pool) returns any
unused promised memory to the pool.  mm_dl_promise_free(pool) abandons
the current promise and frees any memory allocated from it.  It can also
be called after a promise is ended, in which case it will free the last
promise.

2.1.6. PostScript VM in the MMI

.mmi.psvm.pools: There are two pools for PostScript "Virtual Memory" (a
term from the Red Book, and nothing to do with real "virtual memory" at
all): mm_pool_ps_local and mm_pool_ps_global, for local and global
memory respectively.

.mmi.psvm.alloc: mm_ps_alloc_obj(pool, size) allocates a PS object of
size 'size' bytes.  mm_ps_alloc_string(pool, size) allocates a PS string
of size 'size' bytes.

.mmi.psvm.save-restore: PSVM pools support a notion of "current save
level".  Every object is allocated "at" the current save level.
mm_ps_save(level) increases the save level on the local pool (and on the
global pool if the old level is MAXGLOBALSAVELEVEL or below) to 'level'.
mm_ps_restore(level) restores to level 'level', freeing all objects
allocated above that level.

.mmi.psvm.check: mm_ps_check (level, what) checks that 'what' does not
point into PSVM allocated at a save level higher than 'level'.  It
returns MM_SUCCESS if it does not (this includes pointers which are not
to PSVM at all, and NULL pointers).

.mmi.psvm.constants: MINSAVELEVEL and MAXSAVELEVEL are defined in
mm_core.h to be the minimum and maximum save levels.  MAXGLOBALSAVELEVEL
controls the effect of save and restore on the global pool.  For
instance, a value of 1 means that global PS VM supports save levels 0
and 1.

2.1.7. Low Memory in the MMI

.mmi.low.summary: If an allocation fails because of insufficient memory,
or succeeds and causes free memory to become less than
mm_free_size_limit, it calls the low memory handler mm_low_handler() in
the RIP.  There is quite a complex protocol here, inherited from the old
memory manager; it might be simplified after cut-over.  Briefly, the low
memory handler might:

- say "out of memory" and quit the job (VM error), or
- say "memory low" and make this allocation fail, or
- say "go ahead", and allow the allocation to succeed despite low memory, or
- purge caches &c and tell the allocator to retry.

The allocation is retried in a loop so that if it still results in a low
memory condition, the low memory handler will be called again, &c.  For
more details, see mm.h.

.mmi.low.action: The type mm_low_action_t is the type of a local
variable passed by reference to the low memory handler.  The low memory
handler can use this to record the "low memory state" so that future
low-memory loop iterations will not retry the same purging actions.  The
first time around the loop, this variable has the value of the external
variable mm_low_actions, which can be set to prohibit certain low-memory
actions in critical sections.  There are various macros for manipulating
these values; see mm_core.h for details.

.mmi.low.check: mm_low_check() is equivalent to an empty allocation; it
calls the low memory handler if memory is low.

.mmi.low.test: mm_low_test() evaluates the low memory condition.  It
returns MM_FAILURE if memory is low, and MM_SUCCESS otherwise.

2.1.8. Debugging Support in the MMI

.mmi.debug: mm.h may define various preprocessor macros which control
debugging in the MM.  There are several different levels and kinds of
debugging, some of which may be controlled independently:

   MM_DEBUG_TOTALS
     - keep object/byte totals for each pool.  This helps with
       leak detection and is quite cheap.x

   MM_DEBUG_TAG
     - tracks each allocated object, detecting overlapped
       allocations or incorrect frees.  This is currently expensive,
       and uses non-MM memory (malloc) to keep tracking information.

   MM_DEBUG_FENCEPOST
     - keeps fenceposts at either end of each allocated object,
       and checks every fencepost at every allocation and free.
     - This changes the layout of objects in memory.
     - The checking is very frequent and therefore expensive.
     - This implies MM_DEBUG_TAG.

   MM_DEBUG_ALLOC_CLASS
     - Requires each allocation point to identify the 'class'
       of the object being allocated.  These classes (values of
       type mm_alloc_class_t, from enum mm_alloc_class_e, see
       mm_core.h) are useful if MM_DEBUG_TAG or MM_DEBUG_WATCH
       are on.
     - This requires recompilation of the core RIP.

   MM_DEBUG_LOCN
     - when used with MM_DEBUG_TAG, records the file and line
       of each allocation in the tag.
     - This requires recompilation of the core RIP.

   MM_DEBUG_WATCH
     - allows the core RIP to interrogate the MM about
       allocated objects, and to watch as objects are allocated
       and freed.
     - This implies MM_DEBUG_TAG.

.mmi.debug.watch: A 'watcher' function can be informed about objects in
the DL and temporary pools.  Watchers may be called on events, or applied
to every live object.  For more information, see mm.h.

2.2. Glue Code

.glue: Glue code resides in SWmm_common_src, which is a child compound
of SWmm_mac, SWmm_pc and SWmm_unix.  It translates MMI calls into MPS
calls, and exists to maintain compatibility with the old memory manager.
It also performs various debugging functions.  After cut-over, the MMI
and the glue code will become the responsibility of EP, who may choose
to dissolve large amounts of it.

.glue.vm: Most glue code is in vm.c.  This file translates calls to the
MMI into calls to the MPS, and implements promises, multiple
allocations, the low-memory loop, and various forms of debugging.  It
also provides an assertion handler to the MPS.

.glue.mmps: Handling for PS VM is done in mmps.c.  This functionality
will move into the MPS in a later release, and PS VM will be garbage
collected.

.glue.mpslibep: The MPS is freestanding code, with no requirement for
any libraries.  This means that certain functionality (e.g.  I/O for
assert messages before any assertion handler is installed) must be
provided by the client to the MPS.  This is implemented in mpslibep.c

.glue.headers: Also in SWmm_common_src are various header files
declaring the interface to the MPS.  These are copied directly from the
MMsrc compound, and should not be modified locally.

2.3. Compatibility Code

.compat: When the memory manager was moved out of v20, some
memory-related code was felt to be so specific to the RIP that it should
remain in v20.  This code is in v20/src/mmcompat.c.  It includes the
low-memory handler, some specific allocation code, &c.

.compat.header: The old memory manager supported allocation with and
without a header, containing the object size.  The layered and new memory
managers do not support object headers, and this functionality has been
left in mmcompat.c: mm_alloc_with_header &c.

2.4. MPS Binaries

.binary.why: The choice to deliver the MPS to EP as binaries is a
controversial one, which deserves some discussion here.  There are
various reasons behind this choice, which boil down to the fact that MM
development needs to remain independent of EP to flexibly provide high
quality support to all our diverse clients:

- it allows the MM Group to develop on their own trunk without affecting EP
  except at carefully-controlled release points;
- the MM Group has its own build system, which would be difficult to integrate
  with the SW build system (e.g. uses different compilers);
- the MM Group has its own Hope structures, which must remain independent of
  SW Hope structures (e.g. cannot share triggers);
- it maintains our control over MM source changes (which is essential as it
  directly affects the other clients of the MM Group);
- it involves the MM Group directly in ports to new platforms (which
  may have different MM or build considerations);
- it gives the MM Group practice in building binaries on diverse
  platforms and releasing products to real clients.

See also mail.richard.1996-05-09.12-22, mail.richard.1996-05-14.14-27,
mail.kap.1996-05-24.15-52, and mail.richard.1996-05-07.12-08.1 in the
InfoSys.

For information, the MM/EP-Core binaries are built from MM sources in
the MMsrc compound.  The "trapping beta" releases are built from
checkpoints on the branch MMdevel_sw_eq of this compound.


.binary.where: The binaries for different platforms live in different places:

SWmm_mac!s7m6ac/dp/mmsw_DP.o: MacOS 7, 68k, Apple C compiler, debugging
SWmm_mac!s7m6ac/ro/mmsw_RO.o: MacOS 7, 68k, Apple C compiler, release
SWmm_mac!s7ppac/dp/mmsw_DP.o: MacOS 7, PPC, Apple C compiler, debugging
SWmm_mac!s7ppac/ro/mmsw_RO.o: MacOS 7, PPC, Apple C compiler, release

SWmm_pc!ntalmv/dp/mmsw.lib: Windows NT, Alpha, VC++, debugging
SWmm_pc!ntalmv/ro/mmsw.lib: Windows NT, Alpha, VC++, release
SWmm_pc!ntppmv/dp/mmsw.lib: Windows NT, PPC, VC++, debugging
SWmm_pc!ntppmv/ro/mmsw.lib: Windows NT, PPC, VC++, release
SWmm_pc!nti3mv/dp/mmsw.lib: Windows NT, x86, VC++, debugging
SWmm_pc!nti3mv/ro/mmsw.lib: Windows NT, x86, VC++, release

SWmm_unix!i4r4cc/dp/mmsw.a: Irix 4, Mips R4k, CC, debugging
SWmm_unix!i4r4cc/ro/mmsw.a: Irix 4, Mips R4k, CC, release
SWmm_unix!i5r4cc/dp/mmsw.a: Irix 5 (or 6 O32), Mips R4k, CC, debugging
SWmm_unix!i5r4cc/ro/mmsw.a: Irix 5 (or 6 O32), Mips R4k, CC, release
SWmm_unix!iam4cc/dp/mmsw.a: Irix 6 N32, Mips R4k, CC, debugging
SWmm_unix!iam4cc/ro/mmsw.a: Irix 6 N32, Mips R4k, CC, release
SWmm_unix!suspgc/dp/mmsw.a: SunOS, Sparc, GCC, debugging
SWmm_unix!suspgc/ro/mmsw.a: SunOS, Sparc, GCC, release
SWmm_unix!sospgc/dp/mmsw.a: Solaris, Sparc, GCC, debugging
SWmm_unix!sospgc/ro/mmsw.a: Solaris, Sparc, GCC, release
SWmm_unix!suspgc/ro/mmsw.a: Irix 4, Mips R4k, CC, release

These are automatically found by the appropriate SW make file.  The names
of these files will change in future releases (the platform codes inside
the MM are undergoing revision).


3. The Memory Pool System

.mps:This section gives a very brief overview of the Memory Pool System,
or MPS.  For more information, see design.mps.arch in the InfoSys.

The MPS is the portable memory management framework produced by the
Memory Management Group.  It forms the core of various MM Group products.

3.1. Pool Classes and the Memory Pool Manager

.mps.mpm: The Memory Pool System consists of the Memory Pool Manager and
a collection of pool classes.  The MPM interfaces with the client and
provides underlying memory services (for instance, operating system
memory management functions, client communications), while each pool
class implements a single memory management policy (for instance, the
MFS pool class manages manually-freed fixed-size small objects).  The
core RIP uses the MV pool class (manual-freed variable-sized objects)
and the EPDL pool class (for display lists).

.mps.pool: A pool is an instantiation of a pool class.  It may provide
various general memory methods, such as allocation and freeing, and also
various class-specific methods, such as save and restore in a pool class
for PostScript VM.  Separate instantiations of a given pool class are
independent.

3.2. Arenas and Spaces

.mps.arena: The MPS can obtain memory directly from the OS, or via
malloc, or from the client.  This distinction is abstracted as an "arena"
from which pools obtain memory resources.

.mps.space: Through carefully controlled use of global variables, the
MPS supports several independent instantiations, called spaces.  This
means that, for instance, several different components of a piece of
software can use the MPS without cooperating (or even knowing of each
other's existence): they will each occupy a separate "space".

3.3. GC

.mps.gc: Several pools support garbage collection, and the MPM provides
considerable infrastructure for GC of many different kinds (e.g.  copying or
not copying, exact or conservative, incremental or non-incremental).  For
instance, Dylan uses an incremental mostly-copying GC built on the MPS.  GC
for PSVM will use a new pool class built on this infrastructure.

.mps.gc.format: The MPS has no knowledge of object format, but supports
callbacks into client code to manipulate or traverse objects of
client-defined formats.

3.4. Assertions

.mps.check: The MPS source code performs a great deal of checking on its
arguments and internal data structures.  Our experience is that data
corruption almost always results in a swift assertion failure rather
than a core dump.  At least one ScriptWorks bug has been detected in this
way (when a file buffering routine was trampling on other memory).

.mps.handle: An MPS client can provide an assertion handler to which MPS
assertion failures should be passed.  For instance, MPS assertion
failures in the core RIP result in a core RIP assertion failure.

3.5. Allocation Points

.mps.ap: Several pools support "allocation points", which are an
abstract datatype allowing very fast (inline) object allocation.

3.6. The MM Product

.mps.event: The MPS provides a notion of events, which can be
communicated to an analysis or debugging tool.  For instance: allocating
an object, copying an object and starting a garbage collection are all
examples of events.

.mps.product: We are developing a commercial product based upon the MPS,
including a variety of pool classes, which will include one or more
graphical tools using events to assist users to debug and optimize their
memory use.  This tool will naturally be available to EP developers.


4. Future Work

.future: Several future releases of MM/EP-Core are planned, to add new
functionality and improve performance.  The principal planned changes are
as follows:

4.1. Garbage Collection

.future.gc: The original motivation for the MM/EP-Core project was to
draw upon the Memory Management Group's expertise to provide ScriptWorks
with garbage collection for PostScript virtual memory.  This is required
for Level 2 PostScript, and will be provided in a forthcoming release
(after PSVM support has moved into the MPS).

4.2. More Pool Functionality

.future.pool: More pool classes and pool functionality may be provided
in future.  For instance, determining the pool which owns a given
address.  After cut-over, the RIP may soon begin using more pools, to
take advantage of per-pool memory flushing.  The RIP may also use
allocation points to make allocation faster.

4.3. Performance

.future.perf: It is a requirement of the MM/EP-Core project that the
performance of ScriptWorks not be degraded significantly by the presence
of the MM code (see req.epcore.attr.run-time and req.epcore.attr.tp in
the InfoSys).  In fact the MM Group has done considerable work to ensure
this, and will continue to endeavour to improve performance.

4.4. Arena Extension

.future.extend: The MPS as currently delivered to EP supports arena
extension.  After cut-over, the RIP will be able to use this to provide
graceful performance degradation in severe low-memory conditions
(instead of painting to disk).

4.5. PDF Support

.future.pdf: PDF support in the RIP has various MM requirements in the
long-term.  For instance, GC of PDF objects, using PDF-specific pool
classes, &c.

4.6. Operating System Support

.future.os: Many OSes provide memory-mapping and memory-protection
primitives.  The MPS is able to use these to reduce fragmentation and
improve memory performance.  Currently this is disabled in MM/EP-Core as
there is no interface for the RIP to specify when this is permissible
(e.g.  to turn it off when running in an embedded controller).  Such an
interface will probably be forthcoming.

4.7. Debugging and Analysis

.future.debug: Some debugging currently performed in the glue code may
move into the MPS.  Certainly the MM product and related tools will be
available to EP developers to debug and optimize the RIP's memory use.


5. The Memory Management Group

.mm-group: The Memory Management Group is a part of Harlequin's SP
division.  It includes people with a variety of experience and expertise
in memory management and garbage collection.  The goals of the group are
(from goal.general in the InfoSys):

  - To make products for sale by Harlequin.
  - To provide solutions for other groups within Harlequin.
  - To reduce duplicated effort within Harlequin.
  - To improve the quality of solutions within Harlequin.
  - To broaden and improve the image of Harlequin.

