                  INVESTIGATION INTO MEMORY MANAGEMENT
                             survey.invest
                              obsolete doc
                           richard 1994-04-14

NOTE: This document was written very eary on in the project, and used to be 
part of the HOPE structure.

 - invest.doc 

1  Introduction

1.1  Document History

This document was created by Richard Brooksby as an expansion of notes taken 
during a survey of techniques used within Harlequin, and of notes taken during 
subsequent design meetings with Tucker Withington.

2  Survey of Harlequin Techniques and Requirements

Between 21st March and 1st April 1994, Richard Brooksby carried out a survey of 
memory management techniques and ideas within Harlequin.  The survey was 
conducted by interviewing the experts in the relevant fields.  The notes taken 
during these interviews formed the basis for this section of the document.

The interviewees were asked to consider the following questions:
- What are your requirements of a memory management subsystem?
- How does your memory manager work?
- What are the reasons for the current design?  Which parts are genuine design 
and which are historical accidents?
- What are the good and bad points of the memory manager?
- How do you measure the performance of your memory manager and the code which 
uses it?
- How do you debug the memory manager and the code which uses it?
- What do you wish the memory manager could do?
- What requirements can you imagine for a wider Harlequin memory management 
product?
- Where can more information be obtained?  What are you favourite papers?  
Where is your documentation?

2.1  RCLE

RCLE  is the Real-time Common Lisp Environment, a system being developed under 
contract to AT&T which must satisfy certain hard real-time requirements.  

2.1.1  Requirements

A detailed description of the acceptance criteria can be found in 
~rcle/doc/admin/acceptance-criteria.ps.
- Lisp must respond within 10ms 99% of the time.
- Overall speed must be no more than about three times slower than LispWorks.
- The platform is a 64Mb SPARC2 running VxWorks, a real-time Unix without 
virtual memory.  About 48Mb is available for RCLE.

2.1.2  Operation

Finding and fixing pointers held by the mutator takes too long.  An object 
table is  used.  All references to objects are in fact the index of a table 
entry, and do not change throughout the lifetime of the object.

There is a single 8Mb object table of 10-byte records: a generation byte, a 
type byte, and two words, the meaning of which depends on the type.  Cons 
cells, floats, and doubles fit into these two words.  Other objects live on the 
main heap and are referred to by the contents of the two words.  All objects 
have a header which is a pointer back to the object table entry from the heap.  
Even objects which are completely inlined into the object table have a header 
on the heap so that they can be found during a scan.  All references from one 
object to another go via the object table.

The structure of an example object is shown in Fig.\x111.  It shows a cons cell 
inlined into the object table, containing a record and zero.  The record 
contains the numbers one to four.
- Describe mapping of pages onto generations
- Entry tables
- Return address swizzling
- Algorithm
- Strategy
- Incrementality

2.1.3  Alternatives considered

- Copying collector with read barrier (mutator follows forwarding pointers).  
Overheads even on cons.  Experience lacking.
- Adapting LispWorks -- each move phase locates pointers which need to be 
fixed.  Increases complexity.  Problems scanning stacks.  Large overheads.
- Calico -- overheads on objects.

2.1.4  Measurement

RCLE development uses ad-hoc measurement techniques.  Since Lisp is dynamic 
measurement is usually done by redefining parts of the system with 
instrumentation added.  Currently, they measure the following parameters:

- heap sizes,
- number of objects allocated,
- size of each generation,
- number of live objects, and
- length of timeslice.

Longer term measurements will be necessary to tune the load-balancing 
algorithms and progress controls.  These will be done later.

2.1.5  Debugging

Debugging is done manually using redefinition, instrumentation, gdb, and trace 
macros.  The system does not have any debugging infrastructure built in.

2.1.6  Future work

More phases will become incremental.  Progress control and monitoring will be 
improved.  Page management is required.  The system must also deal with 
fragmentation of the large object area, and compaction of large object space 
across time slices.  Performance improvements will be made all round.  Some 
sort of object re-use would be good for AT&T in particular.  Stack allocation 
will be investigated.

Need also to consider applications other than AT&T to make the system more 
generally useful and marketable.

2.1.7  References

A description the Calico by Steve Englestad of AT&T.
Internal documentation of RCLE is in ~rcle/doc/tech/design.
RCLE contract acceptance criteria are in ~rcle/doc/admin/acceptance-criteria.ps.

2.2  LispWorks

Text from Richard's notes.

2.3  PC LispWorks

Text from Richard's notes.

2.4  MLWorks

Text from Richard's notes.

2.5  ScriptWorks

Text from Richard's notes.

2.6  Genera

Text from Richard's notes.


3  Operating System Techniques

3.1  DOS and Windows

Text from Richard's notes.

3.2  Macintosh

Text from Richard's notes.


4  Analysis

4.1  Requirements

- Time-class: Real-time, Incremental, Batch, Explicit
- Efficient
- Robust: 1) internal coping -- few constraints on users; 2) external abuse
- Measurable
- Observable
- Flexible
- Component-based
- Low overheads
- Unified engineering framework
- Third party support

4.2  Rival Products

4.2.1  Purify

Text from Richard's notes.

4.2.2  SmartHeap

Text from Richard's notes.

4.2.3  Free Software

Text from Richard's notes.

4.3  Product Features

4.3.1  Measurement

Many developers take dynamic memory allocation for granted, and are unaware of 
their own allocation and usage patterns, especially when their use of 
dynamically allocated memory is complex.  Memory usage patterns are hard to 
determine by static analysis of code because the lifetimes of the objects are, 
by nature, dynamic, and often have little to do with the program structure. 
Measurement is the memory management equivalent of profiling -- it provides 
programmers with the information necessary to tune their use of the memory 
management subsystem to increase memory efficiency and thereby the performance 
of their program.

Measurement tools would collect data about objects including:
- size,
- creation time,
- creator (creation call tree),
- parent (locate references to an object),
- class (for any user definition of the term),
- source location, and
- representation category.

This data is both multi-dimensional and voluminous, so flexible graphical and 
interactive presentation is essential if the user is to understand and analyze 
it.  Some example presentations are shown in Fig.\x112.  Ideally, the user 
should be able to select the presentation of the data independently of the data 
source, perhaps using some sort of simple graphical programming tool.

4.3.2  Debugging

System supplied memory managers provide little or no debugging support.  Memory 
management bugs fall into several categories:

- abuse of memory management subsystem interface (e.g. double-freeing),
- incorrect use of memory (clobbering), and
- lost memory (leaks).

Another area where a memory manager can provide debugging support is on the 
visualization of object structure.  Garbage collectors implicitly understand 
the structure of objects by examining the contents of memory.  This may allow 
observation of the data structures using an appropriate graphing tool.  It may 
even be possible to step through structure changes as the program runs.  The 
benefits of such a tool are enormous, as it provides easy visualization of the 
programs data structures.

4.3.3  Performance Enhancement

4.3.4  Garbage Collection

4.4  Hazards

- OOPS have bespoke run-time systems
- Can't rely on compiler support
- System MM code and hardware is often buggy and difficult to deal with
- Threading and OS interfaces aren't cooperative
- PC and Mac MM is changing due to new architecture development
- Object file formats are a nightmare and should be avoided
- We must invent marketable terminology--'Garbage Collection' is right out!

4.5  Further Investigation

- C and C++ development environment tools
- Rival products: Amherst GC Toolkit, Boehm's conservative GC.
- Cache and Virtual Memory simulators and analyzers
- Code instrumentation schemes, c.f., Laurus & Ball
- C++ standardization process with respect to garbage collection
- System memory manager replacements
- Cache architectures
- Virtual Memory paging algorithms and interfaces
- Presentation of and interaction with voluminous data
- Benchmarks for garbage collection, c.f., Zorn
- Lower level memory management techniques

5  Proposals

The key concept behind the Automated Storage Management product suite is that 
it offers a continuum of products, from simple to sophisticated, with each 
product providing a natural impetus for the user to step up to the next product 
as their needs grow. 

5.1  Marketing

- Document the internals and release them to developers to engender trust.
- Sell to managers on the grounds of improved bug rates and development time.
- Sell to developers on the grounds of being able to do whizzy things.
- Make feedback from customers easy/desirable/mandatory.
- Get into close relationships with customers.
- Sell a range of products, each of which leads on to the next.  Provide 
analysis which suggests the need for management.
- Nurture early reference accounts.
- Market testing.
- Go to developer conferences.
- Sell courses in how to use the products effectively.
- Consultancy on how to use the products effectively.
- Consultancy to tune products to customer programs.
- Contracts to develop bespoke solutions.

5.2  Engineering

- Don't be afraid to use architecture and OS specific stuff to get 
performance.  Tune the software for the host environment.
- Use a unified engineering framework and architecture for the products.
- Identify three layers: strategic, algorithmic, interface & representation.
- Products must be easy to 'plug-in' to user programs.  They must be components 
where the user has control of the amount of cooperation and intrusiveness.
- Management algorithms should apply in an object-oriented manner to 'areas', 
'heaps', or 'memory sources', thus allowing them to cooperate and the user to 
apply the appropriate method.
- See Fig.\x113 for a possible configuration of components within the products.
- Benchmarks
- Test suite

5.3  Products

- Low-level memory managers for taget platforms: block allocation, page 
optimizations, etc.
- Explicit memory manager
- Memory Analyzer
- Memory Debugger

5.3.1  Benchmark Suite

The first "product" will not be a revenue-producing product but will serve 
several purposes.  A suite of benchmarks, along the lines of the SPECMark 
suite, but specifically aimed at exercising memory-allocation/managment systems 
will be developed.  This suite will be run against competetive products, 
potentially using instrumented code techniques to discover weaknesses.  The 
same benchmarks will be used to ensure that our products overcome those 
weaknesses, as far as possible.  The benchmark suite can act both as a quality 
and regression test for releases of our products.  The results of the 
benchmarks accross a range of products (both ours and competetive) can be the 
basis for a technical paper that also can be used for marketing purposes.

5.3.2  Profiler

A logical outcome of the benchmark work should be the development of a 
profiling tool that can eventually become a product.  The profiling tool can 
initially be offered as a way to debug and tune existing memory managers, but 
it also gets our "foot in the door".  If possible, it is desirable for the 
profiler to be able to spot situations that would be solved by one of our 
successive products and recommend its use.  Clearly this cannot be done in the 
first version, as the successive products will not yet be available.

5.3.3  Malloc&Free Replacement

Another "foot in the door" product.  The malloc/free replacement product should 
attempt to address any shortcomings we have discovered in competing products 
(such as poor VM interactions) and at the same time introduce concepts such as 
multiple heaps and any other interface extensions we expect to be required by 
the ASMWorks product.

5.3.4  ASMWorks

ASMWorks is the top-end product, but within that product there are still a 
range of choices.  Recognizing that no one storage management algorithm can 
cover all possible application requirements, ASMWorks includes a range of 
managment algorithms which can be used stand-alone or multiple algorithms can 
be used together within a single application, where desirable.  There are a 
number of axes that define the product matrix, chief among them:  "when to 
collect", "what to reclaim", and "how to recycle".

ATTACHMENT
   "invest.doc"

