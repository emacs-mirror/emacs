                           ANALYSIS OF PURIFY
                         rival.purify.analysis
                               draft doc
                             ptw 1994-05-16

Executive Summary

Purify's market appeal likely stems from it's approach:  it does not
require any change in program source to run.  It targets the most annoying
and baffling source of C and C++ errors:  memory mis-management.  It's
short-coming is that it cannot be used in the delivered application due to
overhead (although it is a factor of 10 better than the CenterLine approach
of emulating).  Additionally, some of it's checks are only statistical, in
that they only cover more likely cases or depend on the error case being
executed while running under Purify.

I believe a garbage-collection-based tool could do a more complete job (in
some areas– it cannot replace some of the checking and monitoring features)
and has the potential of having a low enough overhead that it could be used
in the delivered application, but it will have to overcome the drawback
that source changes  and a change in coding style will very likely be
needed to take advantage of it.

Summary of operation

Purify uses some fairly simple techniques to catch memory-leaks (allocated
memory that has become inaccessible without being freed), memory access
errors (limited array bounds checking, access to freed memory, attempts to
free non-heap or already free memory, and use of uninitialized locations),
deeply recursive function-calls, and some run-time library-call argument
errors (although this is not a highly advertised feature).

Purify operates by modifying object code to intercept reads, writes, frame
establishment and dis-establishment; it also wraps monitoring code around
malloc/free and some other standard library functions.  It tags all heap,
stack, and common memory so that the intercepted operations can verify
allocation and initialization states.  It seems likely there are 2-3 tag
bits per memory location (stored in a parallel table) which allows the
intercepted reads and writes to quickly check their validity (I'm guessing
there is a valid bit, an initialized bit, and a pointer bit.  The valid bit
may have several interpretations depending on the actual location of the
storage and can also be overloaded to monitor individual locations.  I'm
guessing there is a pointer bit that is used to scan for memory leaks,
although they may just use a conservative scan technology and not bother
with the pointer bit.  It's also possible there is some more clever
encoding that keeps the tag bits to a minimum when large blocks are
allocated.)

There is some capability to check for out of bound array references (and
other stray references through pointers), done by creating "boundary zones"
around all heap-allocated arrays.  Clearly this only works for references
that aren't too out of bounds.  The boundary-zone is cleverly used to also
store a short call-trace at the time of allocation so that the creation
point of objects can easily be found (e.g., if the object is used before
initialized, you may want to initialize it when created).

Memory leaks are discovered most likely by a simple mark/sweep algorithm
that can be called at any time (e.g., from the debugger) to verify that all
allocated storage is still in use.  If leaked memory is found, the
call-trace can be used to discover it's origin, but it is then up to the
programmer to examine the code and use a combination of breakpoints and
mark/sweep verifications to discover where the memory is actually "lost".
(This is exactly the technique used in the Symbolics IFEP to verify its
memory allocation, although it has the further luxury of being able to
describe the lost objects by looking at their tags and headers.)

Malloc and free calls are intercepted by Purify to implement the boundary
zones and other accounting.  Purify also delays actually freeing objects,
keeping them on a queue to detect possible references to freed objects,
although this is also only done "statistically", since objects eventually
are recycled off the queue.

Since every read and write is emulated, it is easy for Purify to provide
monitoring of locations to discover clobbered memory (the one caveat is
that kernel trap handlers, which obviously won't have been purified, will
not be intercepted).

The manual claims a 2-5 times slowdown in performance when "purifying",
which leads me to believe that the intercepted operations are replaced with
subroutine calls that perform a validity check and simulate the operation.
The manual claims a 1.5 times storage penalty, which may imply there are
more tag bits than I described although some of that storage penalty is due
to the technique of not immediately freeing objects for detecting
references to freed memory.

My guess is that the bulk of the work by the Purify developers has been in
adapting their schemes to work with different compilers and platforms.
Their choice to modify object code allows them to monitor libraries in
addition to user code, but probably comes at some expense.  They are
certainly strongly dependent on the emitted code being fairly structured.
I can imagine that some hand-coded routines could either fool their error
checking or work incorrectly.

Purify is currently only available on SPARC and HP platforms, but it may be
that since it can only be used in development, it isn't necessary for them
to support other platforms, such as PC's (if you believe the C is
portable).

Comparison with Garbage Collection

Introducing garbage collection to C eliminates some of the classes of
errors that Purify is attempting to detect, but by a different tack.
Memory leaks will no longer occur, since GC will automatically reclaim
such.  Free memory references will not occur, since memory will not get
freed if it can still be referenced (although some proposals allow manual
deletion as a "hint", in which case either the deletion or the subsequent
reference is an error).  On the other hand, you can get the inverse of a
leak:  memory that is erroneously held onto simply because a pointer to it
is left lying around.  There is GC technology (e.g., the Symbolics Minima
GC cons-tracing) that attempts to deal with this class of errors, but like
the Purify techniques, it has enough overhead that it can only be used in
development.

There is nothing in GC technology that will replace Purify's other checking
capabilities (e.g., bounds checking, location monitoring, etc.)  Most of
these come from the tagged-memory simulation they use, which is also the
source of their overhead.  (In the Symbolics VLM, simulating a
tagged-memory read with no exceptions took 10 cycles, whereas a straight
memory read is at most 3 cycles, 2 of which are stalls that can usually be
avoided).

The biggest challenge in trying to market a GC for C compilers to my mind
is the fact that most (if not all) proposals for adding GC to C require a
change in programming style to make C a "gc-safe" language.  Programmers
will initially be very reluctant to give up the full freedom they have with
C for what may be very skeptically perceived as a benefit, especially if
they believe a tool like Purify will allow them to find and repair the
errors GC would prevent at no cost in the delivered system.  However, if a
standard for adding GC to C should be adopted, much of the marketing
challenge will have been solved.

Along a similar line, the Purify approach lends itself very well to code
maintenance, which the GC approach does not.  Thus a C-GC product should be
targeted at new development, especially large new developments.

