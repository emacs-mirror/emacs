                 GENERATE DOCUMENTED TESTING PROCEDURES
                             obj.test-proc
                          obsolete started obj
                           richard 1995-08-11

Responsible: rom
Due: 30/11/95
Products: dsm's report is now part of analysis.test


Our tests are currently generated in an ad-hoc manner.  We need to work out 
what tests we need, how to make them, how to run them and automate them, and 
what we consider acceptable for exit criteria for release.


REPORT:

dsm: This is as far as I got

INTRODUCTION

Testing is a way of detecting defects.  In the literature this includes
Reviews, Inspection, Unit testing, System testing, etc. i.e. any way
of finding errors.  I am using it here to mean runnable tests, which
I suppose includes unit testing and system testing.

THINGS WE ALREADY DO

.cover: Write coverage tests.  These tests are intented to ensure that
all lines of code are executed.

.stress: Write stress tests.  A stress test may be a simulation (of real
program).  It covers some functionality.  It may attempt to 'stress' a
particular part of the system.

.oneoff: Write oneoff tests in a hacky way while making a change.  This
is often done by modifying existing tests, as we do not have much of a
harness at the moment.

.automate: Automate running of our tests.  At the moment the sources
are checked out on every night and built on a SUN.  Some tests are run
and we are mailed if they fail.


SUGGESTIONS

For each piece of code we have some method or methods for testing that
piece of code.  That is there are continually maintained documents 
(eg impl.std.testdoc?) which describe the procedure for testing each
piece of code.  Writing code (proc.impl.create?) and updating code
(proc.impl.edit?) involves maintaining this document.


What a Testdoc should have in it

1. Map from code to test procedure 
this might be a general procedure eg
- .run.blind  (build & run. exit code  0?)
- .run.examine (build & run examine output)
- .run.step (step the code)
- test unfinished
- not tested
- .hope.blind
- or a specific one (eg build & run this program.  Time it.  Check it takes 
less that 7 seconds)

2. Other information
- deficiencies of tests
- ideas for improvements to tests
- justification for why a piece of code isn't tested, or testing difficult


Suggested Outline of Test Procedures

.cover.run:
- proc.build(the test)
- run the test
- check it ran ok (check exit code 0?)

.cover.update:
- check all functions in interface are tested, add if not.
- proc.build it
- profile test prog (there are hacks in some NT makefiles to do this)
- check by hand it covers
- either add coverage or document in (.testdoc) where it does not cover with 
alternative test or justification for absence

.cover.create: 
- like cover.update except build it first

.stress.run: The procedure at the moment seems to be
- proc.build it
- check it runs ok

Test Plans

What is a test plan?  Is what I call testdoc a test plan?  We should find out.  
They are probably a good idea.


Test Reviews

We could from time to time review test code


Regression Tests

We also need tests corresponding to defects found, that hopefully will
detect when they reoccur.  There should be a documented way of going from 
detected defect (issue) to test that implements it.  Perhaps this should
go in the testdoc.


Test Creation Guidelines

.test.impl.guide:  We need to have some procedures for creating tests.  There 
are some areas
where this is weak at the moment.  These can be updated as our experience 
improves.

Whether a test is worthwhile (results justify costs) depends on:
1. The cost of creating the test.
2. The cost of running the test.
3. The effectiveness of the test.  How many errors are detected?
4. The severity of the errors detected.
5. Orthogonality to other means of detecting errors.  How expensive is
it to detect the errors by other means?


Deficiencies in Tests

There are a couple of things to think about more when writing tests:
.check.results: We do not check results of functions enough.  We should check 
code gives correct results not just that it returns ErrSUCCESS.
.assert: Use asserts that will fire in both debug & release versions.  At the 
momemt we use AVER in test code, which is therefore turned off in the release 
version of the test -- this is unneccessary.


Testing During Development.

At the moment this is often done by messing around with an existing
test.  As proc.impl will probably include updating tests this will
encourage all tests to be added permanently to the existing test suite.
I think that would be very cost effective.


Testing for Release.

This will be a more or less full run of all test procedures, including
the tedious checking by hand ones.


Automation

.automate.more: The cheapest way to run a test is automatically.  This gives us 
a lot of
checking with a low running cost.  Ultimately, I would like to see blind
runs of all tests run on all platforms (most importantly NT) regularly and
automatically.


Acceptance Tests

.acceptance.attr: We would like a test that we could do to tell us how
far from meeting our various attribute requirements we are.  We need a
definitive way of knowing when we have met them.  This may involve
running a program and seeing how long it takes; or for example writing
special tests that measure pause times.

.acceptance.fun: I would include tests for functional requirements
too.  We should test our interfaces against what they are required to
do, if possible.  Running real programs is a weak form of this.


Perfomance Testing

.check.perform: Having measured our tests we should check for
any unacceptable regression or unexpected change in performance.


THINGS TO THINK ABOUT

.realprog: Use real programs as tests.  These are good for detecting
errors that are likely to arise in practice.  Errors that stop a
typical program running are the most severe.
richard: I think this is particularly valuable.  We should get frozen
Dylan systems and include them as tests.
Nosa has pointed us in the direction of the cmu test suite.


.method.emit: An effective lightweight way of generating tests is to
write code that prints data that needs to be checked.  The emitted data
is compared from one run to the next.  This is easy to automate, and
any change in functionality is immediately picked up.  Our current
tests are not very suitable for this as they print out machine
addresses and pictures of the heap.  Difference in output here would
not imply a change in functionality.

.bench: Write tests to measure performance of specific features.  We
need to have a better idea of what we wish to measure before we do
this.  Real programs are useful for giving overall performance
estimates.

.analyse: Analyse measurements to provide feedback about what can
be done to improve the code, and what effects various changes to the
code have.

.investigate.literature: There is some literature about testing, which
could be read.  From a brief glance it seems that perhaps the most
useful stuff is the hard figures about the effectiveness of different
types of testing.

.harness: We use C as our test harness.  This isn't very
sophisticated...



