      PRODUCE ASSESSMENT OF JONATHAN'S PERSISTENCE IMPLEMENTATION
                     obj.dylan-persistence-analysis
                         obsolete finished obj
                           richard 1995-08-09

Responsible: ptw
Due: 18/08/95
Products: .report
Sources: dylan.person.jonathan


Look at Jonathan's prototype persistence implementation.  Distill important 
information into a document describing:
  - requirements
  - how it works
  - clever implementation tips
  - how it might fit into the MM system.

The reasons for this objective are twofold:

1. Produce enough information so that we can analyse the impact of actually 
implementing this stuff, and tell the Dylan team what the impact would be.  
Andy needs _that_ information to make a decision as to whether to make 
persistence a requirement for MM or not.  In other words, they need to decide 
whether we should be the solution to their requirements.  Since their 
requirements are on the undefined side at the moment, this is a difficult 
decision to make.  I'm going to work with Andy to help him sort this out.

2. Make sure we know what might be coming.  If the Dylan team say they want 
persistence from us then we'll have the beginnings of a design, and will be 
prepared.  Forewarned is forearmed, or something.  I'd like to make sure we do 
the incremental collector and framework in such a way as to permit persistence.


REPORT:

Date: Fri, 25 Aug 1995 09:44:56 -0400
To: richard (Richard Brooksby)
From: ptw (P. Tucker Withington)
Subject: Re: obj.dylan.analysis.persistence clarified

Summary of my discussions w/ JB.  His document is attached ( - ps.pdf ), if you 
want the
gory details.

[Later:  I have clarified some of the questions I had (---) w/ Jonathan.
These notes are marked ->]

Dylan Persistent Store Requirements (per Jonathan):
  Source Database
    replaces file system, stores code sections, hypertext links
    --- it is unclear to me that this is in rev. 0?
    --- this would seem to imply sharing and fine-grained locking?
->  There is some belief that JRD is going to join the project to do the
    source database implementation and magically solve these issues.  JRD
    worked on such a system at Symbolics, but it was based on using OStore
    as the database
  Derived Database
    stores compiler analysis of source, primarily for linking, also
    for debugging

  Requirements:
    Easy integration
      persistent objects should be the same as transient
    High performance
      will be a bottleneck on compiler performance
      DOSS (batch loaded) is deemed insufficient
    Minimal memory consumption
      needs to run on small configurations where database may be paged in/out
      --- this conflicts with the High performance requirement, which could
      easily be solved by mapping in the entire database
->    In discussion Jonathan mentioned the need for queries to the database,
      which I pointed out are made efficient in commercial implementations
      by using indices -- which there is no stated requirement for (yet).
    Incremental redefinition
      in database terms, schema evolution
      --- this appears to be a high risk item
->    I misunderstood this point.  The requirement for the derived database is
      simply that imports get correctly updated when the exports they are based
      on change.

  Non-requirements:
    Multi-user
      rev. 0 is only single user
    Transactions/Logging/Rollback
      derived database is recomputable from source
      source database gets its reliability elsewhere --- How?
->    Jonathan admitted that some form of transaction is actually necessary.
      Consider the case of a compile that encounters an error -- the database
      needs to be rolled back to the pre-compile state.
    Versioning
      source database implements versioning above persistence
      --- what about derived database?
->    In rev. 0 versioning could be simulated by snapshotting/copying the
      database.
    Inter-database pointers
      explicit import/export is sufficient (--- true for source?)
    Architecture-independent
      rev. 0 has a single platform
    Garbage collection
      "normal" GC should suffice (--- meaning anything not accessible from
        database root can be collected.  contrast w/ ostore which does not
        collect because indices essentially make all objects "accesible")
->    Jonathan meant that he thought it would be okay to accidentally cons
      temporaries in the permanent pool because they would get collected
      before they were stored in the database.  I think this is unrealistic (it
      would imply doing a GC at the end of each transaction).
    Huge objects
      rev. 0 databases and their objects fit easily in VM
      --- c/f Memory consumption which implies they may _not_ fit in phys. mem.
    Meta-objects
      in database terms, the db is not required to store the schema
      in Dylan terms, the db stores only instances, not classes

  Future requirements:
    (--- presumably all of the non-requirements, plus)
    Clustering/Anti-clustering
      object placement on disk
    Managed cache size
      control number of objects "paged in"
    Reinitialization
      re-initialize "transient slots" in permanent objects
->    Jonathan pointed out that this might be a requirement after all.  I
      suggested what is really wanted is a de-initialization or finalization
      of transient slots on write to the database and that it could be modelled
      on how we plan to implement weak slots.  (You don't want MM to have to
      call user code at page-in time to re-initialize.)
    Batch schema evolution
      --- incremental is harder, why is this not rev. 0 and incremental future?

Jonathan's "simulation":
  Based on Texas PStore design
    pointer swizzling at page-fault time
    --- architecture independence would imply swizzling of all data, e.g.,
      fixing byte-order, etc.

  Simulation uses meta-objects which is the "wrong" solution

  API
    shutdown-pools - close all pools
    restart-pools - shutdown and reinitialize
    *pool* - fluid-bindable, where to allocate
    <transient-pool> - "normal" pool class
    <persistent-pool>
    dump - describe pool contents (for debugging)
    add-export - enter name/value in pool root
    remove-export
    export-value - lookup name in pool root, return value
    export-value-setter
    add-import - external reference to another pool
    remove-import
    add-all-imports - import everything exported by another pool
    remove-all-imports
    flush-dirty-pages - write changed pages to disk
->    This is Jonathan's simplistic concept of transactions -- all changes
      occur only in memory and then are optionally written to disk.  This
      ignores the case of there being more modifications than can be kept
      in memory, or nested transactions.
    dump-persistent-pool - save pool to disk (--- in some particular format?)
->    This appears to be the same as flush, but it (re-)writes unmodified pages
      too.  Jonathan couldn't say what this was useful for.
    open-persistent-pool - create a persistent pool on filename
    reopen-persistent-pool - open extant pool
    maybe-reopen-persistent-pool - (may already be open)
    abandon - close persistent pool
->    Intended to be abort-transaction, but see above.
    shutdown - close and dump (--- what does this mean?)

  Implementation
    --- It's unclear how useful the detailed implementation of the simulation
    is
    Basic design:
      When an object is referenced in a persistent pool the object is brought
      into memory, all it's references are converted from disk addresses to
      VM addresses and those VM addresses protected.  Following a reference
      faults, repeating the cycle.  The root object of a persistent pool is
      the export name->value table, mapped in on open.
    Key details:
      depends on read/write barriers to page in data on reference
      requires knowing object size from reference (so VM can be allocated
        for reference without examining object on disk)
      needs VM<->disk mapping table

How does this fit in MM framework?
  Pool model seems appropriate for ease of use requirement
  MM already must support read/write barrier
  MM understands object references, so can detect pointers for swizzling

What extensions to MM would be needed to support this?
  MM does not currently do any disk I/O
  Pool interface needs extending to associate a file, support re-opening
  Import/export interface
  barrier service needs to be multiplexed to handle not only GC needs but
    also virtual DB access

Risks:
  Allocation by fluid-binding deemed undesirable
  Swizzling may need more than disk<->vm mapping, e.g., it may need to encode
    size of object referenced.
    --- Is this really true? Consider paging on card-groups instead.
  Simple disk I/O vs. reliability
  All the "non-requirements" are things that "real" databases satisfy
  Neither Dylan group nor MM group are Database experts.


-> I guess what has made me uncomfortable about this whole process is that
Jonathan has written a toy implementation and then derived his
"requirements" from what his toy implementation does, not from what Dylan
actually needs.  As you can see above, I have been able to get him to
realize that there are several difficult areas his toy implementation
completely avoids that are really necessay to satisfy Dylan requirements.

It seems to me that before we can go too far down the path of sketching an
MM implementation, we first have to get Jonathan to realize what his
requirements really are (perhaps this is what you were saying was Andy's
hidden agenda).

My seat-of-the-pants reaction is that perhaps we could develop an interface
to OStore in the timeframe they need, but that it is highly unlikely we can
develop an entire database in that same timeframe.  I think Jonathan and
SWM are overlooking a number of difficult issues because they haven't
realized the requirements that drive them.



ATTACHMENT:

   "ps.pdf"


