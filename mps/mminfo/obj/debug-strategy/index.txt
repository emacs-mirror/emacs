              INVESTIGATE DEBUGGING STRATEGY REQUIREMENTS
                           obj.debug-strategy
                         obsolete complete obj
                           richard 1995-08-22

Responsible: dsm
Due: 25/08/95
Products: .report


issue.debug-stategy uncovers the fact that we haven't thought hard about how we 
are going to debug the MM, especially once it leaves our hands.

See request.process.170362

People we might debug for are:
  - ourselves
  - DylanWorks
  - EP Core RIP
  - external customers
    - of DylanWorks
    - of EP
    - directly (at some point in future).

For each of these, try to think of scenarios where we have to support the 
system and debug it.  Write out what the procedure might be, and what we would 
need to successfully solve problems in each scenario.  Effectively, you will be 
deriving new requirements from req.*.fun.support.available, the requirement to 
keep the system available.

The mission, should you choose to accept it, is to provide proto-requirements 
we can refine, and some ideas we can assess against those requirements in order 
to decide on our strategy.

Paste your notes into the "report" section below, and we'll work out what to do 
next.

The level of detail required for these notes is such that you could mail them 
to someone who has not been in dicussion with us and they could understand and 
respond usefully.  This is also the level of detail we would need to be able to 
put the task down for some months then come back to it and deal with it equally 
effectively.  (I'm not sure when we'll deal with this properly, and it may not 
be until after the incremental release.)


REPORT:

THOUGHTS ON DEBUGGING STRATEGY

Sources
  issue.debug-strategy
  book.gilb88.reqs
  issue.repeatability
  talking to ptw, drj and richard

[TO DO:
summary
analysis of support process critical path and dependencies
tag everything?
restructure
reformat (I don't understand when Notes makes things wrap)
]

SUMMARY

This is not a summary but this report contains:
- support process requirements, but weak in inspection,
  deployment and testing, and test evaluation
- large number of debugging ideas
- some requirements analysis


Support Process Requirements

Fun
- resolve issues
- make sure we have feedback when there is a problem
- avoid issues being raised uneccessarily

Attr
- satisfaction of client
- impact on image
- speed of resolution

Or perhaps Gilb's analysis is more complete (from book.gilb88.req):
maintainability
  recovery time
  recovery time by severity
  problem recognition
  administrative delay
  tool gathering
  problem analysis
  solution formation
  inspection
  deployment
  testing
  test evaluation


SUPPORT PROCESS

- problem recognition
- issue raised i.e. reported to us
do {
- information gathering
- tool gathering
- problem analysis
- further information requested/provoked 
- solution formation
- solution suggested (if no good continue)
- (partial) solution implemented (could be work around or patch)
  and tested (inspected?)
- deployment (solution provided to client)
} until client happy (support issue closed)

repeat {
- information gathering
- tool gathering
- problem analysis
- further information requested/provoked 
- solution formation
- solution implemented (implies testing)
} until we're happy
- add regression test?
- inspection
- process improvement 

Time
[Analyse how long process takes.  what is critical path? ]
- Communication overheads.
- Time to acquire resources (gather tools)
- Time to rebuild and preform tests.  May not have time to run full tests.
  [andy said that they did not run their test suite after fixing a bug.]

Resource Dependencies [expand]
- tools
- machines
- knowlege
- experience
- documentation
- brainpower


DEBUGGING


Problem Detection

We currently have assertions (in debug version only) which check...
- validity of function parameters (most assertions fall into this category) 
- object formats during scanning (These are notionally provided by client
  i.e. in dw layer at present)
- object format on allocation (added in version.dylan.debug)
We could also:
- check validity of adjacent objects
- provide large scale checks such as:
  - are all objects valid?
  - we have finished GC.  Are there any pointers into old space left?
- object ids.  On allocation reserve extra words containing:
  - unique id (so that we can track an objects movement)
  - time-stamp (may be simple counter incremented on allocation)
  - reference that caused the object to live
  - which thread allocated, GC'ed object
  - allocation call site
- fence posts.  Distinguished sequences around an object enabling
  us to determine where objects start and finish just by looking
  at heap near a pointer.

There may not be too much of an overhead to store some of this info
for larger objects in release code.  We could store some information for 
smaller objects by placing them in distinguished areas.


Solution Formation

For each type of error we can detect we need a policy for dealing with it.
There is no analysis of this process here.

At the moment our policy is:  if the error is detected by assertion
we report which assert failed and abort.
Possible forms of solution:
- provide a way to work around the error
- use a handler to provide debugging info
- report error to client in polite way (e.g. raise dylan condition)
- log error and attempt to continue
- suspend waiting for manual fix (via access path)
- give up waiting for debugged code
- provide option to make heap consistent in some automatic way
- pin down bad area of memory and continue without GCing it.
  Similar to bad-blocks on a disc.
- turn off some feature that may be causing or provoking error.
  (e.g. turn incrementality off).
- say "sorry, can't isolate cause"
- say "sorry, can't fix bug"

Even if we think error is not our fault we can do something:
- if client misuses our interface.  
  - interface could be too complex -- could provide simpler one.
  - could be doc problem -- could improve doc.
- we rely on someone else's program (SEP) to work and it doesn't
  - aim to reduce such dependencies
  - suggest work arounds

It is likely that most of the time we will be detecting corruptions
of memory caused by someone else.

Acceptability of solution depends on scenario e.g. may:
- not care about crash but want bug fixed in future (e.g. during development)
- want to preserve data, but not continue
- want to resolve problem and continue
- not care about crash but want to try again
- simply not care at all


DEBUGGING TOOLS

Purpose
- gather information
- process information
- perturb system (to increase usefulness of gathered information)

At the momemt we use:
- assertions
  These tell us when there is a problem.  They seem to provide little direct
  help as to the cause, but together with a stack trace, they help greatly.
- gdb
- MS VC debugger
- WinDBG
  These debuggers allow us to step through code, set breakpoints and 
  examine state at the C language and machine level.



Possible Information sources

- description of problem (bug report).  This could be a written or spoken
  description provided by the client.  We may wish to know what change in use 
provoked
  the problem.  [What Qs are useful to ask]
- result of experiements we ask the clients to run.  (e.g. try again with 
debugging code)
- core dump.  This might be an entire dump of the process state.
  [When is this dump generated?  If the problem is repeatable can
  we gather information before crash?]
- trace.  This is information explicitly dumped by the program.  It may be
  desirable to produce a limited amount of information so that it is easier to
  send (see ways of gathering info).  Processing may be done to cut down the
  amount of information or produce an immediate answer. (e.g. full consistency 
check)
  Heap, stats, stack backtrace.
- One line info.  [What information is it useful to have here?]
  (eg. assert tag -- a tag to say which assert fired).
- event log.  This is information continually dumped by the program.  It 
provides
  a history of what was happening up to the moment of the crash.  This may be
  implemented as records in a circular buffer kept in memory which is dumped as
  part of the trace.
  (e.g. record is one word recording class and location of event)
- MM code (part no.).  We have our own code and may wish to examine it.
- client code.  We may or may not have the code the client was executing.
- environment info which may not be in core dump.
  - platform
  - page tables
  - segments
  - fault handler


Ways of Gathering Information

Usually there is a particular process that throws up a bug.  I refer
to this as the 'problem process'.  An 'access path' defines a protocol
for interacting with problem process.
The protocol may provide means of:
- reading from memory
- writing writing to memory
- executing code

This may be implemented in a number of ways.  Usually the problem process
is stopped and most facilities are provided by a separate diagnostic module.
The diagnostic module may be running in a separate thread, process, machine.
This could provide cross platform debugging.
- use Windows debugging support (can ReadProcessMemory())
- read from /proc
- talk using serial or network connection to a piece of code called a 'spy'
  which resides in code of problem process.
- shared memory
- using debugger
- examining core dump

Less direct means:
- dumped info sent by floppy disc
- dumped info sent by mail
- short descriptions over the phone
- simulation/reproduction of problem.  We may want to have as much 
  raw information stored as possible in case the problem proves
  difficult to reproduce.


Tools for Processing Information

- code to produce trace may do processing
- diagnostic module which uses access path
- could use gdb or other debug utility
- could write home grown debugger (make use of existing code)


Processing Information (what would be useful?)

- dump backtrace (maybe provided by client system e.g. dylan will
have to do this)
- check for consistency (of heap, of objects)
- dump of bad object and adjacent objects
- statistical summary
  - size of heap
  - amount of allocation done
  - number of collections
  - how long system running
  - amount of free memory
  - average size of allocation
  - etc.
- map over objects (in pool, of type)
- check for validity of object
- describe object
- save object (maybe for recovering lost data)
- make heap consistent (lost+found object?)
- find references (follow pointers backwards)
- analyse at C language level including symbolic info structures etc.

[where should this paragraph go?]
It's a good idea to turn GC off while trying to fix bug.  Perhaps we can
do this for a particular pool (or even segment) and switch new allocation
to a different (newly allocated) pool.



SCENARIOS

Who, what, when are we debugging?
- us; our code; during development
- ep, dylan group; our code; during development
- ep, dylan group; their code; during development
- dylan developer; our code; during app development
- dylan developer; their code; during app development
- third party (dylan app user, or sw user); any code; during end use

The exact form of the support process will depend on the scenario,
but they are sufficiently similar that we can use the same model.
Differences in scenario will affect:
- urgency.  How quickly we will be expected to solve a problem.
- importance.  How costly it is to be unable to solve the problem.
- cooperation.  The degree of cooperation and flexibility we can
  expect from the client.
- communication.  The means by which we communicate about the problem.
- information availability.  The ease or possibility of obtaining certain
  pieces of information.
- information quality.  The accuracy and appriateness of the information
  supplied.
- resource/tool availability.  Most critically, whether we have a platform
  on which we can analyse the users problem.  [andy mentioned that they
  have an ongoing problem in EP, because one of their customers has a bug,
  I think due to a hardware problem.  Unfortunately EP do not have such a
  machine they can use to track down the problem.]
- error conditions.  The tolerated behaviour of our code on detecting an error.
  E.g. we may be expected to report the error as a dylan error condition.
- recoverability.  Our clients may need to tolerate errors and recover from 
them.

Requirements (arising from different scenarios)

Each of these differences in scenario essentially presents a risk to our 
ability to
support the memory manager.  We can combat these risks by placing extra
requirements our code on the debugging tools we have.
- tool availability.  We need to ensure our tools are supported.
- remote.  We can improve our communication ability, and the quality
  and availability of information by having an access path protocol
  that can work remotely.
- traces & dumps.  We need to be able to get information on the problem
  even if we have no direct access to a problem process.  We need to
  ensure that this information is generated automatically by our code.
- foreign.  We need the ability to analyse information that comes from
  platforms with hardware/software configurations we do not have ourselves.
  Making the access path work across platforms would be nice.
- speed.  We will need to debug to specified time scales.  This means
  that all the tools and processes we use will need to have specified
  performance requirements.
- error detection.  We need to detect errors such as corruptions of the
  heap not just in the debug code.
- error handling.  We need to have a more flexible attitude towards
  dealing with errors.
- robustness.  Our code needs to be more fault tolerant.
- documentation.  We need to tell the clients what to do when they
  have a problem, so we can ensure that we get appropriate information.


Current Debugging Methods

- examine stack backtrace (most valuable)
- examine heap (also useful)
- use breakpoints and single stepping on repeatable error to find what happened
recently (useful)
Knowing which assertion fired gives very little clue as to what the error is.
However, examining the backtrace often gives us most of the information.

The Dylan group (well this is what tony said he did pre version.dylan.debug)
will run with release code, and try again with debugging code when a problem
is found.  Tracking down the problem is then easy.

I conclude:
- we rely heavily on repeatability (see issue.repeatbility)
- knowing recent history (events leading up to error) is very useful.
(this is also backed up by ptw's event logging experience)


General Requirements

We need mechanisms in place to combat issue.repeatbility soon as
our ability to provide support effectively is at risk.

In particular we need to
- implement event logs.
- develop analysis tools.
- implement tracing and dumping mechanisms.


Process Feedback

What's useful to record about how an issue was resolved?
- which bits of info were useful
- what would be nice but was missing
- what tools were useful
- what tools would be nice
etc.


CLIENTS

Dylan

Requirements
req.dylan.fun.soft.support.available
req.dylan.fun.soft.control.dial.culprit
- How does debugging interact with Dylan debugger/environment?

Probably also
- need to generate error conditions
- fault tolerance
- some tools for app developers

EP

Requirements
req.epcore.fun.support.available
req.epcore.fun.soft.debug: Debugging of memory and memory manager usage

meeting.epcore.1995-08-08.debug: Debugging support must be at least as
good as the present system.
- What is the present system? [ask someone]
- Potential improvements.  There are many tools we could provide.

Possible extra requirements:
- fault tolerance
- means of recovery?  restart?
- report error conditions?
- support turn around time
- provide debug tools to assist support?



